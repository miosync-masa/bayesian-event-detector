{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda¬≥ Weather Analysis - Automated Pair Analysis System\n",
    "\n",
    "This notebook analyzes meteorological data structure tensor interactions based on Lambda¬≥ theory.\n",
    "\n",
    "## Workflow\n",
    "1. Retrieve necessary code from GitHub repository\n",
    "2. Fetch Tokyo weather data from Open-Meteo API\n",
    "3. Extract Lambda¬≥ features\n",
    "4. Execute automated analysis for all parameter pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install openmeteo-requests requests-cache retry-requests pymc arviz numba networkx -q\n",
    "\n",
    "print(\"‚úì Library installation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve Lambda¬≥ Code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone GitHub repository\n",
    "!git clone https://github.com/your-username/bayesian-event-detector.git\n",
    "\n",
    "# Set working directory\n",
    "import os\n",
    "os.chdir('bayesian-event-detector/sample')\n",
    "\n",
    "# Check for available files\n",
    "import glob\n",
    "files = glob.glob('*.py')\n",
    "print(f\"Available Python files: {files}\")\n",
    "\n",
    "# Handle case where WeatherAnalysis.py doesn't exist\n",
    "if 'WeatherAnalysis.py' not in files:\n",
    "    print(\"‚ö†Ô∏è WeatherAnalysis.py not found. Please upload it.\")\n",
    "else:\n",
    "    print(\"‚úì WeatherAnalysis.py confirmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Automated Pair Analysis Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lambda3_auto_pair.py\n",
    "auto_pair_code = '''# ===============================\n",
    "# Lambda¬≥ Automatic Pair Analysis System\n",
    "# ===============================\n",
    "from itertools import combinations\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class PairAnalysisConfig:\n",
    "    \"\"\"Configuration for automatic pair analysis.\"\"\"\n",
    "    # Analysis parameters\n",
    "    analyze_all_pairs: bool = True\n",
    "    max_pairs: Optional[int] = None  # None means analyze all\n",
    "    min_series_length: int = 100\n",
    "    \n",
    "    # Pair filtering criteria\n",
    "    min_correlation: Optional[float] = None  # Filter pairs by minimum correlation\n",
    "    exclude_patterns: List[str] = field(default_factory=list)  # Patterns to exclude\n",
    "    include_only_patterns: List[str] = field(default_factory=list)  # If set, only these\n",
    "    \n",
    "    # Analysis depth\n",
    "    detailed_analysis_limit: int = 5  # Number of pairs for detailed plots\n",
    "    summary_only_after: int = 10  # Switch to summary mode after this many pairs\n",
    "    \n",
    "    # Output configuration\n",
    "    save_results: bool = True\n",
    "    output_dir: str = \"lambda3_results\"\n",
    "    generate_report: bool = True\n",
    "\n",
    "class Lambda3AutoPairAnalyzer:\n",
    "    \"\"\"Automatic pair analysis system for Lambda¬≥ framework.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PairAnalysisConfig = None):\n",
    "        self.config = config or PairAnalysisConfig()\n",
    "        self.analysis_results = {}\n",
    "        self.pair_metadata = {}\n",
    "        \n",
    "    def detect_data_structure(self, data: Union[pd.DataFrame, Dict[str, np.ndarray]]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Automatically detect data structure and categorize columns.\"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            columns = data.columns.tolist()\n",
    "            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        else:\n",
    "            columns = list(data.keys())\n",
    "            numeric_cols = columns\n",
    "        \n",
    "        # Categorize columns based on patterns\n",
    "        categories = {\n",
    "            \\'weather\\': [],\n",
    "            \\'temperature\\': [],\n",
    "            \\'humidity\\': [],\n",
    "            \\'pressure\\': [],\n",
    "            \\'wind\\': [],\n",
    "            \\'precipitation\\': [],\n",
    "            \\'other\\': []\n",
    "        }\n",
    "        \n",
    "        patterns = {\n",
    "            \\'temperature\\': [\\'temp\\', \\'temperature\\', \\'dewpoint\\', \\'dew_point\\'],\n",
    "            \\'humidity\\': [\\'humid\\', \\'rh\\', \\'relative_humidity\\'],\n",
    "            \\'pressure\\': [\\'pressure\\', \\'press\\', \\'hpa\\', \\'mbar\\'],\n",
    "            \\'wind\\': [\\'wind\\', \\'gust\\', \\'speed\\'],\n",
    "            \\'precipitation\\': [\\'precip\\', \\'rain\\', \\'snow\\', \\'precipitation\\']\n",
    "        }\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            col_lower = col.lower()\n",
    "            categorized = False\n",
    "            \n",
    "            for category, keywords in patterns.items():\n",
    "                if any(keyword in col_lower for keyword in keywords):\n",
    "                    categories[category].append(col)\n",
    "                    categories[\\'weather\\'].append(col)\n",
    "                    categorized = True\n",
    "                    break\n",
    "            \n",
    "            if not categorized:\n",
    "                categories[\\'other\\'].append(col)\n",
    "        \n",
    "        return categories\n",
    "    \n",
    "    def generate_pair_list(self, \n",
    "                          series_dict: Dict[str, np.ndarray],\n",
    "                          categories: Optional[Dict[str, List[str]]] = None) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Generate intelligent pair list based on data structure.\"\"\"\n",
    "        all_series = list(series_dict.keys())\n",
    "        \n",
    "        # Apply include/exclude patterns\n",
    "        if self.config.include_only_patterns:\n",
    "            all_series = [s for s in all_series \n",
    "                         if any(pattern in s.lower() for pattern in self.config.include_only_patterns)]\n",
    "        \n",
    "        if self.config.exclude_patterns:\n",
    "            all_series = [s for s in all_series \n",
    "                         if not any(pattern in s.lower() for pattern in self.config.exclude_patterns)]\n",
    "        \n",
    "        # Generate all unique pairs\n",
    "        all_pairs = list(combinations(all_series, 2))\n",
    "        \n",
    "        # Apply correlation filter if specified\n",
    "        if self.config.min_correlation is not None:\n",
    "            filtered_pairs = []\n",
    "            for a, b in all_pairs:\n",
    "                corr = np.corrcoef(series_dict[a], series_dict[b])[0, 1]\n",
    "                if abs(corr) >= self.config.min_correlation:\n",
    "                    filtered_pairs.append((a, b))\n",
    "                    self.pair_metadata[(a, b)] = {\\'correlation\\': corr}\n",
    "            all_pairs = filtered_pairs\n",
    "        \n",
    "        # Apply max pairs limit\n",
    "        if self.config.max_pairs and len(all_pairs) > self.config.max_pairs:\n",
    "            print(f\"Limiting analysis to {self.config.max_pairs} pairs out of {len(all_pairs)} total\")\n",
    "            all_pairs = all_pairs[:self.config.max_pairs]\n",
    "        \n",
    "        return all_pairs\n",
    "\n",
    "# Continue with remaining methods...\n",
    "# (Full implementation would be too long for this cell)\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open('lambda3_auto_pair.py', 'w') as f:\n",
    "    f.write(auto_pair_code)\n",
    "\n",
    "print(\"‚úì lambda3_auto_pair.py created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetch Tokyo Weather Data from Open-Meteo API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup API client\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# API call parameters\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "    \"latitude\": 35.6812,\n",
    "    \"longitude\": 139.7671,\n",
    "    \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "               \"precipitation\", \"wind_speed_10m\", \"surface_pressure\"],\n",
    "    \"timezone\": \"Asia/Tokyo\",\n",
    "    \"start_date\": \"2025-06-20\",\n",
    "    \"end_date\": \"2025-06-27\"\n",
    "}\n",
    "\n",
    "print(\"üåê Fetching data from Open-Meteo API...\")\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process response\n",
    "response = responses[0]\n",
    "print(f\"\\nüìç Coordinates: {response.Latitude()}¬∞N {response.Longitude()}¬∞E\")\n",
    "print(f\"üèîÔ∏è Elevation: {response.Elevation()} m\")\n",
    "print(f\"üïê Timezone: {response.Timezone()}{response.TimezoneAbbreviation()}\")\n",
    "\n",
    "# Process time series data\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_wind_speed_10m = hourly.Variables(4).ValuesAsNumpy()\n",
    "hourly_surface_pressure = hourly.Variables(5).ValuesAsNumpy()\n",
    "\n",
    "# Create DataFrame\n",
    "hourly_data = {\n",
    "    \"date\": pd.date_range(\n",
    "        start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    ),\n",
    "    \"temperature_2m\": hourly_temperature_2m,\n",
    "    \"relative_humidity_2m\": hourly_relative_humidity_2m,\n",
    "    \"dew_point_2m\": hourly_dew_point_2m,\n",
    "    \"precipitation\": hourly_precipitation,\n",
    "    \"wind_speed_10m\": hourly_wind_speed_10m,\n",
    "    \"surface_pressure\": hourly_surface_pressure\n",
    "}\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "# Display data summary\n",
    "print(\"\\nüìä Data overview:\")\n",
    "print(hourly_dataframe.info())\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "print(hourly_dataframe.head())\n",
    "\n",
    "# Save to CSV\n",
    "hourly_dataframe.to_csv(\"tokyo_weather_days.csv\", index=False)\n",
    "print(\"\\n‚úÖ CSV output completed: tokyo_weather_days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Lambda¬≥ Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WeatherAnalysis.py\n",
    "from WeatherAnalysis import (\n",
    "    L3Config, \n",
    "    calc_lambda3_features_v2,\n",
    "    load_csv_data,\n",
    "    validate_series_lengths\n",
    ")\n",
    "\n",
    "# Load data\n",
    "print(\"üìÇ Loading data from CSV file...\")\n",
    "series_dict = load_csv_data(\n",
    "    \"tokyo_weather_days.csv\",\n",
    "    time_column=\"date\",\n",
    "    value_columns=[\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "                   \"precipitation\", \"wind_speed_10m\", \"surface_pressure\"]\n",
    ")\n",
    "\n",
    "# Validate data\n",
    "series_dict = validate_series_lengths(series_dict)\n",
    "\n",
    "# Lambda¬≥ configuration\n",
    "config = L3Config(\n",
    "    T=len(next(iter(series_dict.values()))),\n",
    "    draws=4000,  # Reduced for Colab\n",
    "    tune=4000,\n",
    "    delta_percentile=95.0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Lambda¬≥ Configuration:\")\n",
    "print(f\"  - Time series length: {config.T}\")\n",
    "print(f\"  - MCMC draws: {config.draws}\")\n",
    "print(f\"  - Jump detection threshold: {config.delta_percentile}th percentile\")\n",
    "\n",
    "# Extract features for each series\n",
    "print(\"\\nüî¨ Extracting Lambda¬≥ features...\")\n",
    "features_dict = {}\n",
    "\n",
    "for name, data in series_dict.items():\n",
    "    print(f\"\\n  Processing: {name}\")\n",
    "    feats = calc_lambda3_features_v2(data, config)\n",
    "    \n",
    "    features_dict[name] = {\n",
    "        'data': data,\n",
    "        'delta_LambdaC_pos': feats[0],\n",
    "        'delta_LambdaC_neg': feats[1],\n",
    "        'rho_T': feats[2],\n",
    "        'time_trend': feats[3],\n",
    "        'local_jump': feats[4]\n",
    "    }\n",
    "    \n",
    "    # Statistics\n",
    "    n_pos = np.sum(feats[0])\n",
    "    n_neg = np.sum(feats[1])\n",
    "    mean_tension = np.mean(feats[2])\n",
    "    \n",
    "    print(f\"    - Positive structural changes (‚àÜŒõC+): {n_pos}\")\n",
    "    print(f\"    - Negative structural changes (‚àÜŒõC-): {n_neg}\")\n",
    "    print(f\"    - Mean tension scalar (œÅT): {mean_tension:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature extraction completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute Automated Pair Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple automated pair analysis implementation\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate all possible pairs\n",
    "series_names = list(series_dict.keys())\n",
    "all_pairs = list(combinations(series_names, 2))\n",
    "\n",
    "print(f\"üîó Number of pairs to analyze: {len(all_pairs)}\")\n",
    "print(\"\\nPair list:\")\n",
    "for i, (a, b) in enumerate(all_pairs):\n",
    "    print(f\"  {i+1}. {a} ‚Üî {b}\")\n",
    "\n",
    "# Calculate synchronization rates (simplified version)\n",
    "from WeatherAnalysis import calculate_sync_profile\n",
    "\n",
    "sync_results = {}\n",
    "print(\"\\nüìä Executing synchronization rate analysis...\")\n",
    "\n",
    "for pair in all_pairs:\n",
    "    a, b = pair\n",
    "    \n",
    "    # Calculate sync rate for positive jump events\n",
    "    sync_profile, max_sync, optimal_lag = calculate_sync_profile(\n",
    "        features_dict[a]['delta_LambdaC_pos'].astype(np.float64),\n",
    "        features_dict[b]['delta_LambdaC_pos'].astype(np.float64),\n",
    "        lag_window=10\n",
    "    )\n",
    "    \n",
    "    sync_results[pair] = {\n",
    "        'sync_rate': max_sync,\n",
    "        'optimal_lag': optimal_lag,\n",
    "        'profile': sync_profile\n",
    "    }\n",
    "    \n",
    "    print(f\"  {a} ‚Üî {b}: œÉ‚Çõ = {max_sync:.3f} (optimal lag: {optimal_lag})\")\n",
    "\n",
    "# Create synchronization rate matrix\n",
    "n = len(series_names)\n",
    "sync_matrix = np.zeros((n, n))\n",
    "\n",
    "for i, a in enumerate(series_names):\n",
    "    for j, b in enumerate(series_names):\n",
    "        if i == j:\n",
    "            sync_matrix[i, j] = 1.0\n",
    "        elif (a, b) in sync_results:\n",
    "            sync_matrix[i, j] = sync_results[(a, b)]['sync_rate']\n",
    "        elif (b, a) in sync_results:\n",
    "            sync_matrix[i, j] = sync_results[(b, a)]['sync_rate']\n",
    "\n",
    "# Display heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sync_matrix, \n",
    "            annot=True, \n",
    "            fmt='.3f',\n",
    "            xticklabels=series_names,\n",
    "            yticklabels=series_names,\n",
    "            cmap='Blues',\n",
    "            vmin=0, vmax=1,\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Synchronization Rate œÉ‚Çõ'})\n",
    "\n",
    "plt.title('Structural Synchronization Matrix Between Weather Parameters', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top synchronized pairs\n",
    "sorted_pairs = sorted(sync_results.items(), key=lambda x: x[1]['sync_rate'], reverse=True)\n",
    "\n",
    "print(\"\\nüèÜ Top 5 synchronized pairs:\")\n",
    "for i, ((a, b), data) in enumerate(sorted_pairs[:5]):\n",
    "    print(f\"  {i+1}. {a} ‚Üî {b}: œÉ‚Çõ = {data['sync_rate']:.3f} (lag: {data['optimal_lag']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Interaction Analysis (Selective Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the highest synchronized pair\n",
    "if sorted_pairs:\n",
    "    top_pair = sorted_pairs[0][0]\n",
    "    name_a, name_b = top_pair\n",
    "    \n",
    "    print(f\"\\nüîç Detailed analysis of highest synchronized pair: {name_a} ‚Üî {name_b}\")\n",
    "    \n",
    "    from WeatherAnalysis import (\n",
    "        fit_l3_bayesian_regression_asymmetric,\n",
    "        plot_sync_profile\n",
    "    )\n",
    "    \n",
    "    # Fit Bayesian regression model\n",
    "    print(\"\\nüìà Fitting Bayesian regression model...\")\n",
    "    print(\"(This may take several minutes)\")\n",
    "    \n",
    "    # Influence of B on A\n",
    "    trace_a = fit_l3_bayesian_regression_asymmetric(\n",
    "        data=features_dict[name_a]['data'],\n",
    "        features_dict={\n",
    "            'delta_LambdaC_pos': features_dict[name_a]['delta_LambdaC_pos'],\n",
    "            'delta_LambdaC_neg': features_dict[name_a]['delta_LambdaC_neg'],\n",
    "            'rho_T': features_dict[name_a]['rho_T'],\n",
    "            'time_trend': features_dict[name_a]['time_trend']\n",
    "        },\n",
    "        config=config,\n",
    "        interaction_pos=features_dict[name_b]['delta_LambdaC_pos'],\n",
    "        interaction_neg=features_dict[name_b]['delta_LambdaC_neg'],\n",
    "        interaction_rhoT=features_dict[name_b]['rho_T']\n",
    "    )\n",
    "    \n",
    "    # Results summary\n",
    "    import arviz as az\n",
    "    summary_a = az.summary(trace_a)\n",
    "    \n",
    "    print(f\"\\nüìä Interaction effects {name_b} ‚Üí {name_a}:\")\n",
    "    if 'beta_interact_pos' in summary_a.index:\n",
    "        print(f\"  Positive interaction: Œ≤ = {summary_a.loc['beta_interact_pos', 'mean']:.3f}\")\n",
    "    if 'beta_interact_neg' in summary_a.index:\n",
    "        print(f\"  Negative interaction: Œ≤ = {summary_a.loc['beta_interact_neg', 'mean']:.3f}\")\n",
    "    \n",
    "    # Visualize synchronization profile\n",
    "    plot_sync_profile(\n",
    "        sync_results[top_pair]['profile'],\n",
    "        title=f\"Synchronization Profile: {name_a} ‚Üî {name_b}\"\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No analyzable pairs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create DataFrame from synchronization results\n",
    "sync_df = pd.DataFrame([\n",
    "    {\n",
    "        'series_a': pair[0],\n",
    "        'series_b': pair[1],\n",
    "        'sync_rate': data['sync_rate'],\n",
    "        'optimal_lag': data['optimal_lag']\n",
    "    }\n",
    "    for pair, data in sync_results.items()\n",
    "])\n",
    "\n",
    "# Sort and save\n",
    "sync_df = sync_df.sort_values('sync_rate', ascending=False)\n",
    "sync_df.to_csv('lambda3_sync_results.csv', index=False)\n",
    "print(\"‚úÖ Synchronization results saved: lambda3_sync_results.csv\")\n",
    "\n",
    "# Generate markdown report\n",
    "report = f\"\"\"# Lambda¬≥ Weather Analysis Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Analysis Overview\n",
    "- Analysis period: 2025-06-20 to 2025-06-27\n",
    "- Location: Tokyo (35.6812¬∞N, 139.7671¬∞E)\n",
    "- Number of parameters analyzed: {len(series_names)}\n",
    "- Number of pairs analyzed: {len(all_pairs)}\n",
    "\n",
    "## Lambda¬≥ Theory Interpretation\n",
    "Meteorological phenomena are understood not as temporal causalities but as \n",
    "interactions in the semantic space of structure tensors (Œõ). High synchronization \n",
    "rates indicate structural resonance.\n",
    "\n",
    "## Top Synchronized Pairs\n",
    "| Rank | Parameter A | Parameter B | Sync Rate œÉ‚Çõ | Optimal Lag |\n",
    "|------|-------------|-------------|--------------|-------------|\n",
    "\"\"\"\n",
    "\n",
    "for i, row in sync_df.head(10).iterrows():\n",
    "    report += f\"| {i+1} | {row['series_a']} | {row['series_b']} | {row['sync_rate']:.3f} | {row['optimal_lag']} |\\n\"\n",
    "\n",
    "report += f\"\"\"\\n## Structural Insights\n",
    "The highest synchronized pair ({sync_df.iloc[0]['series_a']} ‚Üî {sync_df.iloc[0]['series_b']}) \n",
    "shows strong structural resonance with œÉ‚Çõ = {sync_df.iloc[0]['sync_rate']:.3f}.\n",
    "This suggests both parameters share ‚àÜŒõC pulsations in a common semantic space.\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "with open('lambda3_analysis_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n‚úÖ Analysis report saved: lambda3_analysis_report.md\")\n",
    "print(\"\\nüéâ Lambda¬≥ automated pair analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Results (For Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download result files if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"üì• Making result files available for download...\")\n",
    "    \n",
    "    # Files to download\n",
    "    files_to_download = [\n",
    "        'tokyo_weather_days.csv',\n",
    "        'lambda3_sync_results.csv',\n",
    "        'lambda3_analysis_report.md'\n",
    "    ]\n",
    "    \n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"  ‚úì {file}\")\n",
    "        except:\n",
    "            print(f\"  ‚úó {file} - File not found\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Running in local environment. Files are saved in current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Information\n",
    "\n",
    "### About Lambda¬≥ Theory\n",
    "- All phenomena are understood as interactions of structure tensors (Œõ)\n",
    "- No assumption of temporal causality; analyzes structural resonance in semantic space\n",
    "- ‚àÜŒõC represents structural changes (jumps)\n",
    "- œÅT (tension scalar) indicates local instability\n",
    "\n",
    "### Customization Options\n",
    "1. Change analysis period: Adjust `start_date` and `end_date`\n",
    "2. Change location: Adjust `latitude` and `longitude`\n",
    "3. Add parameters: Add to Open-Meteo API `hourly` list\n",
    "4. Adjust analysis depth: Modify `draws` and `tune` in `L3Config`\n",
    "\n",
    "### Troubleshooting\n",
    "- Memory issues: Reduce `draws` and `tune` (e.g., to 2000)\n",
    "- Long execution time: Set `max_pairs` to limit number of analyzed pairs\n",
    "- Import errors: Reinstall required libraries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
   "version": "3.8.0"\n",
   "language_info": {\n",
   "codemirror_mode": {\n",
   "name": "ipython",\n",
   "version": 3\n",
   "},\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.8.0"\n",
   "}\n",
   "nbformat": 4,\n",
   "nbformat_minor": 4\n",
   "}
