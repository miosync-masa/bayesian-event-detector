{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda³ Weather Analysis - 自動ペア分析システム\n",
    "\n",
    "このノートブックは、Lambda³理論に基づいて気象データの構造テンソル相互作用を分析します。\n",
    "\n",
    "## ワークフロー\n",
    "1. GitHubリポジトリから必要なコードを取得\n",
    "2. Open-Meteo APIから東京の気象データを取得\n",
    "3. Lambda³特徴量を抽出\n",
    "4. 全パラメータペアの自動分析を実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境セットアップと依存関係のインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install openmeteo-requests requests-cache retry-requests pymc arviz numba networkx -q\n",
    "\n",
    "print(\"✓ ライブラリのインストール完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GitHubから Lambda³ コードを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHubリポジトリをクローン\n",
    "!git clone https://github.com/your-username/bayesian-event-detector.git\n",
    "\n",
    "# ワーキングディレクトリを設定\n",
    "import os\n",
    "os.chdir('bayesian-event-detector/sample')\n",
    "\n",
    "# 必要なファイルの存在確認\n",
    "import glob\n",
    "files = glob.glob('*.py')\n",
    "print(f\"利用可能なPythonファイル: {files}\")\n",
    "\n",
    "# WeatherAnalysis.pyが存在しない場合の対処\n",
    "if 'WeatherAnalysis.py' not in files:\n",
    "    print(\"⚠️ WeatherAnalysis.pyが見つかりません。アップロードしてください。\")\n",
    "else:\n",
    "    print(\"✓ WeatherAnalysis.py 確認完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 自動ペア分析モジュールの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda3_auto_pair.py を作成\n",
    "auto_pair_code = '''# ===============================\n",
    "# Lambda³ Automatic Pair Analysis System\n",
    "# ===============================\n",
    "from itertools import combinations\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class PairAnalysisConfig:\n",
    "    \"\"\"Configuration for automatic pair analysis.\"\"\"\n",
    "    # Analysis parameters\n",
    "    analyze_all_pairs: bool = True\n",
    "    max_pairs: Optional[int] = None  # None means analyze all\n",
    "    min_series_length: int = 100\n",
    "    \n",
    "    # Pair filtering criteria\n",
    "    min_correlation: Optional[float] = None  # Filter pairs by minimum correlation\n",
    "    exclude_patterns: List[str] = field(default_factory=list)  # Patterns to exclude\n",
    "    include_only_patterns: List[str] = field(default_factory=list)  # If set, only these\n",
    "    \n",
    "    # Analysis depth\n",
    "    detailed_analysis_limit: int = 5  # Number of pairs for detailed plots\n",
    "    summary_only_after: int = 10  # Switch to summary mode after this many pairs\n",
    "    \n",
    "    # Output configuration\n",
    "    save_results: bool = True\n",
    "    output_dir: str = \"lambda3_results\"\n",
    "    generate_report: bool = True\n",
    "\n",
    "class Lambda3AutoPairAnalyzer:\n",
    "    \"\"\"Automatic pair analysis system for Lambda³ framework.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PairAnalysisConfig = None):\n",
    "        self.config = config or PairAnalysisConfig()\n",
    "        self.analysis_results = {}\n",
    "        self.pair_metadata = {}\n",
    "        \n",
    "    def detect_data_structure(self, data: Union[pd.DataFrame, Dict[str, np.ndarray]]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Automatically detect data structure and categorize columns.\"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            columns = data.columns.tolist()\n",
    "            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        else:\n",
    "            columns = list(data.keys())\n",
    "            numeric_cols = columns\n",
    "        \n",
    "        # Categorize columns based on patterns\n",
    "        categories = {\n",
    "            \\'weather\\': [],\n",
    "            \\'temperature\\': [],\n",
    "            \\'humidity\\': [],\n",
    "            \\'pressure\\': [],\n",
    "            \\'wind\\': [],\n",
    "            \\'precipitation\\': [],\n",
    "            \\'other\\': []\n",
    "        }\n",
    "        \n",
    "        patterns = {\n",
    "            \\'temperature\\': [\\'temp\\', \\'temperature\\', \\'dewpoint\\', \\'dew_point\\'],\n",
    "            \\'humidity\\': [\\'humid\\', \\'rh\\', \\'relative_humidity\\'],\n",
    "            \\'pressure\\': [\\'pressure\\', \\'press\\', \\'hpa\\', \\'mbar\\'],\n",
    "            \\'wind\\': [\\'wind\\', \\'gust\\', \\'speed\\'],\n",
    "            \\'precipitation\\': [\\'precip\\', \\'rain\\', \\'snow\\', \\'precipitation\\']\n",
    "        }\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            col_lower = col.lower()\n",
    "            categorized = False\n",
    "            \n",
    "            for category, keywords in patterns.items():\n",
    "                if any(keyword in col_lower for keyword in keywords):\n",
    "                    categories[category].append(col)\n",
    "                    categories[\\'weather\\'].append(col)\n",
    "                    categorized = True\n",
    "                    break\n",
    "            \n",
    "            if not categorized:\n",
    "                categories[\\'other\\'].append(col)\n",
    "        \n",
    "        return categories\n",
    "    \n",
    "    def generate_pair_list(self, \n",
    "                          series_dict: Dict[str, np.ndarray],\n",
    "                          categories: Optional[Dict[str, List[str]]] = None) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Generate intelligent pair list based on data structure.\"\"\"\n",
    "        all_series = list(series_dict.keys())\n",
    "        \n",
    "        # Apply include/exclude patterns\n",
    "        if self.config.include_only_patterns:\n",
    "            all_series = [s for s in all_series \n",
    "                         if any(pattern in s.lower() for pattern in self.config.include_only_patterns)]\n",
    "        \n",
    "        if self.config.exclude_patterns:\n",
    "            all_series = [s for s in all_series \n",
    "                         if not any(pattern in s.lower() for pattern in self.config.exclude_patterns)]\n",
    "        \n",
    "        # Generate all unique pairs\n",
    "        all_pairs = list(combinations(all_series, 2))\n",
    "        \n",
    "        # Apply correlation filter if specified\n",
    "        if self.config.min_correlation is not None:\n",
    "            filtered_pairs = []\n",
    "            for a, b in all_pairs:\n",
    "                corr = np.corrcoef(series_dict[a], series_dict[b])[0, 1]\n",
    "                if abs(corr) >= self.config.min_correlation:\n",
    "                    filtered_pairs.append((a, b))\n",
    "                    self.pair_metadata[(a, b)] = {\\'correlation\\': corr}\n",
    "            all_pairs = filtered_pairs\n",
    "        \n",
    "        # Apply max pairs limit\n",
    "        if self.config.max_pairs and len(all_pairs) > self.config.max_pairs:\n",
    "            print(f\"Limiting analysis to {self.config.max_pairs} pairs out of {len(all_pairs)} total\")\n",
    "            all_pairs = all_pairs[:self.config.max_pairs]\n",
    "        \n",
    "        return all_pairs\n",
    "\n",
    "# Continue with remaining methods...\n",
    "# (Full implementation would be too long for this cell)\n",
    "'''\n",
    "\n",
    "# ファイルに保存\n",
    "with open('lambda3_auto_pair.py', 'w') as f:\n",
    "    f.write(auto_pair_code)\n",
    "\n",
    "print(\"✓ lambda3_auto_pair.py 作成完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Open-Meteo APIから東京の気象データを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# APIクライアントのセットアップ\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# API呼び出しパラメータ\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "    \"latitude\": 35.6812,\n",
    "    \"longitude\": 139.7671,\n",
    "    \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "               \"precipitation\", \"wind_speed_10m\", \"surface_pressure\"],\n",
    "    \"timezone\": \"Asia/Tokyo\",\n",
    "    \"start_date\": \"2025-06-20\",\n",
    "    \"end_date\": \"2025-06-27\"\n",
    "}\n",
    "\n",
    "print(\"🌐 Open-Meteo APIからデータを取得中...\")\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# レスポンスの処理\n",
    "response = responses[0]\n",
    "print(f\"\\n📍 座標: {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"🏔️ 標高: {response.Elevation()} m\")\n",
    "print(f\"🕐 タイムゾーン: {response.Timezone()}{response.TimezoneAbbreviation()}\")\n",
    "\n",
    "# 時系列データの処理\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_wind_speed_10m = hourly.Variables(4).ValuesAsNumpy()\n",
    "hourly_surface_pressure = hourly.Variables(5).ValuesAsNumpy()\n",
    "\n",
    "# DataFrameの作成\n",
    "hourly_data = {\n",
    "    \"date\": pd.date_range(\n",
    "        start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    ),\n",
    "    \"temperature_2m\": hourly_temperature_2m,\n",
    "    \"relative_humidity_2m\": hourly_relative_humidity_2m,\n",
    "    \"dew_point_2m\": hourly_dew_point_2m,\n",
    "    \"precipitation\": hourly_precipitation,\n",
    "    \"wind_speed_10m\": hourly_wind_speed_10m,\n",
    "    \"surface_pressure\": hourly_surface_pressure\n",
    "}\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "# データの概要表示\n",
    "print(\"\\n📊 取得したデータの概要:\")\n",
    "print(hourly_dataframe.info())\n",
    "print(\"\\n🔍 最初の5行:\")\n",
    "print(hourly_dataframe.head())\n",
    "\n",
    "# CSVに保存\n",
    "hourly_dataframe.to_csv(\"tokyo_weather_days.csv\", index=False)\n",
    "print(\"\\n✅ CSV出力完了: tokyo_weather_days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lambda³ 特徴量の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WeatherAnalysis.pyをインポート\n",
    "from WeatherAnalysis import (\n",
    "    L3Config, \n",
    "    calc_lambda3_features_v2,\n",
    "    load_csv_data,\n",
    "    validate_series_lengths\n",
    ")\n",
    "\n",
    "# データの読み込み\n",
    "print(\"📂 CSVファイルからデータを読み込み中...\")\n",
    "series_dict = load_csv_data(\n",
    "    \"tokyo_weather_days.csv\",\n",
    "    time_column=\"date\",\n",
    "    value_columns=[\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "                   \"precipitation\", \"wind_speed_10m\", \"surface_pressure\"]\n",
    ")\n",
    "\n",
    "# データの検証\n",
    "series_dict = validate_series_lengths(series_dict)\n",
    "\n",
    "# Lambda³設定\n",
    "config = L3Config(\n",
    "    T=len(next(iter(series_dict.values()))),\n",
    "    draws=4000,  # Colab用に削減\n",
    "    tune=4000,\n",
    "    delta_percentile=95.0\n",
    ")\n",
    "\n",
    "print(f\"\\n⚙️ Lambda³設定:\")\n",
    "print(f\"  - 時系列長: {config.T}\")\n",
    "print(f\"  - MCMCドロー数: {config.draws}\")\n",
    "print(f\"  - ジャンプ検出閾値: {config.delta_percentile}パーセンタイル\")\n",
    "\n",
    "# 各系列の特徴量抽出\n",
    "print(\"\\n🔬 Lambda³特徴量を抽出中...\")\n",
    "features_dict = {}\n",
    "\n",
    "for name, data in series_dict.items():\n",
    "    print(f\"\\n  処理中: {name}\")\n",
    "    feats = calc_lambda3_features_v2(data, config)\n",
    "    \n",
    "    features_dict[name] = {\n",
    "        'data': data,\n",
    "        'delta_LambdaC_pos': feats[0],\n",
    "        'delta_LambdaC_neg': feats[1],\n",
    "        'rho_T': feats[2],\n",
    "        'time_trend': feats[3],\n",
    "        'local_jump': feats[4]\n",
    "    }\n",
    "    \n",
    "    # 統計情報\n",
    "    n_pos = np.sum(feats[0])\n",
    "    n_neg = np.sum(feats[1])\n",
    "    mean_tension = np.mean(feats[2])\n",
    "    \n",
    "    print(f\"    - 正の構造変化 (∆ΛC+): {n_pos}\")\n",
    "    print(f\"    - 負の構造変化 (∆ΛC-): {n_neg}\")\n",
    "    print(f\"    - 平均張力スカラー (ρT): {mean_tension:.3f}\")\n",
    "\n",
    "print(\"\\n✅ 特徴量抽出完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 自動ペア分析の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡易版の自動ペア分析実装\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# すべての可能なペアを生成\n",
    "series_names = list(series_dict.keys())\n",
    "all_pairs = list(combinations(series_names, 2))\n",
    "\n",
    "print(f\"🔗 分析するペア数: {len(all_pairs)}\")\n",
    "print(\"\\nペアリスト:\")\n",
    "for i, (a, b) in enumerate(all_pairs):\n",
    "    print(f\"  {i+1}. {a} ↔ {b}\")\n",
    "\n",
    "# 同期率の計算（簡易版）\n",
    "from WeatherAnalysis import calculate_sync_profile\n",
    "\n",
    "sync_results = {}\n",
    "print(\"\\n📊 同期率分析を実行中...\")\n",
    "\n",
    "for pair in all_pairs:\n",
    "    a, b = pair\n",
    "    \n",
    "    # 正のジャンプイベントの同期率を計算\n",
    "    sync_profile, max_sync, optimal_lag = calculate_sync_profile(\n",
    "        features_dict[a]['delta_LambdaC_pos'].astype(np.float64),\n",
    "        features_dict[b]['delta_LambdaC_pos'].astype(np.float64),\n",
    "        lag_window=10\n",
    "    )\n",
    "    \n",
    "    sync_results[pair] = {\n",
    "        'sync_rate': max_sync,\n",
    "        'optimal_lag': optimal_lag,\n",
    "        'profile': sync_profile\n",
    "    }\n",
    "    \n",
    "    print(f\"  {a} ↔ {b}: σₛ = {max_sync:.3f} (最適ラグ: {optimal_lag})\")\n",
    "\n",
    "# 同期率マトリックスの作成\n",
    "n = len(series_names)\n",
    "sync_matrix = np.zeros((n, n))\n",
    "\n",
    "for i, a in enumerate(series_names):\n",
    "    for j, b in enumerate(series_names):\n",
    "        if i == j:\n",
    "            sync_matrix[i, j] = 1.0\n",
    "        elif (a, b) in sync_results:\n",
    "            sync_matrix[i, j] = sync_results[(a, b)]['sync_rate']\n",
    "        elif (b, a) in sync_results:\n",
    "            sync_matrix[i, j] = sync_results[(b, a)]['sync_rate']\n",
    "\n",
    "# ヒートマップの表示\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sync_matrix, \n",
    "            annot=True, \n",
    "            fmt='.3f',\n",
    "            xticklabels=series_names,\n",
    "            yticklabels=series_names,\n",
    "            cmap='Blues',\n",
    "            vmin=0, vmax=1,\n",
    "            square=True,\n",
    "            cbar_kws={'label': '同期率 σₛ'})\n",
    "\n",
    "plt.title('気象パラメータ間の構造同期率マトリックス', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# トップ同期ペアの表示\n",
    "sorted_pairs = sorted(sync_results.items(), key=lambda x: x[1]['sync_rate'], reverse=True)\n",
    "\n",
    "print(\"\\n🏆 トップ5同期ペア:\")\n",
    "for i, ((a, b), data) in enumerate(sorted_pairs[:5]):\n",
    "    print(f\"  {i+1}. {a} ↔ {b}: σₛ = {data['sync_rate']:.3f} (ラグ: {data['optimal_lag']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 詳細な相互作用分析（選択的実行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最も同期率の高いペアの詳細分析\n",
    "if sorted_pairs:\n",
    "    top_pair = sorted_pairs[0][0]\n",
    "    name_a, name_b = top_pair\n",
    "    \n",
    "    print(f\"\\n🔍 最高同期ペアの詳細分析: {name_a} ↔ {name_b}\")\n",
    "    \n",
    "    from WeatherAnalysis import (\n",
    "        fit_l3_bayesian_regression_asymmetric,\n",
    "        plot_sync_profile\n",
    "    )\n",
    "    \n",
    "    # ベイズ回帰モデルの適合\n",
    "    print(\"\\n📈 ベイズ回帰モデルを適合中...\")\n",
    "    print(\"（これには数分かかる場合があります）\")\n",
    "    \n",
    "    # A に対する B の影響\n",
    "    trace_a = fit_l3_bayesian_regression_asymmetric(\n",
    "        data=features_dict[name_a]['data'],\n",
    "        features_dict={\n",
    "            'delta_LambdaC_pos': features_dict[name_a]['delta_LambdaC_pos'],\n",
    "            'delta_LambdaC_neg': features_dict[name_a]['delta_LambdaC_neg'],\n",
    "            'rho_T': features_dict[name_a]['rho_T'],\n",
    "            'time_trend': features_dict[name_a]['time_trend']\n",
    "        },\n",
    "        config=config,\n",
    "        interaction_pos=features_dict[name_b]['delta_LambdaC_pos'],\n",
    "        interaction_neg=features_dict[name_b]['delta_LambdaC_neg'],\n",
    "        interaction_rhoT=features_dict[name_b]['rho_T']\n",
    "    )\n",
    "    \n",
    "    # 結果のサマリー\n",
    "    import arviz as az\n",
    "    summary_a = az.summary(trace_a)\n",
    "    \n",
    "    print(f\"\\n📊 {name_b} → {name_a} の相互作用効果:\")\n",
    "    if 'beta_interact_pos' in summary_a.index:\n",
    "        print(f\"  正の相互作用: β = {summary_a.loc['beta_interact_pos', 'mean']:.3f}\")\n",
    "    if 'beta_interact_neg' in summary_a.index:\n",
    "        print(f\"  負の相互作用: β = {summary_a.loc['beta_interact_neg', 'mean']:.3f}\")\n",
    "    \n",
    "    # 同期プロファイルの可視化\n",
    "    plot_sync_profile(\n",
    "        sync_results[top_pair]['profile'],\n",
    "        title=f\"{name_a} ↔ {name_b} の同期プロファイル\"\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ 分析可能なペアが見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 結果の保存とレポート生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をCSVに保存\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 同期率結果のDataFrame作成\n",
    "sync_df = pd.DataFrame([\n",
    "    {\n",
    "        'series_a': pair[0],\n",
    "        'series_b': pair[1],\n",
    "        'sync_rate': data['sync_rate'],\n",
    "        'optimal_lag': data['optimal_lag']\n",
    "    }\n",
    "    for pair, data in sync_results.items()\n",
    "])\n",
    "\n",
    "# ソートして保存\n",
    "sync_df = sync_df.sort_values('sync_rate', ascending=False)\n",
    "sync_df.to_csv('lambda3_sync_results.csv', index=False)\n",
    "print(\"✅ 同期率結果を保存: lambda3_sync_results.csv\")\n",
    "\n",
    "# マークダウンレポートの生成\n",
    "report = f\"\"\"# Lambda³ Weather Analysis Report\n",
    "生成日時: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## 分析概要\n",
    "- 分析期間: 2025-06-20 〜 2025-06-27\n",
    "- 場所: 東京 (35.6812°N, 139.7671°E)\n",
    "- 分析パラメータ数: {len(series_names)}\n",
    "- 分析ペア数: {len(all_pairs)}\n",
    "\n",
    "## Lambda³ 理論による解釈\n",
    "気象現象は時間的因果性ではなく、構造テンソル（Λ）の意味空間における\n",
    "相互作用として理解されます。高い同期率は構造的共鳴を示唆します。\n",
    "\n",
    "## トップ同期ペア\n",
    "| 順位 | パラメータ A | パラメータ B | 同期率 σₛ | 最適ラグ |\n",
    "|------|-------------|-------------|----------|----------|\n",
    "\"\"\"\n",
    "\n",
    "for i, row in sync_df.head(10).iterrows():\n",
    "    report += f\"| {i+1} | {row['series_a']} | {row['series_b']} | {row['sync_rate']:.3f} | {row['optimal_lag']} |\\n\"\n",
    "\n",
    "report += f\"\"\"\\n## 構造的洞察\n",
    "最高同期ペア ({sync_df.iloc[0]['series_a']} ↔ {sync_df.iloc[0]['series_b']}) は\n",
    "σₛ = {sync_df.iloc[0]['sync_rate']:.3f} の強い構造的共鳴を示しています。\n",
    "これは両パラメータが共通の意味空間で∆ΛC pulsationを共有していることを示唆します。\n",
    "\"\"\"\n",
    "\n",
    "# レポートを保存\n",
    "with open('lambda3_analysis_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n✅ 分析レポートを保存: lambda3_analysis_report.md\")\n",
    "print(\"\\n🎉 Lambda³ 自動ペア分析が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 結果のダウンロード（Colab用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabで実行している場合、結果ファイルをダウンロード\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"📥 結果ファイルをダウンロード可能にしています...\")\n",
    "    \n",
    "    # ダウンロード可能なファイル\n",
    "    files_to_download = [\n",
    "        'tokyo_weather_days.csv',\n",
    "        'lambda3_sync_results.csv',\n",
    "        'lambda3_analysis_report.md'\n",
    "    ]\n",
    "    \n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"  ✓ {file}\")\n",
    "        except:\n",
    "            print(f\"  ✗ {file} - ファイルが見つかりません\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"ℹ️ ローカル環境で実行中です。ファイルは現在のディレクトリに保存されています。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 補足情報\n",
    "\n",
    "### Lambda³ Theory について\n",
    "- すべての現象は構造テンソル（Λ）の相互作用として理解されます\n",
    "- 時間的因果性は仮定せず、意味空間での構造的共鳴を分析します\n",
    "- ∆ΛC は構造的変化（ジャンプ）を表します\n",
    "- ρT（張力スカラー）は局所的な不安定性を示します\n",
    "\n",
    "### カスタマイズ方法\n",
    "1. 分析期間の変更: `start_date` と `end_date` を調整\n",
    "2. 場所の変更: `latitude` と `longitude` を調整\n",
    "3. パラメータの追加: Open-Meteo APIの `hourly` リストに追加\n",
    "4. 分析深度の調整: `L3Config` の `draws` と `tune` を調整\n",
    "\n",
    "### トラブルシューティング\n",
    "- メモリ不足: `draws` と `tune` を減らす（例: 2000）\n",
    "- 実行時間が長い: `max_pairs` を設定して分析ペア数を制限\n",
    "- インポートエラー: 必要なライブラリを再インストール"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}