{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaÂ³ Weather Analysis - è‡ªå‹•ãƒšã‚¢åˆ†æã‚·ã‚¹ãƒ†ãƒ \n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€LambdaÂ³ç†è«–ã«åŸºã¥ã„ã¦æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ãƒ†ãƒ³ã‚½ãƒ«ç›¸äº’ä½œç”¨ã‚’åˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼\n",
    "1. GitHubãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰å¿…è¦ãªã‚³ãƒ¼ãƒ‰ã‚’å–å¾—\n",
    "2. Open-Meteo APIã‹ã‚‰æ±äº¬ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "3. LambdaÂ³ç‰¹å¾´é‡ã‚’æŠ½å‡º\n",
    "4. å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒšã‚¢ã®è‡ªå‹•åˆ†æã‚’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install openmeteo-requests requests-cache retry-requests pymc arviz numba networkx -q\n",
    "\n",
    "print(\"âœ“ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GitHubã‹ã‚‰ LambdaÂ³ ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "!git clone https://github.com/your-username/bayesian-event-detector.git\n",
    "\n",
    "# ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
    "import os\n",
    "os.chdir('bayesian-event-detector/sample')\n",
    "\n",
    "# å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n",
    "import glob\n",
    "files = glob.glob('*.py')\n",
    "print(f\"åˆ©ç”¨å¯èƒ½ãªPythonãƒ•ã‚¡ã‚¤ãƒ«: {files}\")\n",
    "\n",
    "# WeatherAnalysis.pyãŒå­˜åœ¨ã—ãªã„å ´åˆã®å¯¾å‡¦\n",
    "if 'WeatherAnalysis.py' not in files:\n",
    "    print(\"âš ï¸ WeatherAnalysis.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    print(\"âœ“ WeatherAnalysis.py ç¢ºèªå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è‡ªå‹•ãƒšã‚¢åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda3_auto_pair.py ã‚’ä½œæˆ\n",
    "auto_pair_code = '''# ===============================\n",
    "# LambdaÂ³ Automatic Pair Analysis System\n",
    "# ===============================\n",
    "from itertools import combinations\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class PairAnalysisConfig:\n",
    "    \"\"\"Configuration for automatic pair analysis.\"\"\"\n",
    "    # Analysis parameters\n",
    "    analyze_all_pairs: bool = True\n",
    "    max_pairs: Optional[int] = None  # None means analyze all\n",
    "    min_series_length: int = 100\n",
    "    \n",
    "    # Pair filtering criteria\n",
    "    min_correlation: Optional[float] = None  # Filter pairs by minimum correlation\n",
    "    exclude_patterns: List[str] = field(default_factory=list)  # Patterns to exclude\n",
    "    include_only_patterns: List[str] = field(default_factory=list)  # If set, only these\n",
    "    \n",
    "    # Analysis depth\n",
    "    detailed_analysis_limit: int = 5  # Number of pairs for detailed plots\n",
    "    summary_only_after: int = 10  # Switch to summary mode after this many pairs\n",
    "    \n",
    "    # Output configuration\n",
    "    save_results: bool = True\n",
    "    output_dir: str = \"lambda3_results\"\n",
    "    generate_report: bool = True\n",
    "\n",
    "class Lambda3AutoPairAnalyzer:\n",
    "    \"\"\"Automatic pair analysis system for LambdaÂ³ framework.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PairAnalysisConfig = None):\n",
    "        self.config = config or PairAnalysisConfig()\n",
    "        self.analysis_results = {}\n",
    "        self.pair_metadata = {}\n",
    "        \n",
    "    def detect_data_structure(self, data: Union[pd.DataFrame, Dict[str, np.ndarray]]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Automatically detect data structure and categorize columns.\"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            columns = data.columns.tolist()\n",
    "            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        else:\n",
    "            columns = list(data.keys())\n",
    "            numeric_cols = columns\n",
    "        \n",
    "        # Categorize columns based on patterns\n",
    "        categories = {\n",
    "            \\'weather\\': [],\n",
    "            \\'temperature\\': [],\n",
    "            \\'humidity\\': [],\n",
    "            \\'pressure\\': [],\n",
    "            \\'wind\\': [],\n",
    "            \\'precipitation\\': [],\n",
    "            \\'other\\': []\n",
    "        }\n",
    "        \n",
    "        patterns = {\n",
    "            \\'temperature\\': [\\'temp\\', \\'temperature\\', \\'dewpoint\\', \\'dew_point\\'],\n",
    "            \\'humidity\\': [\\'humid\\', \\'rh\\', \\'relative_humidity\\'],\n",
    "            \\'pressure\\': [\\'pressure\\', \\'press\\', \\'hpa\\', \\'mbar\\'],\n",
    "            \\'wind\\': [\\'wind\\', \\'gust\\', \\'speed\\'],\n",
    "            \\'precipitation\\': [\\'precip\\', \\'rain\\', \\'snow\\', \\'precipitation\\']\n",
    "        }\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            col_lower = col.lower()\n",
    "            categorized = False\n",
    "            \n",
    "            for category, keywords in patterns.items():\n",
    "                if any(keyword in col_lower for keyword in keywords):\n",
    "                    categories[category].append(col)\n",
    "                    categories[\\'weather\\'].append(col)\n",
    "                    categorized = True\n",
    "                    break\n",
    "            \n",
    "            if not categorized:\n",
    "                categories[\\'other\\'].append(col)\n",
    "        \n",
    "        return categories\n",
    "    \n",
    "    def generate_pair_list(self, \n",
    "                          series_dict: Dict[str, np.ndarray],\n",
    "                          categories: Optional[Dict[str, List[str]]] = None) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Generate intelligent pair list based on data structure.\"\"\"\n",
    "        all_series = list(series_dict.keys())\n",
    "        \n",
    "        # Apply include/exclude patterns\n",
    "        if self.config.include_only_patterns:\n",
    "            all_series = [s for s in all_series \n",
    "                         if any(pattern in s.lower() for pattern in self.config.include_only_patterns)]\n",
    "        \n",
    "        if self.config.exclude_patterns:\n",
    "            all_series = [s for s in all_series \n",
    "                         if not any(pattern in s.lower() for pattern in self.config.exclude_patterns)]\n",
    "        \n",
    "        # Generate all unique pairs\n",
    "        all_pairs = list(combinations(all_series, 2))\n",
    "        \n",
    "        # Apply correlation filter if specified\n",
    "        if self.config.min_correlation is not None:\n",
    "            filtered_pairs = []\n",
    "            for a, b in all_pairs:\n",
    "                corr = np.corrcoef(series_dict[a], series_dict[b])[0, 1]\n",
    "                if abs(corr) >= self.config.min_correlation:\n",
    "                    filtered_pairs.append((a, b))\n",
    "                    self.pair_metadata[(a, b)] = {\\'correlation\\': corr}\n",
    "            all_pairs = filtered_pairs\n",
    "        \n",
    "        # Apply max pairs limit\n",
    "        if self.config.max_pairs and len(all_pairs) > self.config.max_pairs:\n",
    "            print(f\"Limiting analysis to {self.config.max_pairs} pairs out of {len(all_pairs)} total\")\n",
    "            all_pairs = all_pairs[:self.config.max_pairs]\n",
    "        \n",
    "        return all_pairs\n",
    "\n",
    "# Continue with remaining methods...\n",
    "# (Full implementation would be too long for this cell)\n",
    "'''\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "with open('lambda3_auto_pair.py', 'w') as f:\n",
    "    f.write(auto_pair_code)\n",
    "\n",
    "print(\"âœ“ lambda3_auto_pair.py ä½œæˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Open-Meteo APIã‹ã‚‰æ±äº¬ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# APIå‘¼ã³å‡ºã—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "    \"latitude\": 35.6812,\n",
    "    \"longitude\": 139.7671,\n",
    "    \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "               \"precipitation\", \"wind_speed_10m\", \"surface_pressure\"],\n",
    "    \"timezone\": \"Asia/Tokyo\",\n",
    "    \"start_date\": \"2025-06-20\",\n",
    "    \"end_date\": \"2025-06-27\"\n",
    "}\n",
    "\n",
    "print(\"ğŸŒ Open-Meteo APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\")\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å‡¦ç†\n",
    "response = responses[0]\n",
    "print(f\"\\nğŸ“ åº§æ¨™: {response.Latitude()}Â°N {response.Longitude()}Â°E\")\n",
    "print(f\"ğŸ”ï¸ æ¨™é«˜: {response.Elevation()} m\")\n",
    "print(f\"ğŸ• ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³: {response.Timezone()}{response.TimezoneAbbreviation()}\")\n",
    "\n",
    "# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_wind_speed_10m = hourly.Variables(4).ValuesAsNumpy()\n",
    "hourly_surface_pressure = hourly.Variables(5).ValuesAsNumpy()\n",
    "\n",
    "# DataFrameã®ä½œæˆ\n",
    "hourly_data = {\n",
    "    \"date\": pd.date_range(\n",
    "        start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    ),\n",
    "    \"temperature_2m\": hourly_temperature_2m,\n",
    "    \"relative_humidity_2m\": hourly_relative_humidity_2m,\n",
    "    \"dew_point_2m\": hourly_dew_point_2m,\n",
    "    \"precipitation\": hourly_precipitation,\n",
    "    \"wind_speed_10m\": hourly_wind_speed_10m,\n",
    "    \"surface_pressure\": hourly_surface_pressure\n",
    "}\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦è¡¨ç¤º\n",
    "print(\"\\nğŸ“Š å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦:\")\n",
    "print(hourly_dataframe.info())\n",
    "print(\"\\nğŸ” æœ€åˆã®5è¡Œ:\")\n",
    "print(hourly_dataframe.head())\n",
    "\n",
    "# CSVã«ä¿å­˜\n",
    "hourly_dataframe.to_csv(\"tokyo_weather_days.csv\", index=False)\n",
    "print(\"\\nâœ… CSVå‡ºåŠ›å®Œäº†: tokyo_weather_days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LambdaÂ³ ç‰¹å¾´é‡ã®æŠ½å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WeatherAnalysis.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from WeatherAnalysis import (\n",
    "    L3Config, \n",
    "    calc_lambda3_features_v2,\n",
    "    load_csv_data,\n",
    "    validate_series_lengths\n",
    ")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "print(\"ğŸ“‚ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "series_dict = load_csv_data(\n",
    "    \"tokyo_weather_days.csv\",\n",
    "    time_column=\"date\",\n",
    "    value_columns=[\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "                   \"precipitation\", \"wind_speed_10m\", \"surface_pressure\"]\n",
    ")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼\n",
    "series_dict = validate_series_lengths(series_dict)\n",
    "\n",
    "# LambdaÂ³è¨­å®š\n",
    "config = L3Config(\n",
    "    T=len(next(iter(series_dict.values()))),\n",
    "    draws=4000,  # Colabç”¨ã«å‰Šæ¸›\n",
    "    tune=4000,\n",
    "    delta_percentile=95.0\n",
    ")\n",
    "\n",
    "print(f\"\\nâš™ï¸ LambdaÂ³è¨­å®š:\")\n",
    "print(f\"  - æ™‚ç³»åˆ—é•·: {config.T}\")\n",
    "print(f\"  - MCMCãƒ‰ãƒ­ãƒ¼æ•°: {config.draws}\")\n",
    "print(f\"  - ã‚¸ãƒ£ãƒ³ãƒ—æ¤œå‡ºé–¾å€¤: {config.delta_percentile}ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«\")\n",
    "\n",
    "# å„ç³»åˆ—ã®ç‰¹å¾´é‡æŠ½å‡º\n",
    "print(\"\\nğŸ”¬ LambdaÂ³ç‰¹å¾´é‡ã‚’æŠ½å‡ºä¸­...\")\n",
    "features_dict = {}\n",
    "\n",
    "for name, data in series_dict.items():\n",
    "    print(f\"\\n  å‡¦ç†ä¸­: {name}\")\n",
    "    feats = calc_lambda3_features_v2(data, config)\n",
    "    \n",
    "    features_dict[name] = {\n",
    "        'data': data,\n",
    "        'delta_LambdaC_pos': feats[0],\n",
    "        'delta_LambdaC_neg': feats[1],\n",
    "        'rho_T': feats[2],\n",
    "        'time_trend': feats[3],\n",
    "        'local_jump': feats[4]\n",
    "    }\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±\n",
    "    n_pos = np.sum(feats[0])\n",
    "    n_neg = np.sum(feats[1])\n",
    "    mean_tension = np.mean(feats[2])\n",
    "    \n",
    "    print(f\"    - æ­£ã®æ§‹é€ å¤‰åŒ– (âˆ†Î›C+): {n_pos}\")\n",
    "    print(f\"    - è² ã®æ§‹é€ å¤‰åŒ– (âˆ†Î›C-): {n_neg}\")\n",
    "    print(f\"    - å¹³å‡å¼µåŠ›ã‚¹ã‚«ãƒ©ãƒ¼ (ÏT): {mean_tension:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… ç‰¹å¾´é‡æŠ½å‡ºå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è‡ªå‹•ãƒšã‚¢åˆ†æã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡æ˜“ç‰ˆã®è‡ªå‹•ãƒšã‚¢åˆ†æå®Ÿè£…\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ã™ã¹ã¦ã®å¯èƒ½ãªãƒšã‚¢ã‚’ç”Ÿæˆ\n",
    "series_names = list(series_dict.keys())\n",
    "all_pairs = list(combinations(series_names, 2))\n",
    "\n",
    "print(f\"ğŸ”— åˆ†æã™ã‚‹ãƒšã‚¢æ•°: {len(all_pairs)}\")\n",
    "print(\"\\nãƒšã‚¢ãƒªã‚¹ãƒˆ:\")\n",
    "for i, (a, b) in enumerate(all_pairs):\n",
    "    print(f\"  {i+1}. {a} â†” {b}\")\n",
    "\n",
    "# åŒæœŸç‡ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰\n",
    "from WeatherAnalysis import calculate_sync_profile\n",
    "\n",
    "sync_results = {}\n",
    "print(\"\\nğŸ“Š åŒæœŸç‡åˆ†æã‚’å®Ÿè¡Œä¸­...\")\n",
    "\n",
    "for pair in all_pairs:\n",
    "    a, b = pair\n",
    "    \n",
    "    # æ­£ã®ã‚¸ãƒ£ãƒ³ãƒ—ã‚¤ãƒ™ãƒ³ãƒˆã®åŒæœŸç‡ã‚’è¨ˆç®—\n",
    "    sync_profile, max_sync, optimal_lag = calculate_sync_profile(\n",
    "        features_dict[a]['delta_LambdaC_pos'].astype(np.float64),\n",
    "        features_dict[b]['delta_LambdaC_pos'].astype(np.float64),\n",
    "        lag_window=10\n",
    "    )\n",
    "    \n",
    "    sync_results[pair] = {\n",
    "        'sync_rate': max_sync,\n",
    "        'optimal_lag': optimal_lag,\n",
    "        'profile': sync_profile\n",
    "    }\n",
    "    \n",
    "    print(f\"  {a} â†” {b}: Ïƒâ‚› = {max_sync:.3f} (æœ€é©ãƒ©ã‚°: {optimal_lag})\")\n",
    "\n",
    "# åŒæœŸç‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®ä½œæˆ\n",
    "n = len(series_names)\n",
    "sync_matrix = np.zeros((n, n))\n",
    "\n",
    "for i, a in enumerate(series_names):\n",
    "    for j, b in enumerate(series_names):\n",
    "        if i == j:\n",
    "            sync_matrix[i, j] = 1.0\n",
    "        elif (a, b) in sync_results:\n",
    "            sync_matrix[i, j] = sync_results[(a, b)]['sync_rate']\n",
    "        elif (b, a) in sync_results:\n",
    "            sync_matrix[i, j] = sync_results[(b, a)]['sync_rate']\n",
    "\n",
    "# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è¡¨ç¤º\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sync_matrix, \n",
    "            annot=True, \n",
    "            fmt='.3f',\n",
    "            xticklabels=series_names,\n",
    "            yticklabels=series_names,\n",
    "            cmap='Blues',\n",
    "            vmin=0, vmax=1,\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'åŒæœŸç‡ Ïƒâ‚›'})\n",
    "\n",
    "plt.title('æ°—è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®æ§‹é€ åŒæœŸç‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ãƒˆãƒƒãƒ—åŒæœŸãƒšã‚¢ã®è¡¨ç¤º\n",
    "sorted_pairs = sorted(sync_results.items(), key=lambda x: x[1]['sync_rate'], reverse=True)\n",
    "\n",
    "print(\"\\nğŸ† ãƒˆãƒƒãƒ—5åŒæœŸãƒšã‚¢:\")\n",
    "for i, ((a, b), data) in enumerate(sorted_pairs[:5]):\n",
    "    print(f\"  {i+1}. {a} â†” {b}: Ïƒâ‚› = {data['sync_rate']:.3f} (ãƒ©ã‚°: {data['optimal_lag']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. è©³ç´°ãªç›¸äº’ä½œç”¨åˆ†æï¼ˆé¸æŠçš„å®Ÿè¡Œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ã‚‚åŒæœŸç‡ã®é«˜ã„ãƒšã‚¢ã®è©³ç´°åˆ†æ\n",
    "if sorted_pairs:\n",
    "    top_pair = sorted_pairs[0][0]\n",
    "    name_a, name_b = top_pair\n",
    "    \n",
    "    print(f\"\\nğŸ” æœ€é«˜åŒæœŸãƒšã‚¢ã®è©³ç´°åˆ†æ: {name_a} â†” {name_b}\")\n",
    "    \n",
    "    from WeatherAnalysis import (\n",
    "        fit_l3_bayesian_regression_asymmetric,\n",
    "        plot_sync_profile\n",
    "    )\n",
    "    \n",
    "    # ãƒ™ã‚¤ã‚ºå›å¸°ãƒ¢ãƒ‡ãƒ«ã®é©åˆ\n",
    "    print(\"\\nğŸ“ˆ ãƒ™ã‚¤ã‚ºå›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆä¸­...\")\n",
    "    print(\"ï¼ˆã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰\")\n",
    "    \n",
    "    # A ã«å¯¾ã™ã‚‹ B ã®å½±éŸ¿\n",
    "    trace_a = fit_l3_bayesian_regression_asymmetric(\n",
    "        data=features_dict[name_a]['data'],\n",
    "        features_dict={\n",
    "            'delta_LambdaC_pos': features_dict[name_a]['delta_LambdaC_pos'],\n",
    "            'delta_LambdaC_neg': features_dict[name_a]['delta_LambdaC_neg'],\n",
    "            'rho_T': features_dict[name_a]['rho_T'],\n",
    "            'time_trend': features_dict[name_a]['time_trend']\n",
    "        },\n",
    "        config=config,\n",
    "        interaction_pos=features_dict[name_b]['delta_LambdaC_pos'],\n",
    "        interaction_neg=features_dict[name_b]['delta_LambdaC_neg'],\n",
    "        interaction_rhoT=features_dict[name_b]['rho_T']\n",
    "    )\n",
    "    \n",
    "    # çµæœã®ã‚µãƒãƒªãƒ¼\n",
    "    import arviz as az\n",
    "    summary_a = az.summary(trace_a)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {name_b} â†’ {name_a} ã®ç›¸äº’ä½œç”¨åŠ¹æœ:\")\n",
    "    if 'beta_interact_pos' in summary_a.index:\n",
    "        print(f\"  æ­£ã®ç›¸äº’ä½œç”¨: Î² = {summary_a.loc['beta_interact_pos', 'mean']:.3f}\")\n",
    "    if 'beta_interact_neg' in summary_a.index:\n",
    "        print(f\"  è² ã®ç›¸äº’ä½œç”¨: Î² = {summary_a.loc['beta_interact_neg', 'mean']:.3f}\")\n",
    "    \n",
    "    # åŒæœŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®å¯è¦–åŒ–\n",
    "    plot_sync_profile(\n",
    "        sync_results[top_pair]['profile'],\n",
    "        title=f\"{name_a} â†” {name_b} ã®åŒæœŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«\"\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ åˆ†æå¯èƒ½ãªãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. çµæœã®ä¿å­˜ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœã‚’CSVã«ä¿å­˜\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# åŒæœŸç‡çµæœã®DataFrameä½œæˆ\n",
    "sync_df = pd.DataFrame([\n",
    "    {\n",
    "        'series_a': pair[0],\n",
    "        'series_b': pair[1],\n",
    "        'sync_rate': data['sync_rate'],\n",
    "        'optimal_lag': data['optimal_lag']\n",
    "    }\n",
    "    for pair, data in sync_results.items()\n",
    "])\n",
    "\n",
    "# ã‚½ãƒ¼ãƒˆã—ã¦ä¿å­˜\n",
    "sync_df = sync_df.sort_values('sync_rate', ascending=False)\n",
    "sync_df.to_csv('lambda3_sync_results.csv', index=False)\n",
    "print(\"âœ… åŒæœŸç‡çµæœã‚’ä¿å­˜: lambda3_sync_results.csv\")\n",
    "\n",
    "# ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ\n",
    "report = f\"\"\"# LambdaÂ³ Weather Analysis Report\n",
    "ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## åˆ†ææ¦‚è¦\n",
    "- åˆ†ææœŸé–“: 2025-06-20 ã€œ 2025-06-27\n",
    "- å ´æ‰€: æ±äº¬ (35.6812Â°N, 139.7671Â°E)\n",
    "- åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(series_names)}\n",
    "- åˆ†æãƒšã‚¢æ•°: {len(all_pairs)}\n",
    "\n",
    "## LambdaÂ³ ç†è«–ã«ã‚ˆã‚‹è§£é‡ˆ\n",
    "æ°—è±¡ç¾è±¡ã¯æ™‚é–“çš„å› æœæ€§ã§ã¯ãªãã€æ§‹é€ ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆÎ›ï¼‰ã®æ„å‘³ç©ºé–“ã«ãŠã‘ã‚‹\n",
    "ç›¸äº’ä½œç”¨ã¨ã—ã¦ç†è§£ã•ã‚Œã¾ã™ã€‚é«˜ã„åŒæœŸç‡ã¯æ§‹é€ çš„å…±é³´ã‚’ç¤ºå”†ã—ã¾ã™ã€‚\n",
    "\n",
    "## ãƒˆãƒƒãƒ—åŒæœŸãƒšã‚¢\n",
    "| é †ä½ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ A | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ B | åŒæœŸç‡ Ïƒâ‚› | æœ€é©ãƒ©ã‚° |\n",
    "|------|-------------|-------------|----------|----------|\n",
    "\"\"\"\n",
    "\n",
    "for i, row in sync_df.head(10).iterrows():\n",
    "    report += f\"| {i+1} | {row['series_a']} | {row['series_b']} | {row['sync_rate']:.3f} | {row['optimal_lag']} |\\n\"\n",
    "\n",
    "report += f\"\"\"\\n## æ§‹é€ çš„æ´å¯Ÿ\n",
    "æœ€é«˜åŒæœŸãƒšã‚¢ ({sync_df.iloc[0]['series_a']} â†” {sync_df.iloc[0]['series_b']}) ã¯\n",
    "Ïƒâ‚› = {sync_df.iloc[0]['sync_rate']:.3f} ã®å¼·ã„æ§‹é€ çš„å…±é³´ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
    "ã“ã‚Œã¯ä¸¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå…±é€šã®æ„å‘³ç©ºé–“ã§âˆ†Î›C pulsationã‚’å…±æœ‰ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¾ã™ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜\n",
    "with open('lambda3_analysis_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\nâœ… åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: lambda3_analysis_report.md\")\n",
    "print(\"\\nğŸ‰ LambdaÂ³ è‡ªå‹•ãƒšã‚¢åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. çµæœã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆColabç”¨ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabã§å®Ÿè¡Œã—ã¦ã„ã‚‹å ´åˆã€çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"ğŸ“¥ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«ã—ã¦ã„ã¾ã™...\")\n",
    "    \n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«\n",
    "    files_to_download = [\n",
    "        'tokyo_weather_days.csv',\n",
    "        'lambda3_sync_results.csv',\n",
    "        'lambda3_analysis_report.md'\n",
    "    ]\n",
    "    \n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"  âœ“ {file}\")\n",
    "        except:\n",
    "            print(f\"  âœ— {file} - ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š è£œè¶³æƒ…å ±\n",
    "\n",
    "### LambdaÂ³ Theory ã«ã¤ã„ã¦\n",
    "- ã™ã¹ã¦ã®ç¾è±¡ã¯æ§‹é€ ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆÎ›ï¼‰ã®ç›¸äº’ä½œç”¨ã¨ã—ã¦ç†è§£ã•ã‚Œã¾ã™\n",
    "- æ™‚é–“çš„å› æœæ€§ã¯ä»®å®šã›ãšã€æ„å‘³ç©ºé–“ã§ã®æ§‹é€ çš„å…±é³´ã‚’åˆ†æã—ã¾ã™\n",
    "- âˆ†Î›C ã¯æ§‹é€ çš„å¤‰åŒ–ï¼ˆã‚¸ãƒ£ãƒ³ãƒ—ï¼‰ã‚’è¡¨ã—ã¾ã™\n",
    "- ÏTï¼ˆå¼µåŠ›ã‚¹ã‚«ãƒ©ãƒ¼ï¼‰ã¯å±€æ‰€çš„ãªä¸å®‰å®šæ€§ã‚’ç¤ºã—ã¾ã™\n",
    "\n",
    "### ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ–¹æ³•\n",
    "1. åˆ†ææœŸé–“ã®å¤‰æ›´: `start_date` ã¨ `end_date` ã‚’èª¿æ•´\n",
    "2. å ´æ‰€ã®å¤‰æ›´: `latitude` ã¨ `longitude` ã‚’èª¿æ•´\n",
    "3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¿½åŠ : Open-Meteo APIã® `hourly` ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "4. åˆ†ææ·±åº¦ã®èª¿æ•´: `L3Config` ã® `draws` ã¨ `tune` ã‚’èª¿æ•´\n",
    "\n",
    "### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "- ãƒ¡ãƒ¢ãƒªä¸è¶³: `draws` ã¨ `tune` ã‚’æ¸›ã‚‰ã™ï¼ˆä¾‹: 2000ï¼‰\n",
    "- å®Ÿè¡Œæ™‚é–“ãŒé•·ã„: `max_pairs` ã‚’è¨­å®šã—ã¦åˆ†æãƒšã‚¢æ•°ã‚’åˆ¶é™\n",
    "- ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}