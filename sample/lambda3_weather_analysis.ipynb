{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda¬≥ Weather Analysis - Automated Pair Analysis System\n",
    "\n",
    "This notebook analyzes meteorological data structure tensor interactions based on Lambda¬≥ theory.\n",
    "\n",
    "## Workflow\n",
    "1. Retrieve necessary code from GitHub repository\n",
    "2. Fetch Tokyo weather data from Open-Meteo API\n",
    "3. Extract Lambda¬≥ features\n",
    "4. Execute automated analysis for all parameter pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install openmeteo-requests requests-cache retry-requests pymc arviz numba networkx -q\n",
    "\n",
    "print(\"‚úì Library installation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve Lambda¬≥ Code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone GitHub repository\n",
    "!git clone https://github.com/miosync-masa/bayesian-event-detector.git\n",
    "\n",
    "# Set working directory\n",
    "import os\n",
    "os.chdir('bayesian-event-detector/sample')\n",
    "\n",
    "# Check for available files\n",
    "import glob\n",
    "files = glob.glob('*.py')\n",
    "print(f\"Available Python files: {files}\")\n",
    "\n",
    "# Handle case where WeatherAnalysis.py doesn't exist\n",
    "if 'WeatherAnalysis.py' not in files:\n",
    "    print(\"‚ö†Ô∏è WeatherAnalysis_tokyo.py not found. Please upload it.\")\n",
    "else:\n",
    "    print(\"‚úì WeatherAnalysis.py confirmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Automated Pair Analysis Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lambda3_auto_pair.py\n",
    "auto_pair_code = '''# ===============================\n",
    "# Lambda¬≥ Automatic Pair Analysis System\n",
    "# ===============================\n",
    "from itertools import combinations\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class PairAnalysisConfig:\n",
    "    \"\"\"Configuration for automatic pair analysis.\"\"\"\n",
    "    # Analysis parameters\n",
    "    analyze_all_pairs: bool = True\n",
    "    max_pairs: Optional[int] = None  # None means analyze all\n",
    "    min_series_length: int = 100\n",
    "    \n",
    "    # Pair filtering criteria\n",
    "    min_correlation: Optional[float] = None  # Filter pairs by minimum correlation\n",
    "    exclude_patterns: List[str] = field(default_factory=list)  # Patterns to exclude\n",
    "    include_only_patterns: List[str] = field(default_factory=list)  # If set, only these\n",
    "    \n",
    "    # Analysis depth\n",
    "    detailed_analysis_limit: int = 5  # Number of pairs for detailed plots\n",
    "    summary_only_after: int = 10  # Switch to summary mode after this many pairs\n",
    "    \n",
    "    # Output configuration\n",
    "    save_results: bool = True\n",
    "    output_dir: str = \"lambda3_results\"\n",
    "    generate_report: bool = True\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open('lambda3_auto_pair.py', 'w') as f:\n",
    "    f.write(auto_pair_code)\n",
    "\n",
    "print(\"‚úì lambda3_auto_pair.py created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetch Tokyo Weather Data from Open-Meteo API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup API client\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# API call parameters\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "    \"latitude\": 35.6812,\n",
    "    \"longitude\": 139.7671,\n",
    "    \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "               \"precipitation\", \"wind_speed_10m\", \"surface_pressure\"],\n",
    "    \"timezone\": \"Asia/Tokyo\",\n",
    "    \"start_date\": \"2025-06-20\",\n",
    "    \"end_date\": \"2025-06-27\"\n",
    "}\n",
    "\n",
    "print(\"üåê Fetching data from Open-Meteo API...\")\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process response\n",
    "response = responses[0]\n",
    "print(f\"\\nüìç Coordinates: {response.Latitude()}¬∞N {response.Longitude()}¬∞E\")\n",
    "print(f\"üèîÔ∏è Elevation: {response.Elevation()} m\")\n",
    "print(f\"üïê Timezone: {response.Timezone()}{response.TimezoneAbbreviation()}\")\n",
    "\n",
    "# Process time series data\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_wind_speed_10m = hourly.Variables(4).ValuesAsNumpy()\n",
    "hourly_surface_pressure = hourly.Variables(5).ValuesAsNumpy()\n",
    "\n",
    "# Create DataFrame\n",
    "hourly_data = {\n",
    "    \"date\": pd.date_range(\n",
    "        start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    ),\n",
    "    \"temperature_2m\": hourly_temperature_2m,\n",
    "    \"relative_humidity_2m\": hourly_relative_humidity_2m,\n",
    "    \"dew_point_2m\": hourly_dew_point_2m,\n",
    "    \"precipitation\": hourly_precipitation,\n",
    "    \"wind_speed_10m\": hourly_wind_speed_10m,\n",
    "    \"surface_pressure\": hourly_surface_pressure\n",
    "}\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "# Display data summary\n",
    "print(\"\\nüìä Data overview:\")\n",
    "print(hourly_dataframe.info())\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "print(hourly_dataframe.head())\n",
    "\n",
    "# Save to CSV\n",
    "hourly_dataframe.to_csv(\"tokyo_weather_days.csv\", index=False)\n",
    "print(\"\\n‚úÖ CSV output completed: tokyo_weather_days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Lambda¬≥ Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy first\n",
    "import numpy as np\n",
    "\n",
    "# Import WeatherAnalysis.py\n",
    "try:\n",
    "    from WeatherAnalysis import (\n",
    "        L3Config, \n",
    "        calc_lambda3_features_v2,\n",
    "        load_csv_data,\n",
    "        validate_series_lengths\n",
    "    )\n",
    "    print(\"‚úì WeatherAnalysis module imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Import error: {e}\")\n",
    "    print(\"Please ensure WeatherAnalysis.py is in the current directory\")\n",
    "    \n",
    "# Load data\n",
    "print(\"üìÇ Loading data from CSV file...\")\n",
    "series_dict = load_csv_data(\n",
    "    \"tokyo_weather_days.csv\",\n",
    "    time_column=\"date\",\n",
    "    value_columns=[\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "                   \"precipitation\", \"wind_speed_10m\", \"surface_pressure\"]\n",
    ")\n",
    "\n",
    "# Validate data\n",
    "series_dict = validate_series_lengths(series_dict)\n",
    "\n",
    "# Lambda¬≥ configuration\n",
    "config = L3Config(\n",
    "    T=len(next(iter(series_dict.values()))),\n",
    "    draws=4000,  # Reduced for Colab\n",
    "    tune=4000,\n",
    "    delta_percentile=95.0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Lambda¬≥ Configuration:\")\n",
    "print(f\"  - Time series length: {config.T}\")\n",
    "print(f\"  - MCMC draws: {config.draws}\")\n",
    "print(f\"  - Jump detection threshold: {config.delta_percentile}th percentile\")\n",
    "\n",
    "# Extract features for each series\n",
    "print(\"\\nüî¨ Extracting Lambda¬≥ features...\")\n",
    "features_dict = {}\n",
    "\n",
    "for name, data in series_dict.items():\n",
    "    print(f\"\\n  Processing: {name}\")\n",
    "    feats = calc_lambda3_features_v2(data, config)\n",
    "    \n",
    "    features_dict[name] = {\n",
    "        'data': data,\n",
    "        'delta_LambdaC_pos': feats[0],\n",
    "        'delta_LambdaC_neg': feats[1],\n",
    "        'rho_T': feats[2],\n",
    "        'time_trend': feats[3],\n",
    "        'local_jump': feats[4]\n",
    "    }\n",
    "    \n",
    "    # Statistics\n",
    "    n_pos = np.sum(feats[0])\n",
    "    n_neg = np.sum(feats[1])\n",
    "    mean_tension = np.mean(feats[2])\n",
    "    \n",
    "    print(f\"    - Positive structural changes (‚àÜŒõC+): {n_pos}\")\n",
    "    print(f\"    - Negative structural changes (‚àÜŒõC-): {n_neg}\")\n",
    "    print(f\"    - Mean tension scalar (œÅT): {mean_tension:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature extraction completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute Automated Pair Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple automated pair analysis implementation\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate all possible pairs\n",
    "series_names = list(series_dict.keys())\n",
    "all_pairs = list(combinations(series_names, 2))\n",
    "\n",
    "print(f\"üîó Number of pairs to analyze: {len(all_pairs)}\")\n",
    "print(\"\\nPair list:\")\n",
    "for i, (a, b) in enumerate(all_pairs):\n",
    "    print(f\"  {i+1}. {a} ‚Üî {b}\")\n",
    "\n",
    "# Calculate synchronization rates (simplified version)\n",
    "from WeatherAnalysis import calculate_sync_profile\n",
    "\n",
    "sync_results = {}\n",
    "print(\"\\nüìä Executing synchronization rate analysis...\")\n",
    "\n",
    "for pair in all_pairs:\n",
    "    a, b = pair\n",
    "    \n",
    "    # Calculate sync rate for positive jump events\n",
    "    sync_profile, max_sync, optimal_lag = calculate_sync_profile(\n",
    "        features_dict[a]['delta_LambdaC_pos'].astype(np.float64),\n",
    "        features_dict[b]['delta_LambdaC_pos'].astype(np.float64),\n",
    "        lag_window=10\n",
    "    )\n",
    "    \n",
    "    sync_results[pair] = {\n",
    "        'sync_rate': max_sync,\n",
    "        'optimal_lag': optimal_lag,\n",
    "        'profile': sync_profile\n",
    "    }\n",
    "    \n",
    "    print(f\"  {a} ‚Üî {b}: œÉ‚Çõ = {max_sync:.3f} (optimal lag: {optimal_lag})\")\n",
    "\n",
    "# Create synchronization rate matrix\n",
    "n = len(series_names)\n",
    "sync_matrix = np.zeros((n, n))\n",
    "\n",
    "for i, a in enumerate(series_names):\n",
    "    for j, b in enumerate(series_names):\n",
    "        if i == j:\n",
    "            sync_matrix[i, j] = 1.0\n",
    "        elif (a, b) in sync_results:\n",
    "            sync_matrix[i, j] = sync_results[(a, b)]['sync_rate']\n",
    "        elif (b, a) in sync_results:\n",
    "            sync_matrix[i, j] = sync_results[(b, a)]['sync_rate']\n",
    "\n",
    "# Display heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sync_matrix, \n",
    "            annot=True, \n",
    "            fmt='.3f',\n",
    "            xticklabels=series_names,\n",
    "            yticklabels=series_names,\n",
    "            cmap='Blues',\n",
    "            vmin=0, vmax=1,\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Synchronization Rate œÉ‚Çõ'})\n",
    "\n",
    "plt.title('Structural Synchronization Matrix Between Weather Parameters', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top synchronized pairs\n",
    "sorted_pairs = sorted(sync_results.items(), key=lambda x: x[1]['sync_rate'], reverse=True)\n",
    "\n",
    "print(\"\\nüèÜ Top 5 synchronized pairs:\")\n",
    "for i, ((a, b), data) in enumerate(sorted_pairs[:5]):\n",
    "    print(f\"  {i+1}. {a} ‚Üî {b}: œÉ‚Çõ = {data['sync_rate']:.3f} (lag: {data['optimal_lag']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create DataFrame from synchronization results\n",
    "sync_df = pd.DataFrame([\n",
    "    {\n",
    "        'series_a': pair[0],\n",
    "        'series_b': pair[1],\n",
    "        'sync_rate': data['sync_rate'],\n",
    "        'optimal_lag': data['optimal_lag']\n",
    "    }\n",
    "    for pair, data in sync_results.items()\n",
    "])\n",
    "\n",
    "# Sort and save\n",
    "sync_df = sync_df.sort_values('sync_rate', ascending=False)\n",
    "sync_df.to_csv('lambda3_sync_results.csv', index=False)\n",
    "print(\"‚úÖ Synchronization results saved: lambda3_sync_results.csv\")\n",
    "\n",
    "# Generate markdown report\n",
    "report = f\"\"\"# Lambda¬≥ Weather Analysis Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Analysis Overview\n",
    "- Analysis period: 2025-06-20 to 2025-06-27\n",
    "- Location: Tokyo (35.6812¬∞N, 139.7671¬∞E)\n",
    "- Number of parameters analyzed: {len(series_names)}\n",
    "- Number of pairs analyzed: {len(all_pairs)}\n",
    "\n",
    "## Lambda¬≥ Theory Interpretation\n",
    "Meteorological phenomena are understood not as temporal causalities but as \n",
    "interactions in the semantic space of structure tensors (Œõ). High synchronization \n",
    "rates indicate structural resonance.\n",
    "\n",
    "## Top Synchronized Pairs\n",
    "| Rank | Parameter A | Parameter B | Sync Rate œÉ‚Çõ | Optimal Lag |\n",
    "|------|-------------|-------------|--------------|-------------|\n",
    "\"\"\"\n",
    "\n",
    "for i, row in sync_df.head(10).iterrows():\n",
    "    report += f\"| {i+1} | {row['series_a']} | {row['series_b']} | {row['sync_rate']:.3f} | {row['optimal_lag']} |\\n\"\n",
    "\n",
    "report += f\"\"\"\\n## Structural Insights\n",
    "The highest synchronized pair ({sync_df.iloc[0]['series_a']} ‚Üî {sync_df.iloc[0]['series_b']}) \n",
    "shows strong structural resonance with œÉ‚Çõ = {sync_df.iloc[0]['sync_rate']:.3f}.\n",
    "This suggests both parameters share ‚àÜŒõC pulsations in a common semantic space.\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "with open('lambda3_analysis_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n‚úÖ Analysis report saved: lambda3_analysis_report.md\")\n",
    "print(\"\\nüéâ Lambda¬≥ automated pair analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Results (For Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download result files if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"üì• Making result files available for download...\")\n",
    "    \n",
    "    # Files to download\n",
    "    files_to_download = [\n",
    "        'tokyo_weather_days.csv',\n",
    "        'lambda3_sync_results.csv',\n",
    "        'lambda3_analysis_report.md'\n",
    "    ]\n",
    "    \n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"  ‚úì {file}\")\n",
    "        except:\n",
    "            print(f\"  ‚úó {file} - File not found\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Running in local environment. Files are saved in current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Information\n",
    "\n",
    "### About Lambda¬≥ Theory\n",
    "- All phenomena are understood as interactions of structure tensors (Œõ)\n",
    "- No assumption of temporal causality; analyzes structural resonance in semantic space\n",
    "- ‚àÜŒõC represents structural changes (jumps)\n",
    "- œÅT (tension scalar) indicates local instability\n",
    "\n",
    "### Customization Options\n",
    "1. Change analysis period: Adjust `start_date` and `end_date`\n",
    "2. Change location: Adjust `latitude` and `longitude`\n",
    "3. Add parameters: Add to Open-Meteo API `hourly` list\n",
    "4. Adjust analysis depth: Modify `draws` and `tune` in `L3Config`\n",
    "\n",
    "### Troubleshooting\n",
    "- Memory issues: Reduce `draws` and `tune` (e.g., to 2000)\n",
    "- Long execution time: Set `max_pairs` to limit number of analyzed pairs\n",
    "- Import errors: Reinstall required libraries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
