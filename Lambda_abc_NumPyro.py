# ==========================================================
# Î›Â³NumPyro: LambdaÂ³ Analytics GPU-Accelerated Edition
# ----------------------------------------------------
# Complete NumPyro port of LambdaÂ³ ABC framework
# Author: Mamichi Iizumi (Miosync, Inc.)
# License: MIT
# ----------------------------------------------------

import jax
import jax.numpy as jnp
import jax.random as random
import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS, HMC, Predictive
from numpyro.infer.autoguide import AutoLowRankMultivariateNormal
from numpyro.infer.svi import SVI
from numpyro.optim import Adam

from functools import partial
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any
import networkx as nx
from concurrent.futures import ProcessPoolExecutor, as_completed
import os
import warnings
from pathlib import Path
import time

# GPUå¼·åˆ¶è¨­å®š (Lambda3æ§‹é€ ç©ºé–“ã§ã®æ¼”ç®—åŠ é€Ÿ)
jax.config.update("jax_platform_name", "gpu")
jax.config.update("jax_enable_x64", True)
print(f"JAX devices: {jax.devices()}")
print(f"JAX backend: {jax.default_backend()}")

# YFinance for data fetching
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False
    print("Warning: yfinance not available. Install with: pip install yfinance")

# ===============================
# LambdaÂ³ Configuration
# ===============================
@dataclass
class L3ConfigNumPyro:
    """NumPyroç‰ˆLambdaÂ³è§£æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š"""
    T: int = 150  # æ™‚ç³»åˆ—é•·
    # ç‰¹å¾´æŠ½å‡ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    window: int = 10
    local_window: int = 10
    delta_percentile: float = 97.0
    local_jump_percentile: float = 97.0
    # ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    num_samples: int = 8000  # MCMCã‚µãƒ³ãƒ—ãƒ«æ•°
    num_warmup: int = 8000   # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    num_chains: int = 4      # MCMCãƒã‚§ãƒ¼ãƒ³æ•°
    target_accept_prob: float = 0.95
    max_tree_depth: int = 10
    # ä¸¦åˆ—åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    max_workers: int = 3     # Colabå¯¾å¿œ
    # å¯è¦–åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    hdi_prob: float = 0.94   # ä¿¡é ¼åŒºé–“

# ===============================
# JAX-Compiled Feature Extraction
# ===============================
@jax.jit
def calculate_diff_threshold_jax(data: jnp.ndarray, percentile: float) -> Tuple[jnp.ndarray, float]:
    """JAXæœ€é©åŒ–ã•ã‚ŒãŸå·®åˆ†ãƒ»é–¾å€¤è¨ˆç®—"""
    diff = jnp.diff(data, prepend=data[0])
    abs_diff = jnp.abs(diff)
    threshold = jnp.percentile(abs_diff, percentile)
    return diff, threshold

@jax.jit
def detect_jumps_jax(diff: jnp.ndarray, threshold: float) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """JAXæœ€é©åŒ–ã•ã‚ŒãŸã‚¸ãƒ£ãƒ³ãƒ—æ¤œå‡º"""
    pos_jumps = (diff > threshold).astype(jnp.float32)
    neg_jumps = (diff < -threshold).astype(jnp.float32)
    return pos_jumps, neg_jumps

@jax.jit
def calculate_local_std_jax(data: jnp.ndarray, window: int) -> jnp.ndarray:
    """JAXæœ€é©åŒ–ã•ã‚ŒãŸå±€æ‰€æ¨™æº–åå·®è¨ˆç®—ï¼ˆrolling windowç‰ˆï¼‰"""
    n = len(data)
    
    def compute_std_at_position(i):
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’è¨ˆç®—
        start_idx = jnp.maximum(0, i - window // 2)
        end_idx = jnp.minimum(n, i + window // 2 + 1)
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©²å½“éƒ¨åˆ†ã‚’æ¡ä»¶ä»˜ãã§é¸æŠ
        indices = jnp.arange(n)
        mask = (indices >= start_idx) & (indices < end_idx)
        
        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã¨åˆ†æ•£ã‚’è¨ˆç®—
        masked_data = jnp.where(mask, data, 0.0)
        count = jnp.sum(mask)
        
        # ã‚¼ãƒ­é™¤ç®—å›é¿
        safe_count = jnp.maximum(count, 1.0)
        mean_val = jnp.sum(masked_data) / safe_count
        
        # åˆ†æ•£è¨ˆç®—
        squared_diff = jnp.where(mask, (data - mean_val) ** 2, 0.0)
        variance = jnp.sum(squared_diff) / safe_count
        
        return jnp.sqrt(variance)
    
    return jax.vmap(compute_std_at_position)(jnp.arange(n))

@jax.jit  
def calculate_rho_t_jax(data: jnp.ndarray, window: int) -> jnp.ndarray:
    """JAXãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆÏTï¼‰è¨ˆç®—ï¼ˆç´¯ç©ç‰ˆï¼‰"""
    n = len(data)
    
    def compute_rho_at_position(i):
        # ç¾åœ¨ä½ç½®ã‹ã‚‰éå»windowã‚µãƒ³ãƒ—ãƒ«ã¾ã§
        start_idx = jnp.maximum(0, i - window + 1)
        end_idx = i + 1
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒã‚¹ã‚¯
        indices = jnp.arange(n)
        mask = (indices >= start_idx) & (indices < end_idx)
        
        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆè¨ˆç®—
        masked_data = jnp.where(mask, data, 0.0)
        count = jnp.sum(mask)
        safe_count = jnp.maximum(count, 1.0)
        
        mean_val = jnp.sum(masked_data) / safe_count
        squared_diff = jnp.where(mask, (data - mean_val) ** 2, 0.0)
        variance = jnp.sum(squared_diff) / safe_count
        
        return jnp.sqrt(variance)
    
    return jax.vmap(compute_rho_at_position)(jnp.arange(n))

def extract_lambda3_features_jax(data: jnp.ndarray, config: L3ConfigNumPyro) -> Dict[str, jnp.ndarray]:
    """LambdaÂ³ç‰¹å¾´é‡æŠ½å‡ºï¼ˆJAXæœ€é©åŒ–ç‰ˆï¼‰- å®‰å®šåŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³"""
    
    # ãƒ‡ãƒ¼ã‚¿å‹ç¢ºä¿
    data = jnp.asarray(data, dtype=jnp.float32)
    
    # 1. æ§‹é€ å¤‰åŒ–ï¼ˆÎ”Î›Cï¼‰æ¤œå‡º
    diff, threshold = calculate_diff_threshold_jax(data, config.delta_percentile)
    delta_pos, delta_neg = detect_jumps_jax(diff, threshold)
    
    # 2. å±€æ‰€æ¨™æº–åå·®ï¼ˆå®‰å®šåŒ–ç‰ˆï¼‰
    try:
        local_std = calculate_local_std_jax(data, config.local_window)
    except Exception as e:
        print(f"Local std calculation failed, using simple version: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå˜ç´”ãªç§»å‹•å¹³å‡ãƒ™ãƒ¼ã‚¹
        local_std = jnp.ones_like(data) * jnp.std(data)
    
    # 3. å±€æ‰€ã‚¸ãƒ£ãƒ³ãƒ—æ¤œå‡º
    score = jnp.abs(diff) / (local_std + 1e-6)  # æ•°å€¤å®‰å®šæ€§å‘ä¸Š
    local_threshold = jnp.percentile(score, config.local_jump_percentile)
    local_jump = (score > local_threshold).astype(jnp.float32)
    
    # 4. ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆÏTï¼‰
    try:
        rho_t = calculate_rho_t_jax(data, config.window)
    except Exception as e:
        print(f"Rho_t calculation failed, using simple version: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«æ¨™æº–åå·®
        rho_t = jnp.ones_like(data) * jnp.std(data)
    
    # 5. æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ­£è¦åŒ–ï¼‰
    time_trend = jnp.arange(len(data), dtype=jnp.float32) / len(data)
    
    return {
        'delta_lambda_pos': delta_pos,
        'delta_lambda_neg': delta_neg,
        'rho_t': rho_t,
        'time_trend': time_trend,
        'local_jump': local_jump
    }

# ===============================
# NumPyro Bayesian Models
# ===============================
def lambda3_base_model(features: Dict[str, jnp.ndarray], 
                      y_obs: Optional[jnp.ndarray] = None):
    """åŸºæœ¬LambdaÂ³ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ ãƒ†ãƒ³ã‚½ãƒ«æ¨å®šï¼‰"""
    # æ§‹é€ ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆÎ›ï¼‰ã®äº‹å‰åˆ†å¸ƒ
    beta_0 = numpyro.sample("lambda_intercept", dist.Normal(0.0, 2.0))
    beta_time = numpyro.sample("lambda_flow", dist.Normal(0.0, 1.0))
    beta_pos = numpyro.sample("lambda_struct_pos", dist.Normal(0.0, 3.0))
    beta_neg = numpyro.sample("lambda_struct_neg", dist.Normal(0.0, 3.0))
    beta_rho = numpyro.sample("rho_tension", dist.Normal(0.0, 2.0))
    
    # æ§‹é€ ç©ºé–“ã§ã®ç·šå½¢çµåˆ
    mu = (beta_0 + 
          beta_time * features['time_trend'] +
          beta_pos * features['delta_lambda_pos'] +
          beta_neg * features['delta_lambda_neg'] +
          beta_rho * features['rho_t'])
    
    # è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰
    sigma_obs = numpyro.sample("sigma_obs", dist.HalfNormal(1.0))
    
    # è¦³æ¸¬ï¼ˆÎ”Î›Cè„ˆå‹•ã¨ã—ã¦ï¼‰
    with numpyro.plate("observations", len(mu)):
        return numpyro.sample("y", dist.Normal(mu, sigma_obs), obs=y_obs)

def lambda3_interaction_model(features_a: Dict[str, jnp.ndarray],
                             features_b: Dict[str, jnp.ndarray],
                             y_obs: Optional[jnp.ndarray] = None):
    """éå¯¾ç§°ç›¸äº’ä½œç”¨LambdaÂ³ãƒ¢ãƒ‡ãƒ«"""
    # åŸºæœ¬æ§‹é€ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    beta_0 = numpyro.sample("lambda_intercept", dist.Normal(0.0, 2.0))
    beta_time = numpyro.sample("lambda_flow_self", dist.Normal(0.0, 1.0))
    beta_pos_self = numpyro.sample("lambda_struct_pos_self", dist.Normal(0.0, 3.0))
    beta_neg_self = numpyro.sample("lambda_struct_neg_self", dist.Normal(0.0, 3.0))
    beta_rho_self = numpyro.sample("rho_tension_self", dist.Normal(0.0, 2.0))
    
    # ç›¸äº’ä½œç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¯ãƒ­ã‚¹æ§‹é€ ãƒ†ãƒ³ã‚½ãƒ«ï¼‰
    beta_pos_cross = numpyro.sample("lambda_interact_pos", dist.Normal(0.0, 3.0))
    beta_neg_cross = numpyro.sample("lambda_interact_neg", dist.Normal(0.0, 3.0))
    beta_rho_cross = numpyro.sample("rho_interact", dist.Normal(0.0, 2.0))
    
    # æ§‹é€ ç©ºé–“ã§ã®çµåˆ
    mu = (beta_0 +
          beta_time * features_a['time_trend'] +
          # è‡ªå·±æ§‹é€ é …
          beta_pos_self * features_a['delta_lambda_pos'] +
          beta_neg_self * features_a['delta_lambda_neg'] +
          beta_rho_self * features_a['rho_t'] +
          # ç›¸äº’ä½œç”¨é …
          beta_pos_cross * features_b['delta_lambda_pos'] +
          beta_neg_cross * features_b['delta_lambda_neg'] +
          beta_rho_cross * features_b['rho_t'])
    
    sigma_obs = numpyro.sample("sigma_obs", dist.HalfNormal(1.0))
    
    with numpyro.plate("observations", len(mu)):
        return numpyro.sample("y", dist.Normal(mu, sigma_obs), obs=y_obs)

def lambda3_dynamic_model(features: Dict[str, jnp.ndarray],
                         change_points: Optional[List[int]] = None,
                         y_obs: Optional[jnp.ndarray] = None):
    """å‹•çš„LambdaÂ³ãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ å¤‰åŒ–ç‚¹æ¤œå‡ºï¼‰"""
    T = len(features['time_trend'])
    
    # æ™‚å¤‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
    innovation_scale = numpyro.sample("innovation_scale", dist.HalfNormal(0.1))
    beta_time_series = numpyro.sample("lambda_flow_dynamic",
                                     dist.GaussianRandomWalk(innovation_scale,
                                                           num_steps=T))
    
    # æ§‹é€ å¤‰åŒ–ã‚¸ãƒ£ãƒ³ãƒ—
    if change_points:
        jump_effects = []
        for i, cp in enumerate(change_points):
            jump = numpyro.sample(f"structure_jump_{i}", dist.Normal(0.0, 5.0))
            jump_indicator = jnp.where(features['time_trend'] >= cp, 1.0, 0.0)
            jump_effects.append(jump * jump_indicator)
        total_jumps = jnp.sum(jnp.stack(jump_effects), axis=0)
    else:
        total_jumps = 0.0
    
    # å‹•çš„æ§‹é€ æ–¹ç¨‹å¼
    mu = (beta_time_series +
          features['delta_lambda_pos'] +
          features['delta_lambda_neg'] +
          features['rho_t'] +
          total_jumps)
    
    sigma_obs = numpyro.sample("sigma_obs", dist.HalfNormal(1.0))
    
    with numpyro.plate("observations", T):
        return numpyro.sample("y", dist.Normal(mu, sigma_obs), obs=y_obs)

# ===============================
# MCMC Inference Engine
# ===============================
class Lambda3NumPyroInference:
    """NumPyroæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆGPUæœ€é©åŒ–ï¼‰"""
    
    def __init__(self, config: L3ConfigNumPyro):
        self.config = config
        self.traces = {}
        self.predictions = {}
        
    def fit_base_model(self, data: jnp.ndarray, features: Dict[str, jnp.ndarray], 
                      chain_id: int = 0) -> Dict[str, Any]:
        """åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°"""
        rng_key = random.PRNGKey(chain_id * 42)
        
        # NUTS ã‚«ãƒ¼ãƒãƒ«
        kernel = NUTS(lambda3_base_model,
                     target_accept_prob=self.config.target_accept_prob,
                     max_tree_depth=self.config.max_tree_depth)
        
        # MCMCå®Ÿè¡Œ
        mcmc = MCMC(kernel,
                   num_samples=self.config.num_samples,
                   num_warmup=self.config.num_warmup,
                   num_chains=1,
                   progress_bar=False)
        
        mcmc.run(rng_key, features=features, y_obs=data)
        
        # ã‚µãƒ³ãƒ—ãƒ«å–å¾—
        samples = mcmc.get_samples()
        
        # äºˆæ¸¬ç”Ÿæˆ
        predictive = Predictive(lambda3_base_model, samples)
        pred_key = random.split(rng_key)[0]
        predictions = predictive(pred_key, features=features)
        
        # è¨ºæ–­çµ±è¨ˆï¼ˆè©³ç´°ç‰ˆï¼‰
        extra_fields = mcmc.get_extra_fields()
        diagnostics = {
            'chain_id': chain_id,
            'divergences': int(jnp.sum(extra_fields.get('diverging', 0))),
            'num_steps': float(jnp.mean(extra_fields.get('num_steps', 0))),
            'accept_prob': float(jnp.mean(extra_fields.get('accept_prob', 0.0))),
            'step_size': float(jnp.mean(extra_fields.get('step_size', 0.0)))
        }
        
        # è¿½åŠ è¨ºæ–­æƒ…å ±
        if 'energy' in extra_fields:
            diagnostics['energy'] = float(jnp.mean(extra_fields['energy']))
            diagnostics['energy_std'] = float(jnp.std(extra_fields['energy']))
        if 'potential_energy' in extra_fields:
            diagnostics['potential_energy'] = float(jnp.mean(extra_fields['potential_energy']))
        if 'tree_depth' in extra_fields:
            diagnostics['tree_depth'] = float(jnp.mean(extra_fields['tree_depth']))
            diagnostics['max_tree_depth'] = int(jnp.max(extra_fields['tree_depth']))
        
        # åŠ¹ç‡æ€§æŒ‡æ¨™
        total_steps = jnp.sum(extra_fields.get('num_steps', 0))
        if total_steps > 0:
            diagnostics['sampling_efficiency'] = float(self.config.num_samples / total_steps)
        
        # åæŸè¨ºæ–­ï¼ˆR-hatæ¨å®šï¼‰
        if len(samples) > 0:
            for param_name, param_values in samples.items():
                if param_values.ndim > 0 and len(param_values) > 10:
                    # ç°¡æ˜“R-hatè¨ˆç®—
                    n = len(param_values)
                    first_half = param_values[:n//2]
                    second_half = param_values[n//2:]
                    
                    var_within = (jnp.var(first_half) + jnp.var(second_half)) / 2
                    var_between = n/2 * jnp.var(jnp.array([jnp.mean(first_half), jnp.mean(second_half)]))
                    var_total = (n/2 - 1) / (n/2) * var_within + var_between / (n/2)
                    
                    if var_within > 1e-10:
                        rhat = jnp.sqrt(var_total / var_within)
                        diagnostics[f'rhat_{param_name}'] = float(rhat)
        
        return {
            'samples': samples,
            'predictions': predictions,
            'diagnostics': diagnostics,
            'mcmc': mcmc
        }
    
    def fit_interaction_model(self, data: jnp.ndarray,
                            features_a: Dict[str, jnp.ndarray],
                            features_b: Dict[str, jnp.ndarray],
                            chain_id: int = 0) -> Dict[str, Any]:
        """ç›¸äº’ä½œç”¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°"""
        rng_key = random.PRNGKey(chain_id * 42 + 1000)
        
        kernel = NUTS(lambda3_interaction_model,
                     target_accept_prob=self.config.target_accept_prob,
                     max_tree_depth=self.config.max_tree_depth)
        
        mcmc = MCMC(kernel,
                   num_samples=self.config.num_samples,
                   num_warmup=self.config.num_warmup,
                   num_chains=1,
                   progress_bar=False)
        
        mcmc.run(rng_key, features_a=features_a, features_b=features_b, y_obs=data)
        
        samples = mcmc.get_samples()
        predictive = Predictive(lambda3_interaction_model, samples)
        pred_key = random.split(rng_key)[0]
        predictions = predictive(pred_key, features_a=features_a, features_b=features_b)
        
        diagnostics = {
            'chain_id': chain_id,
            'divergences': int(jnp.sum(mcmc.get_extra_fields().get('diverging', 0))),
            'num_steps': float(jnp.mean(mcmc.get_extra_fields().get('num_steps', 0))),
            'accept_prob': float(jnp.mean(mcmc.get_extra_fields().get('accept_prob', 0.0)))
        }
        
        # è©³ç´°è¨ºæ–­
        extra_fields = mcmc.get_extra_fields()
        if 'energy' in extra_fields:
            diagnostics['energy'] = float(jnp.mean(extra_fields['energy']))
        if 'tree_depth' in extra_fields:
            diagnostics['tree_depth'] = float(jnp.mean(extra_fields['tree_depth']))
        if 'step_size' in extra_fields:
            diagnostics['step_size'] = float(jnp.mean(extra_fields['step_size']))
        
        return {
            'samples': samples,
            'predictions': predictions,
            'diagnostics': diagnostics,
            'mcmc': mcmc
        }
    
    def parallel_inference(self, data_list: List[jnp.ndarray],
                          features_list: List[Dict[str, jnp.ndarray]],
                          model_type: str = 'base') -> List[Dict[str, Any]]:
        """ä¸¦åˆ—æ¨è«–å®Ÿè¡Œ"""
        
        def single_chain_wrapper(args):
            if model_type == 'base':
                data, features, chain_id = args
                return self.fit_base_model(data, features, chain_id)
            else:
                raise ValueError(f"Parallel model type {model_type} not implemented")
        
        # å¼•æ•°æº–å‚™
        chain_args = [(data, features, i) 
                     for i, (data, features) in enumerate(zip(data_list, features_list))]
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        max_workers = min(len(chain_args), self.config.max_workers)
        results = []
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(single_chain_wrapper, args): i 
                      for i, args in enumerate(chain_args)}
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"Chain failed: {e}")
                    results.append(None)
        
        return results

# ===============================
# Data Preprocessing & Scaling
# ===============================
@jax.jit
def standardize_jax(x: jnp.ndarray) -> jnp.ndarray:
    """JAXæœ€é©åŒ–ã•ã‚ŒãŸæ¨™æº–åŒ–ï¼ˆã‚¼ãƒ­å¹³å‡ãƒ»å˜ä½åˆ†æ•£ï¼‰"""
    mean_x = jnp.mean(x)
    std_x = jnp.std(x)
    # ã‚¼ãƒ­é™¤ç®—å›é¿
    safe_std = jnp.maximum(std_x, 1e-8)
    return (x - mean_x) / safe_std

@jax.jit
def minmax_scale_jax(x: jnp.ndarray) -> jnp.ndarray:
    """JAXæœ€é©åŒ–ã•ã‚ŒãŸMin-Maxæ­£è¦åŒ–"""
    min_x, max_x = jnp.min(x), jnp.max(x)
    range_x = jnp.maximum(max_x - min_x, 1e-8)
    return (x - min_x) / range_x

@jax.jit
def robust_scale_standardize_jax(x: jnp.ndarray) -> jnp.ndarray:
    """JAXæœ€é©åŒ–ã•ã‚ŒãŸå …ç‰¢æ¨™æº–åŒ–ï¼ˆä¸­å¤®å€¤ãƒ»MADï¼‰"""
    median_x = jnp.median(x)
    mad = jnp.median(jnp.abs(x - median_x))
    safe_mad = jnp.maximum(mad, 1e-8)
    return (x - median_x) / (1.4826 * safe_mad)

def robust_scale_jax(x: jnp.ndarray, method: str = 'standardize') -> jnp.ndarray:
    """å …ç‰¢ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆLambda3æ§‹é€ ç©ºé–“ç”¨ï¼‰- JITå¯¾å¿œç‰ˆ"""
    if method == 'standardize':
        return standardize_jax(x)
    elif method == 'minmax':
        return minmax_scale_jax(x)
    elif method == 'robust':
        return robust_scale_standardize_jax(x)
    else:
        return x

def preprocess_series_dict(series_dict: Dict[str, jnp.ndarray], 
                          scaling_method: str = 'standardize',
                          variance_threshold: float = 1e-6,
                          verbose: bool = True) -> Tuple[Dict[str, jnp.ndarray], Dict[str, Dict[str, float]]]:
    """ç³»åˆ—è¾æ›¸ã®å‰å‡¦ç†ãƒ»ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
    
    if verbose:
        print("\nğŸ”§ PREPROCESSING & SCALING ANALYSIS")
        print("=" * 50)
        print("Original series statistics:")
    
    scaling_info = {}
    processed_dict = {}
    problematic_series = []
    
    for name, data in series_dict.items():
        # å…ƒã®çµ±è¨ˆ
        original_mean = float(jnp.mean(data))
        original_std = float(jnp.std(data))
        original_var = original_std ** 2
        original_range = float(jnp.max(data) - jnp.min(data))
        
        if verbose:
            print(f"  {name:15s} | Mean: {original_mean:8.4f} | Std: {original_std:8.4f} | Range: {original_range:8.4f}")
        
        # å•é¡Œæ¤œå‡º
        is_problematic = False
        issues = []
        
        if original_var < variance_threshold:
            issues.append(f"Low variance ({original_var:.2e})")
            is_problematic = True
        
        if original_range < 1e-6:
            issues.append(f"Minimal range ({original_range:.2e})")
            is_problematic = True
            
        if jnp.any(jnp.isnan(data)) or jnp.any(jnp.isinf(data)):
            issues.append("Contains NaN/Inf")
            is_problematic = True
        
        if is_problematic:
            problematic_series.append((name, issues))
            if verbose:
                print(f"    âš ï¸  Issues: {', '.join(issues)}")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨
        if scaling_method == 'none':
            processed_data = data
        else:
            processed_data = robust_scale_jax(data, scaling_method)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œçµ±è¨ˆ
        scaled_mean = float(jnp.mean(processed_data))
        scaled_std = float(jnp.std(processed_data))
        
        # æƒ…å ±ä¿å­˜
        scaling_info[name] = {
            'original_mean': original_mean,
            'original_std': original_std,
            'original_var': original_var,
            'scaled_mean': scaled_mean,
            'scaled_std': scaled_std,
            'scaling_method': scaling_method,
            'is_problematic': is_problematic,
            'issues': issues
        }
        
        processed_dict[name] = processed_data
    
    if verbose:
        print(f"\nScaled series statistics (method: {scaling_method}):")
        for name, data in processed_dict.items():
            mean_val = float(jnp.mean(data))
            std_val = float(jnp.std(data))
            print(f"  {name:15s} | Mean: {mean_val:8.4f} | Std: {std_val:8.4f}")
        
        print(f"\nğŸ“Š PREPROCESSING SUMMARY:")
        print(f"  Total series: {len(series_dict)}")
        print(f"  Problematic: {len(problematic_series)}")
        print(f"  Scaling method: {scaling_method}")
        
        if problematic_series:
            print(f"\nâš ï¸  PROBLEMATIC SERIES:")
            for name, issues in problematic_series:
                print(f"    {name}: {', '.join(issues)}")
            print(f"\nğŸ’¡ RECOMMENDATION: Use standardization or robust scaling")
    
    return processed_dict, scaling_info

def recommend_scaling_method(series_dict: Dict[str, jnp.ndarray]) -> str:
    """æœ€é©ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’æ¨å¥¨"""
    
    variance_ratios = []
    range_ratios = []
    
    variances = [float(jnp.var(data)) for data in series_dict.values()]
    ranges = [float(jnp.max(data) - jnp.min(data)) for data in series_dict.values()]
    
    if len(variances) > 1:
        max_var, min_var = max(variances), min(variances)
        max_range, min_range = max(ranges), min(ranges)
        
        if min_var > 0:
            variance_ratio = max_var / min_var
        else:
            variance_ratio = np.inf
            
        if min_range > 0:
            range_ratio = max_range / min_range
        else:
            range_ratio = np.inf
    else:
        variance_ratio = 1.0
        range_ratio = 1.0
    
    print(f"\nğŸ¯ SCALING RECOMMENDATION:")
    print(f"  Variance ratio (max/min): {variance_ratio:.2e}")
    print(f"  Range ratio (max/min): {range_ratio:.2e}")
    
    # æ¨å¥¨ãƒ­ã‚¸ãƒƒã‚¯
    if variance_ratio > 1e6 or range_ratio > 1e6:
        recommendation = 'standardize'
        reason = "Extreme scale differences detected"
    elif variance_ratio > 100 or range_ratio > 100:
        recommendation = 'robust'
        reason = "Moderate scale differences, outliers possible"
    elif any(var < 1e-6 for var in variances):
        recommendation = 'standardize'
        reason = "Very low variance series detected"
    else:
        recommendation = 'minmax'
        reason = "Similar scales, simple normalization sufficient"
    
    print(f"  Recommended method: {recommendation}")
    print(f"  Reason: {reason}")
    
    return recommendation
def fetch_financial_data_numpyro(start_date="2024-01-01", end_date="2024-12-31",
                                csv_filename="financial_data_numpyro.csv") -> Optional[pd.DataFrame]:
    """é‡‘èãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆNumPyroç”¨ï¼‰"""
    if not YFINANCE_AVAILABLE:
        print("yfinance not available. Please install: pip install yfinance")
        return None
    
    tickers = {
        "USD/JPY": "JPY=X",
        "GBP/USD": "GBPUSD=X",
        "GBP/JPY": "GBPJPY=X",
        "Nikkei 225": "^N225",
        "Dow Jones": "^DJI"
    }
    
    try:
        print(f"Fetching data from {start_date} to {end_date}...")
        data_close = yf.download(list(tickers.values()), start=start_date, end=end_date)['Close']
        
        # JPY/GBPè¨ˆç®—
        data_close['JPY/GBP'] = 1 / data_close['GBPJPY=X']
        data_close = data_close.drop(columns=['GBPJPY=X'])
        
        # ã‚«ãƒ©ãƒ åå¤‰æ›´
        reversed_tickers = {v: k for k, v in tickers.items()}
        final_data = data_close.rename(columns=reversed_tickers)
        
        # ä¸¦ã³æ›¿ãˆ
        desired_order = ["USD/JPY", "JPY/GBP", "GBP/USD", "Nikkei 225", "Dow Jones"]
        final_data = final_data[desired_order]
        final_data = final_data.dropna()
        
        # CSVä¿å­˜
        final_data.to_csv(csv_filename, index=True)
        print(f"Data saved to {csv_filename}")
        
        return final_data
        
    except Exception as e:
        print(f"Error fetching data: {e}")
        return None

def load_csv_to_jax(filepath: str, value_columns: Optional[List[str]] = None) -> Dict[str, jnp.ndarray]:
    """CSVâ†’JAXé…åˆ—å¤‰æ›"""
    df = pd.read_csv(filepath, index_col=0, parse_dates=True)
    
    if value_columns is None:
        value_columns = df.select_dtypes(include=[np.number]).columns.tolist()
    
    series_dict = {}
    for col in value_columns:
        if col in df.columns:
            data = df[col].fillna(method='ffill').fillna(method='bfill').values
            series_dict[col] = jnp.array(data, dtype=jnp.float32)
    
    return series_dict

# ===============================
# Advanced Diagnostics & Grid Analysis
# ===============================
def plot_mcmc_diagnostics(result: Dict[str, Any], title: str = "MCMC Diagnostics"):
    """MCMCè¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã€è‡ªå·±ç›¸é–¢ç­‰ï¼‰"""
    samples = result['samples']
    diagnostics = result['diagnostics']
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã«å¿œã˜ã¦å›³ã®ã‚µã‚¤ã‚ºã‚’èª¿æ•´
    param_names = list(samples.keys())
    n_params = len(param_names)
    
    fig, axes = plt.subplots(n_params, 3, figsize=(15, 4 * n_params))
    if n_params == 1:
        axes = axes.reshape(1, -1)
    
    for i, param in enumerate(param_names):
        param_samples = np.array(samples[param])
        
        # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        axes[i, 0].plot(param_samples)
        axes[i, 0].set_title(f'{param}: Trace Plot')
        axes[i, 0].set_xlabel('Iteration')
        axes[i, 0].set_ylabel('Value')
        axes[i, 0].grid(True, alpha=0.3)
        
        # å¯†åº¦ãƒ—ãƒ­ãƒƒãƒˆ
        axes[i, 1].hist(param_samples, bins=50, density=True, alpha=0.7, color='skyblue')
        axes[i, 1].set_title(f'{param}: Posterior Density')
        axes[i, 1].set_xlabel('Value')
        axes[i, 1].set_ylabel('Density')
        axes[i, 1].grid(True, alpha=0.3)
        
        # ç´¯ç©å¹³å‡ãƒ—ãƒ­ãƒƒãƒˆï¼ˆåæŸãƒã‚§ãƒƒã‚¯ï¼‰
        cumulative_mean = np.cumsum(param_samples) / np.arange(1, len(param_samples) + 1)
        axes[i, 2].plot(cumulative_mean)
        axes[i, 2].set_title(f'{param}: Running Mean')
        axes[i, 2].set_xlabel('Iteration')
        axes[i, 2].set_ylabel('Cumulative Mean')
        axes[i, 2].grid(True, alpha=0.3)
        
        # æœ€çµ‚å€¤ã®ç·šã‚’è¿½åŠ 
        final_mean = cumulative_mean[-1]
        axes[i, 2].axhline(y=final_mean, color='red', linestyle='--', alpha=0.7)
    
    plt.suptitle(title, fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # è¨ºæ–­ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\nğŸ“Š MCMC DIAGNOSTICS SUMMARY:")
    print("=" * 50)
    for key, value in diagnostics.items():
        if isinstance(value, float):
            print(f"  {key:20s}: {value:.4f}")
        else:
            print(f"  {key:20s}: {value}")
    
    # è¨ºæ–­åˆ¤å®š
    print("\nğŸ” DIAGNOSTIC ASSESSMENT:")
    if diagnostics.get('divergences', 0) == 0:
        print("  âœ… No divergent transitions")
    else:
        print(f"  âš ï¸  {diagnostics['divergences']} divergent transitions detected")
    
    if diagnostics.get('accept_prob', 0) > 0.7:
        print("  âœ… Good acceptance probability")
    else:
        print("  âš ï¸  Low acceptance probability")
    
    # R-hatè¨ºæ–­
    rhat_issues = [k for k in diagnostics.keys() if k.startswith('rhat_') and diagnostics[k] > 1.1]
    if not rhat_issues:
        print("  âœ… All R-hat values < 1.1 (good convergence)")
    else:
        print(f"  âš ï¸  Convergence issues: {rhat_issues}")

def plot_energy_diagnostics(result: Dict[str, Any]):
    """ã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ"""
    if 'mcmc' not in result:
        print("MCMC object not available for energy diagnostics")
        return
    
    try:
        extra_fields = result['mcmc'].get_extra_fields()
        
        if 'energy' not in extra_fields:
            print("Energy information not available")
            return
        
        energy = np.array(extra_fields['energy'])
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚¹
        ax1.plot(energy)
        ax1.set_title('Energy Trace')
        ax1.set_xlabel('Iteration')
        ax1.set_ylabel('Energy')
        ax1.grid(True, alpha=0.3)
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        ax2.hist(energy, bins=50, density=True, alpha=0.7, color='orange')
        ax2.set_title('Energy Distribution')
        ax2.set_xlabel('Energy')
        ax2.set_ylabel('Density')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼çµ±è¨ˆ
        print(f"\nâš¡ ENERGY DIAGNOSTICS:")
        print(f"  Mean energy: {np.mean(energy):.4f}")
        print(f"  Energy std:  {np.std(energy):.4f}")
        print(f"  Energy range: [{np.min(energy):.4f}, {np.max(energy):.4f}]")
        
    except Exception as e:
        print(f"Energy diagnostics failed: {e}")

def grid_search_lambda3_params(data: jnp.ndarray, 
                              features: Dict[str, jnp.ndarray],
                              param_grid: Dict[str, List[float]],
                              config: L3ConfigNumPyro) -> Dict[str, Any]:
    """LambdaÂ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ"""
    
    print("ğŸ” Starting LambdaÂ³ parameter grid search...")
    print(f"Grid size: {np.prod([len(v) for v in param_grid.values()])} combinations")
    
    results = []
    inference_engine = Lambda3NumPyroInference(config)
    
    # ã‚°ãƒªãƒƒãƒ‰ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
    from itertools import product
    param_names = list(param_grid.keys())
    param_combinations = list(product(*param_grid.values()))
    
    for i, param_values in enumerate(param_combinations):
        param_dict = dict(zip(param_names, param_values))
        
        print(f"\n[{i+1}/{len(param_combinations)}] Testing: {param_dict}")
        
        # ä¸€æ™‚çš„ã«configæ›´æ–°
        temp_config = L3ConfigNumPyro(
            **{**config.__dict__, **param_dict}
        )
        
        try:
            # é«˜é€ŸåŒ–ã®ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å‰Šæ¸›
            temp_config.num_samples = min(config.num_samples, 500)
            temp_config.num_warmup = min(config.num_warmup, 250)
            
            inference_engine.config = temp_config
            result = inference_engine.fit_base_model(data, features, chain_id=i)
            
            # æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
            diagnostics = result['diagnostics']
            samples = result['samples']
            
            # ãƒ¢ãƒ‡ãƒ«é©åˆåº¦ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            predictions = result['predictions']['y']
            if predictions.ndim > 1:
                pred_mean = jnp.mean(predictions, axis=0)
            else:
                pred_mean = predictions
            
            mse = float(jnp.mean((data - pred_mean) ** 2))
            
            # çµæœä¿å­˜
            result_entry = {
                'params': param_dict.copy(),
                'mse': mse,
                'divergences': diagnostics.get('divergences', 0),
                'accept_prob': diagnostics.get('accept_prob', 0),
                'energy': diagnostics.get('energy', np.inf),
                'rhat_max': max([diagnostics.get(k, 1.0) for k in diagnostics.keys() if k.startswith('rhat_')], default=1.0)
            }
            results.append(result_entry)
            
            print(f"  MSE: {mse:.4f}, Divergences: {result_entry['divergences']}")
            
        except Exception as e:
            print(f"  Failed: {e}")
            results.append({
                'params': param_dict.copy(),
                'mse': np.inf,
                'divergences': 999,
                'accept_prob': 0,
                'energy': np.inf,
                'rhat_max': 999
            })
    
    return analyze_grid_results(results)

def analyze_grid_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒçµæœã®è§£æ"""
    
    # æœ‰åŠ¹ãªçµæœã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
    valid_results = [r for r in results if r['mse'] < np.inf]
    
    if not valid_results:
        print("âŒ No valid results found in grid search")
        return {'best_params': None, 'results': results}
    
    # è¤‡åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆMSE + ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
    for result in valid_results:
        penalty = 0
        penalty += result['divergences'] * 0.1  # divergence penalty
        penalty += max(0, 1.1 - result['accept_prob']) * 0.5  # low acceptance penalty
        penalty += max(0, result['rhat_max'] - 1.1) * 2.0  # convergence penalty
        
        result['composite_score'] = result['mse'] + penalty
    
    # æœ€é©çµæœ
    best_result = min(valid_results, key=lambda x: x['composite_score'])
    
    print("\nğŸ† GRID SEARCH RESULTS:")
    print("=" * 50)
    print(f"Best parameters: {best_result['params']}")
    print(f"MSE: {best_result['mse']:.4f}")
    print(f"Composite score: {best_result['composite_score']:.4f}")
    print(f"Divergences: {best_result['divergences']}")
    print(f"Accept prob: {best_result['accept_prob']:.3f}")
    
    # ãƒˆãƒƒãƒ—3çµæœ
    top_results = sorted(valid_results, key=lambda x: x['composite_score'])[:3]
    print(f"\nğŸ“Š TOP 3 CONFIGURATIONS:")
    for i, result in enumerate(top_results, 1):
        print(f"{i}. {result['params']} (score: {result['composite_score']:.4f})")
    
    return {
        'best_params': best_result['params'],
        'best_result': best_result,
        'top_results': top_results,
        'all_results': results
    }

def plot_grid_search_results(grid_results: Dict[str, Any]):
    """ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒçµæœã®å¯è¦–åŒ–"""
    
    if not grid_results.get('all_results'):
        print("No grid results to plot")
        return
    
    results = [r for r in grid_results['all_results'] if r['mse'] < np.inf]
    
    if len(results) < 2:
        print("Insufficient valid results for plotting")
        return
    
    # çµæœã‚’DataFrameã«å¤‰æ›
    import pandas as pd
    
    data_rows = []
    for result in results:
        row = result['params'].copy()
        row.update({
            'mse': result['mse'],
            'divergences': result['divergences'], 
            'accept_prob': result['accept_prob'],
            'composite_score': result.get('composite_score', result['mse'])
        })
        data_rows.append(row)
    
    df = pd.DataFrame(data_rows)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒƒãƒˆ
    param_cols = [c for c in df.columns if c not in ['mse', 'divergences', 'accept_prob', 'composite_score']]
    
    if len(param_cols) == 1:
        # 1Dãƒ—ãƒ­ãƒƒãƒˆ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        param = param_cols[0]
        ax1.scatter(df[param], df['mse'], alpha=0.7, c=df['divergences'], cmap='Reds')
        ax1.set_xlabel(param)
        ax1.set_ylabel('MSE')
        ax1.set_title(f'MSE vs {param}')
        
        ax2.scatter(df[param], df['accept_prob'], alpha=0.7)
        ax2.set_xlabel(param) 
        ax2.set_ylabel('Accept Probability')
        ax2.set_title(f'Accept Prob vs {param}')
        
    elif len(param_cols) == 2:
        # 2Dãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
        
        param1, param2 = param_cols[0], param_cols[1]
        
        # MSEãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        pivot_mse = df.pivot_table(values='mse', index=param1, columns=param2, aggfunc='mean')
        sns.heatmap(pivot_mse, ax=ax1, cmap='viridis_r', annot=True, fmt='.3f')
        ax1.set_title('MSE Heatmap')
        
        # Divergenceãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        pivot_div = df.pivot_table(values='divergences', index=param1, columns=param2, aggfunc='mean')
        sns.heatmap(pivot_div, ax=ax2, cmap='Reds', annot=True, fmt='.0f')
        ax2.set_title('Divergences Heatmap')
        
        # Accept Probãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        pivot_acc = df.pivot_table(values='accept_prob', index=param1, columns=param2, aggfunc='mean')
        sns.heatmap(pivot_acc, ax=ax3, cmap='viridis', annot=True, fmt='.3f')
        ax3.set_title('Accept Probability Heatmap')
        
        # Composite Scoreãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        pivot_comp = df.pivot_table(values='composite_score', index=param1, columns=param2, aggfunc='mean')
        sns.heatmap(pivot_comp, ax=ax4, cmap='viridis_r', annot=True, fmt='.3f')
        ax4.set_title('Composite Score Heatmap')
        
    else:
        # 3Dä»¥ä¸Šï¼šæ•£å¸ƒå›³è¡Œåˆ—
        from pandas.plotting import scatter_matrix
        scatter_matrix(df[param_cols + ['mse', 'composite_score']], figsize=(12, 12), alpha=0.7)
    
    plt.tight_layout()
    plt.show()
def plot_lambda3_results_numpyro(data: jnp.ndarray, 
                                 predictions: jnp.ndarray,
                                 features: Dict[str, jnp.ndarray],
                                 title: str = "LambdaÂ³ NumPyro Results"):
    """NumPyroçµæœå¯è¦–åŒ–ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰"""
    try:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèªãƒ»å¤‰æ›
        data = np.array(data)
        
        # ä¸Šæ®µï¼šãƒ‡ãƒ¼ã‚¿ãƒ»äºˆæ¸¬ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆ
        ax1.plot(data, 'o-', alpha=0.7, label='Observed Data', markersize=3)
        
        # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        if predictions.ndim > 1:
            pred_array = np.array(predictions)
            pred_mean = np.mean(pred_array, axis=0)
            pred_std = np.std(pred_array, axis=0)
            
            ax1.plot(pred_mean, 'r-', label='Prediction Mean', linewidth=2)
            ax1.fill_between(range(len(pred_mean)), 
                            pred_mean - pred_std, pred_mean + pred_std,
                            alpha=0.3, color='red', label='Â±1Ïƒ')
        else:
            pred_array = np.array(predictions)
            ax1.plot(pred_array, 'r-', label='Prediction', linewidth=2)
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ¼ã‚«ãƒ¼ï¼ˆå®‰å…¨ãªå¤‰æ›ï¼‰
        try:
            pos_events = np.where(np.array(features['delta_lambda_pos']) > 0)[0]
            neg_events = np.where(np.array(features['delta_lambda_neg']) > 0)[0]
            
            if len(pos_events) > 0:
                ax1.scatter(pos_events, data[pos_events], 
                           color='blue', s=50, marker='^', label='Positive Î”Î›C', zorder=5)
            if len(neg_events) > 0:
                ax1.scatter(neg_events, data[neg_events],
                           color='orange', s=50, marker='v', label='Negative Î”Î›C', zorder=5)
        except Exception as e:
            print(f"Event marker plotting failed: {e}")
        
        ax1.set_title(title, fontsize=14)
        ax1.set_ylabel('Value')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ä¸‹æ®µï¼šãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚«ãƒ©ãƒ¼
        try:
            rho_t_array = np.array(features['rho_t'])
            ax2.plot(rho_t_array, 'g-', label='Tension Scalar ÏT', linewidth=1.5)
            ax2.set_xlabel('Time Step')
            ax2.set_ylabel('ÏT')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        except Exception as e:
            print(f"Tension scalar plotting failed: {e}")
            ax2.text(0.5, 0.5, f'Tension plot error: {e}', 
                    transform=ax2.transAxes, ha='center')
        
        plt.tight_layout()
        plt.show()
        
    except Exception as e:
        print(f"Complete plotting failed: {e}")
        # æœ€å°é™ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        plt.figure(figsize=(10, 4))
        plt.plot(np.array(data), 'o-', label='Data')
        plt.title(title)
        plt.legend()
        plt.show()

# ===============================
# Advanced Visualization (PyMC Style)
# ===============================
def plot_l3_prediction_dual_numpyro(
    data_dict: Dict[str, jnp.ndarray],
    mu_pred_dict: Dict[str, jnp.ndarray],
    features_dict: Dict[str, Dict[str, jnp.ndarray]],
    series_names: Optional[List[str]] = None,
    titles: Optional[List[str]] = None
):
    """NumPyroç‰ˆã®ãƒ‡ãƒ¥ã‚¢ãƒ«äºˆæ¸¬ãƒ—ãƒ­ãƒƒãƒˆï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰"""
    if series_names is None:
        series_names = list(data_dict.keys())

    n_series = len(series_names)
    fig, axes = plt.subplots(n_series, 1, figsize=(15, 5 * n_series), sharex=True)

    if n_series == 1:
        axes = [axes]

    for i, series in enumerate(series_names):
        ax = axes[i]
        data = np.array(data_dict[series])
        mu_pred = np.array(mu_pred_dict[series])
        features = features_dict[series]
        
        # ãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(data, 'o', color='gray', markersize=4, alpha=0.6, label='Original Data')
        
        # äºˆæ¸¬ï¼ˆä¿¡é ¼åŒºé–“ä»˜ãï¼‰
        if mu_pred.ndim > 1:
            pred_mean = np.mean(mu_pred, axis=0)
            pred_std = np.std(mu_pred, axis=0)
            ax.plot(pred_mean, color='C2', lw=2, label='Model Prediction')
            ax.fill_between(range(len(pred_mean)), 
                           pred_mean - pred_std, pred_mean + pred_std,
                           alpha=0.3, color='C2', label='Â±1Ïƒ')
        else:
            ax.plot(mu_pred, color='C2', lw=2, label='Model Prediction')

        # ã‚¸ãƒ£ãƒ³ãƒ—ã‚¤ãƒ™ãƒ³ãƒˆ
        jump_pos = np.array(features['delta_lambda_pos'])
        jump_neg = np.array(features['delta_lambda_neg'])
        
        jump_pos_idx = np.where(jump_pos > 0)[0]
        if len(jump_pos_idx):
            ax.plot(jump_pos_idx, data[jump_pos_idx], 'o', color='dodgerblue',
                   markersize=10, label='Positive Î”Î›C')
            for idx in jump_pos_idx:
                ax.axvline(x=idx, color='dodgerblue', linestyle='--', alpha=0.5)

        jump_neg_idx = np.where(jump_neg > 0)[0]
        if len(jump_neg_idx):
            ax.plot(jump_neg_idx, data[jump_neg_idx], 'o', color='orange',
                   markersize=10, label='Negative Î”Î›C')
            for idx in jump_neg_idx:
                ax.axvline(x=idx, color='orange', linestyle='-.', alpha=0.5)

        # å±€æ‰€ã‚¸ãƒ£ãƒ³ãƒ—
        if 'local_jump' in features:
            local_jump = np.array(features['local_jump'])
            local_jump_idx = np.where(local_jump > 0)[0]
            if len(local_jump_idx):
                ax.plot(local_jump_idx, data[local_jump_idx], 'o', color='magenta',
                       markersize=7, alpha=0.7, label='Local Jump')

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        plot_title = titles[i] if titles and i < len(titles) else f"Series {series}: LambdaÂ³ Fit + Events"
        ax.set_title(plot_title, fontsize=16)
        ax.set_xlabel('Time Step', fontsize=12)
        ax.set_ylabel('Value', fontsize=12)

        # é‡è¤‡ãƒ©ãƒ™ãƒ«é™¤å»
        handles, labels = ax.get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys(), fontsize=12)
        ax.grid(axis='y', linestyle=':', alpha=0.7)

    plt.tight_layout()
    plt.show()

def plot_posterior_numpyro(samples: Dict[str, jnp.ndarray], 
                          var_names: Optional[List[str]] = None, 
                          hdi_prob: float = 0.89):
    """NumPyroç‰ˆäº‹å¾Œåˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ"""
    if var_names is None:
        var_names = list(samples.keys())

    n_vars = len(var_names)
    fig, axes = plt.subplots(2, (n_vars + 1) // 2, figsize=(12, 8))
    axes = axes.flatten() if n_vars > 1 else [axes]

    for i, var in enumerate(var_names):
        if var not in samples:
            continue
            
        sample_data = np.array(samples[var])
        ax = axes[i]

        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        ax.hist(sample_data, bins=50, density=True, alpha=0.7, color='skyblue')
        
        # HDIè¨ˆç®—
        sorted_samples = np.sort(sample_data)
        lower_idx = int((1 - hdi_prob) / 2 * len(sorted_samples))
        upper_idx = int((1 + hdi_prob) / 2 * len(sorted_samples))
        
        hdi_lower = sorted_samples[lower_idx]
        hdi_upper = sorted_samples[upper_idx]
        
        # HDIè¡¨ç¤º
        ax.axvline(hdi_lower, color='red', linestyle='--', alpha=0.7)
        ax.axvline(hdi_upper, color='red', linestyle='--', alpha=0.7)
        ax.axvline(np.mean(sample_data), color='red', linewidth=2, label='Mean')
        
        ax.set_title(f'{var}\nMean: {np.mean(sample_data):.3f}, HDI: [{hdi_lower:.3f}, {hdi_upper:.3f}]')
        ax.set_xlabel('Value')
        ax.set_ylabel('Density')
        ax.grid(True, alpha=0.3)

    # æœªä½¿ç”¨ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()

def plot_causality_profiles_numpyro(causality_data: List[Dict[int, float]],
                                   labels: List[str],
                                   title: str = "LambdaÂ³ Causality Profiles"):
    """å› æœé–¢ä¿‚ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ­ãƒƒãƒˆ"""
    plt.figure(figsize=(10, 6))
    
    colors = ['royalblue', 'darkorange', 'forestgreen', 'crimson', 'purple']
    
    for i, (causality_dict, label) in enumerate(zip(causality_data, labels)):
        if causality_dict:
            lags, probs = zip(*sorted(causality_dict.items()))
            color = colors[i % len(colors)]
            plt.plot(lags, probs, marker='o', label=label, color=color, linewidth=2, alpha=0.8)

    plt.xlabel('Lag (steps)', fontsize=12)
    plt.ylabel('Causality Probability', fontsize=12)
    plt.title(title, fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(axis='y', linestyle=':', alpha=0.7)
    plt.tight_layout()
    plt.show()

def plot_regime_analysis_numpyro(data: jnp.ndarray, 
                                features: Dict[str, jnp.ndarray],
                                n_regimes: int = 3):
    """ãƒ¬ã‚¸ãƒ¼ãƒ è§£æãƒ—ãƒ­ãƒƒãƒˆï¼ˆNumPyroç‰ˆï¼‰"""
    
    # ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    from sklearn.cluster import KMeans
    
    # ç‰¹å¾´é‡ã‚¹ã‚¿ãƒƒã‚¯
    X = np.column_stack([
        np.array(features['delta_lambda_pos']),
        np.array(features['delta_lambda_neg']),
        np.array(features['rho_t'])
    ])
    
    # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    kmeans = KMeans(n_clusters=n_regimes, random_state=42)
    regime_labels = kmeans.fit_predict(X)
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
    
    # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
    colors = plt.cm.Set1(np.linspace(0, 1, n_regimes))
    data_np = np.array(data)
    
    for regime in range(n_regimes):
        mask = regime_labels == regime
        regime_times = np.where(mask)[0]
        
        if len(regime_times) > 0:
            ax1.scatter(regime_times, data_np[mask], 
                       c=[colors[regime]], s=30, alpha=0.7, 
                       label=f'Regime {regime + 1}')
    
    ax1.plot(data_np, 'k-', alpha=0.3, linewidth=1)
    ax1.set_title('LambdaÂ³ Market Regime Detection', fontsize=14)
    ax1.set_xlabel('Time Step')
    ax1.set_ylabel('Value')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ãƒ¬ã‚¸ãƒ¼ãƒ çµ±è¨ˆ
    regime_stats = []
    for regime in range(n_regimes):
        mask = regime_labels == regime
        freq = np.mean(mask)
        mean_rho = np.mean(np.array(features['rho_t'])[mask])
        regime_stats.append((regime + 1, freq, mean_rho))
    
    # çµ±è¨ˆè¡¨ç¤º
    regimes, freqs, mean_rhos = zip(*regime_stats)
    x_pos = np.arange(len(regimes))
    
    bars = ax2.bar(x_pos, freqs, color=colors[:n_regimes], alpha=0.7)
    ax2.set_xlabel('Regime')
    ax2.set_ylabel('Frequency')
    ax2.set_title('Regime Frequency Distribution')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels([f'Regime {r}' for r in regimes])
    
    # å„ãƒãƒ¼ã®ä¸Šã«å¹³å‡ÏTå€¤ã‚’è¡¨ç¤º
    for i, (bar, rho) in enumerate(zip(bars, mean_rhos)):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'ÏT: {rho:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # ãƒ¬ã‚¸ãƒ¼ãƒ çµ±è¨ˆã‚’å‡ºåŠ›
    print("\nğŸ“Š REGIME ANALYSIS RESULTS:")
    print("-" * 40)
    for regime, freq, mean_rho in regime_stats:
        print(f"Regime {regime}: {freq:.1%} frequency, Mean ÏT: {mean_rho:.3f}")
    
    return regime_labels

def plot_interaction_heatmap_numpyro(interaction_results: Dict[str, Dict[str, float]],
                                    series_names: List[str]):
    """ç›¸äº’ä½œç”¨è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"""
    n = len(series_names)
    interaction_matrix = np.zeros((n, n))
    
    # è¡Œåˆ—ã‚’æ§‹ç¯‰
    for i, name_a in enumerate(series_names):
        for j, name_b in enumerate(series_names):
            if name_a != name_b and name_a in interaction_results:
                if f'interact_{name_b}' in interaction_results[name_a]:
                    interaction_matrix[i, j] = interaction_results[name_a][f'interact_{name_b}']
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(interaction_matrix,
                xticklabels=series_names,
                yticklabels=series_names,
                annot=True, fmt='.3f',
                cmap='RdBu_r', center=0,
                square=True,
                cbar_kws={'label': 'Interaction Coefficient Î²'})
    plt.title("LambdaÂ³ Cross-Series Interaction Effects\n(Column â†’ Row)", fontsize=16)
    plt.xlabel("From Series", fontsize=12)
    plt.ylabel("To Series", fontsize=12)
    plt.tight_layout()
    plt.show()

def plot_network_analysis_numpyro(sync_matrix: jnp.ndarray, 
                                 series_names: List[str],
                                 threshold: float = 0.3):
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æãƒ—ãƒ­ãƒƒãƒˆ"""
    import networkx as nx
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
    G = nx.DiGraph()
    
    # ãƒãƒ¼ãƒ‰è¿½åŠ 
    for name in series_names:
        G.add_node(name)
    
    # ã‚¨ãƒƒã‚¸è¿½åŠ 
    sync_np = np.array(sync_matrix)
    for i, name_a in enumerate(series_names):
        for j, name_b in enumerate(series_names):
            if i != j and sync_np[i, j] >= threshold:
                G.add_edge(name_a, name_b, weight=sync_np[i, j])
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    pos = nx.spring_layout(G, k=2, iterations=50)
    
    plt.figure(figsize=(12, 10))
    
    # ãƒãƒ¼ãƒ‰æç”»
    nx.draw_networkx_nodes(G, pos, node_color='lightblue', 
                          node_size=2000, alpha=0.7)
    
    # ã‚¨ãƒƒã‚¸æç”»ï¼ˆé‡ã¿ã«å¿œã˜ã¦å¤ªã•ã‚’å¤‰æ›´ï¼‰
    edges = G.edges()
    weights = [G[u][v]['weight'] for u, v in edges]
    nx.draw_networkx_edges(G, pos, edgelist=edges, width=[w*5 for w in weights],
                          alpha=0.6, edge_color='gray', arrows=True, arrowsize=20)
    
    # ãƒ©ãƒ™ãƒ«æç”»
    nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold')
    
    # ã‚¨ãƒƒã‚¸ãƒ©ãƒ™ãƒ«
    edge_labels = {(u, v): f'{d["weight"]:.2f}' for u, v, d in G.edges(data=True)}
    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=10)
    
    plt.title(f"LambdaÂ³ Synchronization Network (threshold = {threshold})", fontsize=16)
    plt.axis('off')
    plt.tight_layout()
    plt.show()
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ
    print(f"\nğŸ”— NETWORK ANALYSIS:")
    print(f"  Nodes: {G.number_of_nodes()}")
    print(f"  Edges: {G.number_of_edges()}")
    print(f"  Density: {nx.density(G):.3f}")
    
    # ä¸­å¿ƒæ€§åˆ†æ
    if G.number_of_edges() > 0:
        centrality = nx.in_degree_centrality(G)
        print(f"\nğŸ“ˆ IN-DEGREE CENTRALITY:")
        for node, cent in sorted(centrality.items(), key=lambda x: x[1], reverse=True):
            print(f"  {node}: {cent:.3f}")

# ===============================
# PyMC-Style Advanced Analysis
# ===============================
class Lambda3AdvancedAnalyzer:
    """NumPyroç‰ˆé«˜åº¦è§£æã‚¯ãƒ©ã‚¹ï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰"""
    
    def __init__(self, config: L3ConfigNumPyro):
        self.config = config
        self.results = {}
    
    def analyze_all_pairs(self, series_dict: Dict[str, jnp.ndarray], 
                         features_dict: Dict[str, Dict[str, jnp.ndarray]],
                         max_pairs: int = None) -> Dict[str, Any]:
        """å…¨ãƒšã‚¢ã®è©³ç´°è§£æï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰"""
        
        series_list = list(series_dict.keys())
        n_series = len(series_list)
        
        # å…¨ãƒšã‚¢ç”Ÿæˆ
        from itertools import combinations
        pairs = list(combinations(series_list, 2))
        
        if max_pairs and len(pairs) > max_pairs:
            pairs = pairs[:max_pairs]
        
        print(f"\n{'='*60}")
        print(f"ANALYZING ALL {len(pairs)} PAIRS")
        print(f"{'='*60}")
        
        # ç›¸äº’ä½œç”¨åŠ¹æœä¿å­˜
        interaction_effects = {}
        sync_profiles = {}
        causality_results = {}
        
        for i, (name_a, name_b) in enumerate(pairs, 1):
            print(f"\n[{i}/{len(pairs)}] Analyzing: {name_a} â†” {name_b}")
            
            try:
                # ãƒšã‚¢è§£æå®Ÿè¡Œ
                result = self._analyze_series_pair_detailed(
                    name_a, name_b, series_dict, features_dict
                )
                
                # çµæœä¿å­˜
                interaction_effects[(name_a, name_b)] = result['interactions']
                sync_profiles[(name_a, name_b)] = result['sync_profile']
                causality_results[(name_a, name_b)] = result['causality']
                
            except Exception as e:
                print(f"Error analyzing pair {name_a} â†” {name_b}: {e}")
                continue
        
        return {
            'interaction_effects': interaction_effects,
            'sync_profiles': sync_profiles,
            'causality_results': causality_results,
            'pairs_analyzed': len(pairs)
        }
    
    def _analyze_series_pair_detailed(self, name_a: str, name_b: str,
                                    series_dict: Dict[str, jnp.ndarray],
                                    features_dict: Dict[str, Dict[str, jnp.ndarray]]) -> Dict[str, Any]:
        """è©³ç´°ãƒšã‚¢è§£æ"""
        
        print(f"\n{'='*50}")
        print(f"ANALYZING PAIR: {name_a} â†” {name_b}")
        print(f"{'='*50}")
        
        # æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™
        inference_engine = Lambda3NumPyroInference(self.config)
        
        # ç›¸äº’ä½œç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆA ã« B ã®å½±éŸ¿ï¼‰
        print(f"\nFitting Bayesian model for {name_a} (with {name_b} interaction)...")
        result_a = inference_engine.fit_interaction_model(
            series_dict[name_a],
            features_dict[name_a],
            features_dict[name_b],
            chain_id=hash(f"{name_a}_{name_b}") % 1000
        )
        
        # ç›¸äº’ä½œç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆB ã« A ã®å½±éŸ¿ï¼‰
        print(f"\nFitting Bayesian model for {name_b} (with {name_a} interaction)...")
        result_b = inference_engine.fit_interaction_model(
            series_dict[name_b],
            features_dict[name_b],
            features_dict[name_a],
            chain_id=hash(f"{name_b}_{name_a}") % 1000
        )
        
        # ç›¸äº’ä½œç”¨ä¿‚æ•°æŠ½å‡º
        samples_a = result_a['samples']
        samples_b = result_b['samples']
        
        # B â†’ A ã®å½±éŸ¿
        beta_b_to_a_pos = float(jnp.mean(samples_a.get('lambda_interact_pos', 0)))
        beta_b_to_a_neg = float(jnp.mean(samples_a.get('lambda_interact_neg', 0)))
        
        # A â†’ B ã®å½±éŸ¿
        beta_a_to_b_pos = float(jnp.mean(samples_b.get('lambda_interact_pos', 0)))
        beta_a_to_b_neg = float(jnp.mean(samples_b.get('lambda_interact_neg', 0)))
        
        print(f"\nAsymmetric Interaction Effects:")
        print(f"  {name_b} â†’ {name_a} (pos): Î² = {beta_b_to_a_pos:.3f}")
        print(f"  {name_b} â†’ {name_a} (neg): Î² = {beta_b_to_a_neg:.3f}")
        print(f"  {name_a} â†’ {name_b} (pos): Î² = {beta_a_to_b_pos:.3f}")
        print(f"  {name_a} â†’ {name_b} (neg): Î² = {beta_a_to_b_neg:.3f}")
        
        # åŒæœŸè§£æ
        try:
            lags, sync_values = sync_profile_jax(
                features_dict[name_a]['delta_lambda_pos'].astype(jnp.float32),
                features_dict[name_b]['delta_lambda_pos'].astype(jnp.float32),
                lag_window=10
            )
            max_sync = float(jnp.max(sync_values))
            optimal_lag = int(lags[jnp.argmax(sync_values)])
            
            print(f"\nSync Rate Ïƒâ‚› ({name_a}â†”{name_b}): {max_sync:.3f}")
            print(f"Optimal Lag: {optimal_lag} steps")
            
            sync_profile_dict = {int(lag): float(sync) for lag, sync in zip(lags, sync_values)}
        except Exception as e:
            print(f"Sync calculation failed: {e}")
            max_sync, optimal_lag = 0.0, 0
            sync_profile_dict = {}
        
        # å› æœé–¢ä¿‚è§£æ
        causality_data = self._calculate_causality_profile(
            features_dict[name_a], features_dict[name_b], name_a, name_b
        )
        
        return {
            'interactions': {
                f'{name_b}_to_{name_a}_pos': beta_b_to_a_pos,
                f'{name_b}_to_{name_a}_neg': beta_b_to_a_neg,
                f'{name_a}_to_{name_b}_pos': beta_a_to_b_pos,
                f'{name_a}_to_{name_b}_neg': beta_a_to_b_neg,
            },
            'sync_profile': {
                'max_sync': max_sync,
                'optimal_lag': optimal_lag,
                'profile': sync_profile_dict
            },
            'causality': causality_data
        }
    
    def _calculate_causality_profile(self, features_a: Dict[str, jnp.ndarray],
                                   features_b: Dict[str, jnp.ndarray],
                                   name_a: str, name_b: str) -> Dict[str, Any]:
        """å› æœé–¢ä¿‚ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨ˆç®—"""
        
        pos_a = np.array(features_a['delta_lambda_pos'])
        neg_a = np.array(features_a['delta_lambda_neg'])
        pos_b = np.array(features_b['delta_lambda_pos'])
        neg_b = np.array(features_b['delta_lambda_neg'])
        
        T = len(pos_a)
        
        # ãƒ©ã‚°åˆ¥å› æœé–¢ä¿‚è¨ˆç®—
        causality_a_to_b = {}
        causality_b_to_a = {}
        
        for lag in range(1, 11):
            if lag < T:
                # A â†’ B ã®å› æœé–¢ä¿‚
                count_ab, count_a = 0, 0
                for i in range(T - lag):
                    if pos_a[i] > 0:
                        count_a += 1
                        if pos_b[i + lag] > 0:
                            count_ab += 1
                
                causality_a_to_b[lag] = count_ab / max(count_a, 1)
                
                # B â†’ A ã®å› æœé–¢ä¿‚
                count_ba, count_b = 0, 0
                for i in range(T - lag):
                    if pos_b[i] > 0:
                        count_b += 1
                        if pos_a[i + lag] > 0:
                            count_ba += 1
                
                causality_b_to_a[lag] = count_ba / max(count_b, 1)
            else:
                causality_a_to_b[lag] = 0.0
                causality_b_to_a[lag] = 0.0
        
        return {
            f'{name_a}_to_{name_b}': causality_a_to_b,
            f'{name_b}_to_{name_a}': causality_b_to_a
        }
    
    def detect_market_regimes(self, features_dict: Dict[str, Dict[str, jnp.ndarray]],
                            series_name: str = None, n_regimes: int = 3) -> Dict[str, Any]:
        """å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º"""
        
        if series_name is None:
            series_name = list(features_dict.keys())[0]
        
        features = features_dict[series_name]
        
        # ç‰¹å¾´é‡æº–å‚™
        X = np.column_stack([
            np.array(features['delta_lambda_pos']),
            np.array(features['delta_lambda_neg']),
            np.array(features['rho_t'])
        ])
        
        # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=n_regimes, random_state=42)
        regime_labels = kmeans.fit_predict(X)
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ çµ±è¨ˆ
        regime_stats = {}
        for regime in range(n_regimes):
            mask = regime_labels == regime
            frequency = np.mean(mask)
            mean_rho = np.mean(X[mask, 2])  # ÏTåˆ—
            
            regime_stats[f'Regime-{regime + 1}'] = {
                'frequency': frequency,
                'mean_rhoT': mean_rho
            }
        
        print("Market Regime Detection:")
        for regime_name, stats in regime_stats.items():
            freq_pct = stats['frequency'] * 100
            print(f"  {regime_name}: {freq_pct:.1f}% (Mean ÏT: {stats['mean_rhoT']:.2f})")
        
        return {
            'regime_labels': regime_labels,
            'regime_stats': regime_stats,
            'series_analyzed': series_name
        }
    
    def detect_scale_breaks(self, data: jnp.ndarray, 
                           scales: List[int] = [5, 10, 20, 50]) -> List[Tuple[int, List[int]]]:
        """ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«å¤‰åŒ–ç‚¹æ¤œå‡º"""
        
        data_np = np.array(data)
        scale_breaks = []
        
        for scale in scales:
            # ãƒ­ãƒ¼ãƒªãƒ³ã‚°æ¨™æº–åå·®
            rolling_std = np.array([
                np.std(data_np[max(0, i-scale):i+1]) 
                for i in range(len(data_np))
            ])
            
            # å¤‰åŒ–ç‚¹æ¤œå‡ºï¼ˆé–¾å€¤: å¹³å‡ + 1.5*æ¨™æº–åå·®ï¼‰
            mean_std = np.mean(rolling_std)
            threshold = mean_std + 1.5 * np.std(rolling_std)
            
            breaks = np.where(rolling_std > threshold)[0]
            if len(breaks) > 0:
                scale_breaks.append((scale, breaks.tolist()))
        
        print(f"\nScale Break Locations: {scale_breaks}")
        return scale_breaks
    
    def calculate_conditional_sync(self, features_a: Dict[str, jnp.ndarray],
                                 features_b: Dict[str, jnp.ndarray]) -> float:
        """æ¡ä»¶ä»˜ãåŒæœŸç‡è¨ˆç®—"""
        
        series_a = np.array(features_a['delta_lambda_pos'])
        series_b = np.array(features_b['delta_lambda_pos'])
        condition_series = np.array(features_a['rho_t'])
        
        # é«˜ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æœŸé–“ã§ã®åŒæœŸ
        condition_threshold = np.median(condition_series)
        mask = condition_series > condition_threshold
        
        if np.sum(mask) > 0:
            sync_rate = np.mean(series_a[mask] * series_b[mask])
        else:
            sync_rate = 0.0
        
        print(f"\nConditional Sync Rate (high tension): {sync_rate:.3f}")
        return sync_rate

def build_sync_network_advanced(sync_profiles: Dict[Tuple[str, str], Dict[str, Any]],
                               threshold: float = 0.0) -> nx.DiGraph:
    """é«˜åº¦åŒæœŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
    
    # å…¨ç³»åˆ—åå–å¾—
    all_series = set()
    for (name_a, name_b), profile_data in sync_profiles.items():
        all_series.add(name_a)
        all_series.add(name_b)
    
    series_names = list(all_series)
    G = nx.DiGraph()
    
    # ãƒãƒ¼ãƒ‰è¿½åŠ 
    for series in series_names:
        G.add_node(series)
    
    print(f"\n=== Building Synchronization Network ===")
    print(f"Using threshold: {threshold:.4f}")
    
    # ã‚¨ãƒƒã‚¸è¿½åŠ 
    print(f"\nBuilding sync network with threshold={threshold}")
    edge_count = 0
    
    for (name_a, name_b), profile_data in sync_profiles.items():
        max_sync = profile_data['max_sync']
        optimal_lag = profile_data['optimal_lag']
        
        print(f"{name_a} â†’ {name_b}: max_sync={max_sync:.4f}, lag={optimal_lag}")
        
        if max_sync >= threshold:
            G.add_edge(name_a, name_b,
                      weight=max_sync,
                      lag=optimal_lag)
            edge_count += 1
            print(f"  âœ“ Edge added!")
        
        # é€†æ–¹å‘ã‚‚è¿½åŠ 
        print(f"{name_b} â†’ {name_a}: max_sync={max_sync:.4f}, lag={-optimal_lag}")
        if max_sync >= threshold:
            G.add_edge(name_b, name_a,
                      weight=max_sync,
                      lag=-optimal_lag)
            edge_count += 1
            print(f"  âœ“ Edge added!")
    
    print(f"\nNetwork summary: {G.number_of_nodes()} nodes, {edge_count} edges")
    return G

def create_comprehensive_summary(series_dict: Dict[str, jnp.ndarray],
                               features_dict: Dict[str, Dict[str, jnp.ndarray]],
                               analysis_results: Dict[str, Any]):
    """åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰"""
    
    print("\n" + "="*60)
    print("ANALYSIS SUMMARY")
    print("="*60)
    
    # ã‚¸ãƒ£ãƒ³ãƒ—çµ±è¨ˆ
    print("\nJump Event Statistics:")
    print("-" * 40)
    for name in list(series_dict.keys()):
        features = features_dict[name]
        pos_jumps = int(jnp.sum(features['delta_lambda_pos']))
        neg_jumps = int(jnp.sum(features['delta_lambda_neg']))
        local_jumps = int(jnp.sum(features['local_jump']))
        print(f"{name:15s} | Pos: {pos_jumps:3d} | Neg: {neg_jumps:3d} | Local: {local_jumps:3d}")
    
    # ãƒˆãƒƒãƒ—åŒæœŸãƒšã‚¢
    sync_profiles = analysis_results.get('sync_profiles', {})
    if sync_profiles:
        print("\nTop Synchronization Pairs:")
        print("-" * 40)
        
        sync_pairs = []
        for (name_a, name_b), profile_data in sync_profiles.items():
            max_sync = profile_data['max_sync']
            sync_pairs.append((max_sync, name_a, name_b))
        
        sync_pairs.sort(reverse=True)
        for max_sync, name_a, name_b in sync_pairs[:5]:
            print(f"{name_a:15s} â†” {name_b:15s} | Ïƒâ‚› = {max_sync:.3f}")
    
    print("\n" + "="*60)

# Lambda_abc_NumPyro.py ã«è¿½åŠ ã™ã‚‹é–¢æ•°ç¾¤

# ===============================
# åŒæœŸè¨ˆç®—ã®ä¿®æ­£ç‰ˆé–¢æ•°
# ===============================

def validate_event_series(event_series_dict: Dict[str, jnp.ndarray]):
    """ã‚¤ãƒ™ãƒ³ãƒˆç³»åˆ—ã®æ¤œè¨¼ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›"""
    print("\nğŸ” EVENT SERIES VALIDATION:")
    print("-" * 50)
    
    for name, series in event_series_dict.items():
        series_np = np.array(series)
        n_events = np.sum(series_np > 0)
        event_rate = n_events / len(series_np) if len(series_np) > 0 else 0
        
        print(f"{name:15s} | Length: {len(series_np):4d} | Events: {n_events:3d} | Rate: {event_rate:.3f}")
        
        if n_events == 0:
            print(f"  âš ï¸  Warning: No events detected in {name}")
        elif event_rate < 0.01:
            print(f"  âš ï¸  Warning: Very low event rate in {name}")
    
    print("-" * 50)

@jax.jit
def sync_profile_jax(series_a: jnp.ndarray, series_b: jnp.ndarray, 
                     lag_window: int = 10) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """JAXæœ€é©åŒ–ã•ã‚ŒãŸåŒæœŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨ˆç®—"""
    n = len(series_a)
    lags = jnp.arange(-lag_window, lag_window + 1)
    n_lags = len(lags)
    sync_values = jnp.zeros(n_lags)
    
    def compute_sync_at_lag(lag):
        if lag < 0:
            # è² ã®ãƒ©ã‚°: series_a ãŒ series_b ã‚ˆã‚Šå…ˆè¡Œ
            abs_lag = -lag
            valid_len = n - abs_lag
            if valid_len > 0:
                return jnp.mean(series_a[abs_lag:] * series_b[:valid_len])
            else:
                return 0.0
        elif lag > 0:
            # æ­£ã®ãƒ©ã‚°: series_b ãŒ series_a ã‚ˆã‚Šå…ˆè¡Œ
            valid_len = n - lag
            if valid_len > 0:
                return jnp.mean(series_a[:valid_len] * series_b[lag:])
            else:
                return 0.0
        else:
            # ãƒ©ã‚°0: åŒæœŸ
            return jnp.mean(series_a * series_b)
    
    # å„ãƒ©ã‚°ã§ã®åŒæœŸç‡ã‚’è¨ˆç®—
    for i, lag in enumerate(lags):
        sync_values = sync_values.at[i].set(compute_sync_at_lag(lag))
    
    return lags, sync_values

def build_sync_matrix_jax_fixed(event_series_dict: Dict[str, jnp.ndarray], 
                               lag_window: int = 10) -> Tuple[jnp.ndarray, List[str]]:
    """ä¿®æ­£ç‰ˆJAXåŒæœŸè¡Œåˆ—æ§‹ç¯‰ï¼ˆNumPyãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰"""
    
    series_names = list(event_series_dict.keys())
    n = len(series_names)
    mat = np.zeros((n, n))  # NumPyé…åˆ—ã§åˆæœŸåŒ–
    
    print(f"Building sync matrix for {n} series...")
    
    for i, name_a in enumerate(series_names):
        for j, name_b in enumerate(series_names):
            if i == j:
                mat[i, j] = 1.0  # è‡ªå·±åŒæœŸã¯å®Œå…¨
                continue
            
            try:
                # NumPyé…åˆ—ã¨ã—ã¦å–å¾—
                series_a = np.array(event_series_dict[name_a], dtype=np.float64)
                series_b = np.array(event_series_dict[name_b], dtype=np.float64)
                
                # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
                if len(series_a) == 0 or len(series_b) == 0:
                    print(f"  {name_a} â†’ {name_b}: empty series, setting to 0")
                    continue
                
                if len(series_a) != len(series_b):
                    print(f"  {name_a} â†’ {name_b}: length mismatch, setting to 0")
                    continue
                
                # ã‚¤ãƒ™ãƒ³ãƒˆã®å­˜åœ¨ç¢ºèª
                events_a = np.sum(series_a > 0)
                events_b = np.sum(series_b > 0)
                
                if events_a == 0 or events_b == 0:
                    # ã‚¤ãƒ™ãƒ³ãƒˆãŒãªã„å ´åˆã¯ç›¸é–¢ä¿‚æ•°ã‚’ä½¿ç”¨
                    if np.std(series_a) > 0 and np.std(series_b) > 0:
                        correlation = np.corrcoef(series_a, series_b)[0, 1]
                        if not np.isnan(correlation):
                            mat[i, j] = abs(correlation)
                            print(f"  {name_a} â†’ {name_b}: no events, using correlation: {abs(correlation):.4f}")
                        else:
                            mat[i, j] = 0.0
                    else:
                        mat[i, j] = 0.0
                    continue
                
                # åŒæœŸç‡è¨ˆç®—ï¼ˆNumPyç‰ˆï¼‰
                max_sync = 0.0
                optimal_lag = 0
                
                for lag in range(-lag_window, lag_window + 1):
                    if lag < 0:
                        abs_lag = -lag
                        if abs_lag < len(series_a):
                            sync_rate = np.mean(series_a[abs_lag:] * series_b[:-abs_lag])
                        else:
                            sync_rate = 0.0
                    elif lag > 0:
                        if lag < len(series_b):
                            sync_rate = np.mean(series_a[:-lag] * series_b[lag:])
                        else:
                            sync_rate = 0.0
                    else:
                        sync_rate = np.mean(series_a * series_b)
                    
                    if sync_rate > max_sync:
                        max_sync = sync_rate
                        optimal_lag = lag
                
                mat[i, j] = max_sync
                print(f"  {name_a} â†’ {name_b}: {max_sync:.4f} (lag: {optimal_lag})")
                
            except Exception as e:
                print(f"  {name_a} â†’ {name_b}: calculation failed ({e}), using 0")
                mat[i, j] = 0.0
    
    # JAXé…åˆ—ã«å¤‰æ›ã—ã¦è¿”ã™
    return jnp.array(mat), series_names

def plot_sync_matrix_numpyro_fixed(sync_matrix: jnp.ndarray, series_names: List[str]):
    """åŒæœŸè¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # NumPyé…åˆ—ã«å¤‰æ›
    sync_np = np.array(sync_matrix)
    
    # NaNå€¤ã‚’ãƒã‚§ãƒƒã‚¯
    if np.any(np.isnan(sync_np)):
        print("Warning: NaN values in sync matrix, replacing with 0")
        sync_np = np.nan_to_num(sync_np, nan=0.0)
    
    plt.figure(figsize=(10, 8))
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
    sns.heatmap(sync_np, 
                annot=True, 
                fmt='.3f',
                xticklabels=series_names,
                yticklabels=series_names,
                cmap="Blues", 
                vmin=0, 
                vmax=1,
                square=True, 
                cbar_kws={'label': 'Sync Rate Ïƒâ‚›'})
    
    plt.title("Synchronization Rate Matrix (Ïƒâ‚›)", fontsize=16)
    plt.xlabel("Series")
    plt.ylabel("Series")
    plt.tight_layout()
    plt.show()
    
    # çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
    off_diagonal = []
    n = len(series_names)
    for i in range(n):
        for j in range(n):
            if i != j:
                off_diagonal.append(sync_np[i, j])
    
    if off_diagonal:
        print(f"\nSync Matrix Statistics:")
        print(f"  Mean sync rate (off-diagonal): {np.mean(off_diagonal):.3f}")
        print(f"  Max sync rate: {np.max(off_diagonal):.3f}")
        print(f"  Min sync rate: {np.min(off_diagonal):.3f}")

# ===============================
# mainé–¢æ•°å†…ã§ä½¿ç”¨ã™ã‚‹ä¿®æ­£ç‰ˆã‚³ãƒ¼ãƒ‰
# ===============================
def synchronization_analysis_section(series_names, features_dict):
    """åŒæœŸè§£æã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã‚¨ãƒ©ãƒ¼å‡¦ç†å¼·åŒ–ç‰ˆï¼‰"""
    print("\nSynchronization analysis...")
    try:
        # ã‚¤ãƒ™ãƒ³ãƒˆç³»åˆ—ã®æº–å‚™
        event_series_dict = {
            name: features_dict[name]['delta_lambda_pos']
            for name in series_names
        }
        
        # ã‚¤ãƒ™ãƒ³ãƒˆç³»åˆ—ã®æ¤œè¨¼
        validate_event_series(event_series_dict)
        
        # åŒæœŸè¡Œåˆ—ã®è¨ˆç®—ï¼ˆä¿®æ­£ç‰ˆã‚’ä½¿ç”¨ï¼‰
        sync_matrix, names = build_sync_matrix_jax_fixed(event_series_dict, lag_window=10)
        print(f"Synchronization matrix computed successfully")
        
        # å¯è¦–åŒ–ï¼ˆä¿®æ­£ç‰ˆã‚’ä½¿ç”¨ï¼‰
        plot_sync_matrix_numpyro_fixed(sync_matrix, names)
        
        return sync_matrix, names
        
    except Exception as e:
        print(f"Synchronization analysis failed: {e}")
        import traceback
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚namesã‚’è¿”ã™
        return None, series_names  

# ===============================
# Main Analysis Pipeline
# ===============================
def comprehensive_lambda3_analysis(csv_path: str = None,
                                   series_columns: Optional[List[str]] = None,
                                   run_diagnostics: bool = True,
                                   run_all_pairs: bool = True,
                                   max_pairs: int = None) -> Dict[str, Any]:
    """PyMCã‚¹ã‚¿ã‚¤ãƒ«ã®åŒ…æ‹¬çš„LambdaÂ³è§£æ"""
    
    config = L3ConfigNumPyro(
        num_samples=1000,  # PyMCã®drawsã«ç›¸å½“
        num_warmup=500,    # PyMCã®tuneã«ç›¸å½“
        num_chains=2,
        target_accept_prob=0.8  # PyMCã®target_acceptã«ç›¸å½“
    )
    
    print("ğŸš€ COMPREHENSIVE LAMBDAÂ³ ANALYSIS (PyMC Style)")
    print("=" * 60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if csv_path is None:
        print("Fetching financial data...")
        data_df = fetch_financial_data_numpyro()
        if data_df is None:
            return None
        csv_path = "financial_data_numpyro.csv"
    
    series_dict = load_csv_to_jax(csv_path, series_columns)
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨
    if len(series_dict) > 0:
        scaling_method = recommend_scaling_method(series_dict)
        series_dict, scaling_info = preprocess_series_dict(
            series_dict, 
            scaling_method=scaling_method,
            verbose=True
        )
    
    # 2. ç‰¹å¾´æŠ½å‡º
    print("\nExtracting LambdaÂ³ features...")
    features_dict = {}
    for name, data in series_dict.items():
        features = extract_lambda3_features_jax(data, config)
        features_dict[name] = features
        
        # çµ±è¨ˆè¡¨ç¤º
        n_pos = int(jnp.sum(features['delta_lambda_pos']))
        n_neg = int(jnp.sum(features['delta_lambda_neg']))
        avg_rho = float(jnp.mean(features['rho_t']))
        print(f"  {name:15s} | Pos: {n_pos:3d} | Neg: {n_neg:3d} | ÏT: {avg_rho:.3f}")
    
    # 3. é«˜åº¦è§£æå™¨åˆæœŸåŒ–
    analyzer = Lambda3AdvancedAnalyzer(config)
    
    # 4. å…¨ãƒšã‚¢è§£æï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    if run_all_pairs and len(series_dict) >= 2:
        print(f"\nRunning comprehensive pair analysis...")
        pair_results = analyzer.analyze_all_pairs(
            series_dict, features_dict, max_pairs=max_pairs
        )
    else:
        pair_results = {}
    
    # 5. ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º
    regime_results = analyzer.detect_market_regimes(features_dict)
    
    # 6. ã‚¹ã‚±ãƒ¼ãƒ«å¤‰åŒ–ç‚¹æ¤œå‡º
    first_series = list(series_dict.keys())[0]
    scale_breaks = analyzer.detect_scale_breaks(series_dict[first_series])
    
    # 7. æ¡ä»¶ä»˜ãåŒæœŸ
    if len(features_dict) >= 2:
        series_names = list(features_dict.keys())
        conditional_sync = analyzer.calculate_conditional_sync(
            features_dict[series_names[0]], 
            features_dict[series_names[1]]
        )
    else:
        conditional_sync = 0.0
    
    # 8. åŒæœŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
    if 'sync_profiles' in pair_results:
        sync_network = build_sync_network_advanced(
            pair_results['sync_profiles'], 
            threshold=0.0
        )
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ–
        if sync_network.number_of_edges() > 0:
            plot_network_analysis_numpyro(
                jnp.array([[1.0]]), ['dummy'], threshold=0.0  # ãƒ€ãƒŸãƒ¼ï¼ˆå®Ÿéš›ã¯sync_networkã‚’ä½¿ç”¨ï¼‰
            )
    else:
        sync_network = None
    
    # 9. åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼
    analysis_results = {
        'regime_results': regime_results,
        'scale_breaks': scale_breaks,
        'conditional_sync': conditional_sync,
        **pair_results
    }
    
    create_comprehensive_summary(series_dict, features_dict, analysis_results)
    
    return {
        'series_dict': series_dict,
        'features_dict': features_dict,
        'analysis_results': analysis_results,
        'sync_network': sync_network,
        'scaling_info': scaling_info
    }

def main_lambda3_numpyro_analysis(csv_path: str = None, 
                                 config: L3ConfigNumPyro = None,
                                 series_columns: Optional[List[str]] = None,
                                 auto_scaling: bool = True,
                                 scaling_method: str = 'auto'):
    """LambdaÂ³ NumPyro ãƒ¡ã‚¤ãƒ³è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"""
    
    if config is None:
        config = L3ConfigNumPyro()
    
    print("=" * 60)
    print("LambdaÂ³ NumPyro GPU Analysis Pipeline")
    print("=" * 60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        if csv_path is None:
            print("Fetching financial data...")
            data_df = fetch_financial_data_numpyro()
            if data_df is None:
                return None
            csv_path = "financial_data_numpyro.csv"
        
        print(f"Loading data from: {csv_path}")
        series_dict = load_csv_to_jax(csv_path, series_columns)
        
        if len(series_dict) < 2:
            print("Need at least 2 series for analysis")
            return None
        
        print(f"Loaded {len(series_dict)} series: {list(series_dict.keys())}")
    except Exception as e:
        print(f"Data loading failed: {e}")
        return None
    
    # 1.5. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ»å‰å‡¦ç†
    original_series_dict = series_dict.copy()  # å…ƒãƒ‡ãƒ¼ã‚¿ä¿å­˜
    
    if auto_scaling:
        if scaling_method == 'auto':
            scaling_method = recommend_scaling_method(series_dict)
        
        series_dict, scaling_info = preprocess_series_dict(
            series_dict, 
            scaling_method=scaling_method,
            verbose=True
        )
        
        print(f"\nğŸ”„ Applied scaling method: {scaling_method}")
    else:
        scaling_info = None
        print("ğŸš« Scaling disabled - using raw data")
    
    # 2. ç‰¹å¾´æŠ½å‡ºï¼ˆä¸¦åˆ—åŒ–ï¼‰
    print("\nExtracting LambdaÂ³ features (JAX optimized)...")
    start_time = time.time()
    
    features_dict = {}
    try:
        for name, data in series_dict.items():
            features = extract_lambda3_features_jax(data, config)
            features_dict[name] = features
            
            # çµ±è¨ˆè¡¨ç¤º
            n_pos = int(jnp.sum(features['delta_lambda_pos']))
            n_neg = int(jnp.sum(features['delta_lambda_neg']))
            avg_rho = float(jnp.mean(features['rho_t']))
            print(f"  {name:15s} | Pos: {n_pos:3d} | Neg: {n_neg:3d} | ÏT: {avg_rho:.3f}")
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®æ”¹å–„ã‚’è¡¨ç¤º
            if scaling_info and name in scaling_info:
                info = scaling_info[name]
                if info['is_problematic']:
                    print(f"    âœ… Fixed: {', '.join(info['issues'])}")
        
        feature_time = time.time() - start_time
        print(f"Feature extraction completed in {feature_time:.2f}s")
    except Exception as e:
        print(f"Feature extraction failed: {e}")
        return None
    
    # 3. ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³æ¨è«–ï¼ˆGPUä¸¦åˆ—åŒ–ï¼‰
    print("\nRunning Bayesian inference...")
    inference_engine = Lambda3NumPyroInference(config)
    
    series_names = list(series_dict.keys())
    inference_results = {}
    
    # å„ç³»åˆ—ã®åŸºæœ¬ãƒ¢ãƒ‡ãƒ«
    for i, name in enumerate(series_names[:2]):  # æœ€åˆã®2ç³»åˆ—ã®ã¿ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
        print(f"  Fitting base model for {name}...")
        try:
            result = inference_engine.fit_base_model(
                series_dict[name], 
                features_dict[name],
                chain_id=hash(name) % 1000
            )
            inference_results[name] = result
            
            # è¨ºæ–­æƒ…å ±ï¼ˆå®‰å…¨ãªå–å¾—ï¼‰
            diag = result['diagnostics']
            divergences = diag.get('divergences', 0)
            energy = diag.get('energy', 'N/A')
            accept_prob = diag.get('accept_prob', 'N/A')
            
            if isinstance(energy, (int, float)):
                print(f"    Divergences: {divergences}, Energy: {energy:.3f}, Accept: {accept_prob:.3f}")
            else:
                print(f"    Divergences: {divergences}, Accept: {accept_prob}")
                
            # R-hatæƒ…å ±ãŒã‚ã‚Œã°è¡¨ç¤º
            rhat_keys = [k for k in diag.keys() if k.startswith('rhat_')]
            if rhat_keys:
                max_rhat = max([diag[k] for k in rhat_keys])
                print(f"    Max R-hat: {max_rhat:.3f} {'âœ…' if max_rhat < 1.1 else 'âš ï¸'}")
                
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹æœè¡¨ç¤º
            if scaling_info and name in scaling_info:
                info = scaling_info[name]
                original_std = info['original_std']
                print(f"    Original scale std: {original_std:.6f} â†’ Normalized: {info['scaled_std']:.3f}")
                
        except Exception as e:
            print(f"    Model fitting failed: {e}")
            continue
    
    # 4. ç›¸äº’ä½œç”¨è§£æ
    if len(series_names) >= 2 and len(inference_results) >= 2:
        name_a, name_b = series_names[0], series_names[1]
        print(f"\nFitting interaction model: {name_a} â†” {name_b}")
        
        try:
            interaction_result = inference_engine.fit_interaction_model(
                series_dict[name_a],
                features_dict[name_a],
                features_dict[name_b],
                chain_id=2000
            )
            
            # ç›¸äº’ä½œç”¨ä¿‚æ•°è¡¨ç¤ºï¼ˆå®‰å…¨ãªå–å¾—ï¼‰
            samples = interaction_result['samples']
            interact_pos_mean = float(jnp.mean(samples.get('lambda_interact_pos', 0)))
            interact_neg_mean = float(jnp.mean(samples.get('lambda_interact_neg', 0)))
            rho_interact_mean = float(jnp.mean(samples.get('rho_interact', 0)))
            
            print(f"  Interaction coefficients:")
            print(f"    Positive: {interact_pos_mean:.4f}")
            print(f"    Negative: {interact_neg_mean:.4f}")
            print(f"    Tension:  {rho_interact_mean:.4f}")
            
            # è¨ºæ–­æƒ…å ±
            interact_diag = interaction_result['diagnostics']
            interact_div = interact_diag.get('divergences', 0)
            print(f"    Interaction model divergences: {interact_div}")
            
        except Exception as e:
            print(f"  Interaction model failed: {e}")
    
    # 5. åŒæœŸè§£æï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    print("\nSynchronization analysis...")
    try:
        # ã‚¤ãƒ™ãƒ³ãƒˆç³»åˆ—ã®æº–å‚™
        event_series_dict = {
            name: np.array(features_dict[name]['delta_lambda_pos'], dtype=np.float64)
            for name in series_names
        }
        
        # ã‚¤ãƒ™ãƒ³ãƒˆç³»åˆ—ã®æ¤œè¨¼
        validate_event_series(event_series_dict)
        
        # PyMCã‚¹ã‚¿ã‚¤ãƒ«ã®åŒ…æ‹¬çš„åŒæœŸè§£æã‚’å®Ÿè¡Œ
        sync_matrix, sync_network = comprehensive_sync_analysis_pymc_style(series_names, features_dict)
        print(f"\nSynchronization analysis completed successfully")

        # å‡ºåŠ›è¡Œåˆ—ã®ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨
        names = series_names        
        
        # sync_networkãŒNoneã®å ´åˆã®å‡¦ç†
        if sync_network is None:
            sync_network = nx.DiGraph()  # ç©ºã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        
    except Exception as e:
        print(f"Synchronization analysis failed: {e}")
        import traceback
        traceback.print_exc()
        sync_matrix = None
        sync_network = None
        names = list(series_dict.keys())
    
    # 6. å¯è¦–åŒ–
    print("\nGenerating visualizations...")
    try:
        # å„ç³»åˆ—ã®çµæœãƒ—ãƒ­ãƒƒãƒˆ
        for name in series_names[:2]:
            if name in inference_results:
                result = inference_results[name]
                predictions = result['predictions']['y']
                
                # å…ƒã‚¹ã‚±ãƒ¼ãƒ«ã§ã®å¯è¦–åŒ–ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
                if scaling_info and name in scaling_info:
                    info = scaling_info[name]
                    # äºˆæ¸¬ã‚’å…ƒã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
                    if scaling_method == 'standardize':
                        original_data = original_series_dict[name]
                        if predictions.ndim > 1:
                            pred_rescaled = predictions * info['original_std'] + info['original_mean']
                        else:
                            pred_rescaled = predictions * info['original_std'] + info['original_mean']
                    else:
                        original_data = original_series_dict[name] 
                        pred_rescaled = predictions  # ç°¡æ˜“ç‰ˆ
                    
                    plot_lambda3_results_numpyro(
                        original_data,
                        pred_rescaled,
                        features_dict[name],
                        title=f"LambdaÂ³ Analysis: {name} (Rescaled)"
                    )
                else:
                    plot_lambda3_results_numpyro(
                        series_dict[name],
                        predictions,
                        features_dict[name],
                        title=f"LambdaÂ³ Analysis: {name}"
                    )
    except Exception as e:
        print(f"Visualization failed: {e}")
    
    # 7. çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ANALYSIS SUMMARY")
    print("=" * 60)
    
    print(f"Series analyzed: {len(series_dict)}")
    print(f"Successful inferences: {len(inference_results)}")
    print(f"Scaling method used: {scaling_method if auto_scaling else 'none'}")
    print(f"JAX backend: {jax.default_backend()}")
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹æœã®ã‚µãƒãƒªãƒ¼
    if scaling_info:
        problematic_count = sum(1 for info in scaling_info.values() if info['is_problematic'])
        if problematic_count > 0:
            print(f"ğŸ”§ Fixed {problematic_count} problematic series through scaling")
    
    # ãƒˆãƒƒãƒ—åŒæœŸãƒšã‚¢
    if sync_matrix is not None and names is not None:  # namesã®å­˜åœ¨ã‚’ç¢ºèª
        print("\nTop synchronization pairs:")
        n = len(names)
        sync_pairs = []
        for i in range(n):
            for j in range(i+1, n):
                sync_rate = float(sync_matrix[i, j])
                sync_pairs.append((sync_rate, names[i], names[j]))
        
        sync_pairs.sort(reverse=True)
        for sync_rate, name_a, name_b in sync_pairs[:3]:
            print(f"  {name_a:15s} â†” {name_b:15s} | Ïƒâ‚› = {sync_rate:.3f}")
    
    # 8. åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    final_results = {
        'series_dict': series_dict,
        'original_series_dict': original_series_dict,
        'features_dict': features_dict,
        'inference_results': inference_results,
        'sync_matrix': sync_matrix,
        'series_names': names,  # namesã‚’ä½¿ç”¨
        'scaling_info': scaling_info
    }
    
    create_comprehensive_report_numpyro(final_results)
    
    # 9. å› æœé–¢ä¿‚è§£æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if len(inference_results) >= 2:
        try:
            print(f"\nğŸ”— CAUSALITY ANALYSIS:")
            series_list = list(inference_results.keys())[:2]
            
            # Lambda3æ‹¡å¼µè§£æ
            from collections import defaultdict
            causality_data = []
            labels = []
            
            for series_name in series_list:
                # å˜ç´”åŒ–ã—ãŸå› æœé–¢ä¿‚è¨ˆç®—
                features = features_dict[series_name]
                pos_events = np.array(features['delta_lambda_pos'])
                neg_events = np.array(features['delta_lambda_neg'])
                
                # ãƒ©ã‚°åˆ¥å› æœé–¢ä¿‚
                causality_by_lag = {}
                for lag in range(1, 11):
                    if lag < len(pos_events):
                        # æ­£â†’è² ã®å› æœé–¢ä¿‚
                        pos_to_neg = 0
                        pos_count = 0
                        for i in range(len(pos_events) - lag):
                            if pos_events[i] > 0:
                                pos_count += 1
                                if neg_events[i + lag] > 0:
                                    pos_to_neg += 1
                        
                        causality_by_lag[lag] = pos_to_neg / max(pos_count, 1)
                    else:
                        causality_by_lag[lag] = 0.0
                
                causality_data.append(causality_by_lag)
                labels.append(f"{series_name} (posâ†’neg)")
            
            # å› æœé–¢ä¿‚ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ­ãƒƒãƒˆ
            if causality_data:
                plot_causality_profiles_numpyro(
                    causality_data, 
                    labels, 
                    title="LambdaÂ³ Causal Structure Analysis"
                )
                
        except Exception as e:
            print(f"Causality analysis failed: {e}")
    
    return final_results

# ===============================
# PyMCäº’æ›ãƒ¬ãƒãƒ¼ãƒˆé–¢æ•°
# ===============================
# Lambda_abc_NumPyro.py ã«è¿½åŠ ã™ã‚‹é–¢æ•°
# PyMCã¨å®Œå…¨ã«åŒã˜å‡ºåŠ›ã‚’å®Ÿç¾ã™ã‚‹åŒæœŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é–¢æ•°

def build_sync_network_pymc_style(event_series_dict: Dict[str, np.ndarray],
                                 lag_window: int = 10,
                                 sync_threshold: float = 0.3) -> nx.DiGraph:
    """PyMCã‚¹ã‚¿ã‚¤ãƒ«ã®åŒæœŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ï¼ˆå…ƒã®printå‡ºåŠ›ã‚’å®Œå…¨å†ç¾ï¼‰"""
    
    series_names = list(event_series_dict.keys())
    G = nx.DiGraph()

    # ãƒãƒ¼ãƒ‰è¿½åŠ 
    for series in series_names:
        G.add_node(series)

    print(f"\nBuilding sync network with threshold={sync_threshold}")

    # ã‚¨ãƒƒã‚¸è¿½åŠ 
    edge_count = 0
    for name_a in series_names:
        for name_b in series_names:
            if name_a == name_b:
                continue

            try:
                series_a = np.asarray(event_series_dict[name_a], dtype=np.float64)
                series_b = np.asarray(event_series_dict[name_b], dtype=np.float64)
                
                sync_profile, max_sync, optimal_lag = calculate_sync_profile_simple(
                    series_a, series_b, lag_window
                )

                print(f"{name_a} â†’ {name_b}: max_sync={max_sync:.4f}, lag={optimal_lag}")

                if max_sync >= sync_threshold:
                    G.add_edge(name_a, name_b,
                              weight=max_sync,
                              lag=optimal_lag,
                              profile=sync_profile)
                    edge_count += 1
                    print(f"  âœ“ Edge added!")
                    
            except Exception as e:
                print(f"{name_a} â†’ {name_b}: failed ({e})")

    print(f"\nNetwork summary: {G.number_of_nodes()} nodes, {edge_count} edges")
    return G


def plot_sync_network_pymc_style(G: nx.DiGraph):
    """PyMCã‚¹ã‚¿ã‚¤ãƒ«ã®åŒæœŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•æç”»"""
    pos = nx.spring_layout(G)
    edge_labels = {
        (u, v): f"Ïƒâ‚›:{d['weight']:.2f},lag:{d['lag']}"
        for u, v, d in G.edges(data=True)
    }

    plt.figure(figsize=(12, 10))
    nx.draw(G, pos, with_labels=True, node_color='skyblue',
            node_size=1500, font_size=10, arrowsize=20)
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
    plt.title("Synchronization (Ïƒâ‚›) Network")
    plt.show()

def comprehensive_sync_analysis_pymc_style(series_names: List[str], 
                                          features_dict: Dict[str, Dict[str, np.ndarray]]):
    """PyMCã‚¹ã‚¿ã‚¤ãƒ«ã®åŒ…æ‹¬çš„åŒæœŸè§£æã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
    
    try:
        # Multi-series synchronization analysis
        print("\n" + "="*50)
        print("MULTI-SERIES SYNCHRONIZATION ANALYSIS")
        print("="*50)

        # Build event series dictionary
        event_series_dict = {
            name: np.array(features_dict[name]['delta_lambda_pos'], dtype=np.float64)
            for name in series_names
        }

        # Synchronization matrix (PyMCç‰ˆã®sync_matrix_simpleé–¢æ•°ã‚’ä½¿ç”¨)
        sync_mat, names = sync_matrix_simple(event_series_dict, lag_window=10)

        # Plot sync matrix heatmapï¼ˆPyMCã¨åŒã˜ï¼‰
        plt.figure(figsize=(10, 8))
        sns.heatmap(sync_mat, annot=True, fmt='.3f',
                    xticklabels=names,
                    yticklabels=names,
                    cmap="Blues", vmin=0, vmax=1,
                    square=True, cbar_kws={'label': 'Sync Rate Ïƒâ‚›'})
        plt.title("Synchronization Rate Matrix (Ïƒâ‚›)", fontsize=16)
        plt.tight_layout()
        plt.show()

        # Build and plot sync network
        print("\n=== Building Synchronization Network ===")

        # Find appropriate thresholdï¼ˆPyMCã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        non_diag_values = []
        n = len(names)
        for i in range(n):
            for j in range(n):
                if i != j:
                    non_diag_values.append(sync_mat[i, j])

        G = None  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯None
        if non_diag_values:
            threshold = np.percentile(non_diag_values, 25)  # Use 25th percentile
            print(f"Using threshold: {threshold:.4f}")

            G = build_sync_network_pymc_style(event_series_dict, lag_window=10, sync_threshold=threshold)
            if G.number_of_edges() > 0:
                plt.figure(figsize=(12, 10))
                plot_sync_network_pymc_style(G)

        # Clustering analysisï¼ˆPyMCã¨åŒã˜ï¼‰
        if len(series_names) > 2:
            print("\n=== Clustering Analysis ===")
            n_clusters = min(3, len(series_names) // 2)
            clusters, _ = cluster_series_by_sync_simple(event_series_dict, lag_window=10, n_clusters=n_clusters)
            print(f"Clusters: {clusters}")

            # Plot clustered series - ãƒ‡ãƒ¼ã‚¿è¾æ›¸ã‚’ä½œæˆ
            series_data_dict = {}
            for name in series_names:
                # features_dictã‹ã‚‰å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆdataã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆï¼‰
                if 'data' in features_dict[name]:
                    series_data_dict[name] = np.array(features_dict[name]['data'])
                else:
                    # dataã‚­ãƒ¼ãŒãªã„å ´åˆã¯ã€æœ€åˆã®åˆ©ç”¨å¯èƒ½ãªç³»åˆ—ã‚’ä½¿ç”¨
                    for key in ['delta_lambda_pos', 'delta_lambda_neg', 'rho_t']:
                        if key in features_dict[name]:
                            series_data_dict[name] = np.array(features_dict[name][key])
                            break
            
            if series_data_dict:
                plot_clustered_series(series_data_dict, clusters)

        return sync_mat, G if G is not None else nx.DiGraph()
        
    except Exception as e:
        print(f"Comprehensive sync analysis failed: {e}")
        import traceback
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
        n = len(series_names)
        default_sync_mat = np.eye(n)  # å¯¾è§’è¡Œåˆ—
        return default_sync_mat, nx.DiGraph()

# plot_clustered_seriesé–¢æ•°ï¼ˆå…ƒã®PyMCç‰ˆã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
def plot_clustered_series(series_dict: Dict[str, np.ndarray],
                         clusters: Dict[str, int]):
    """
    Plot time series colored by cluster membership.

    Args:
        series_dict: Dictionary of time series
        clusters: Cluster assignments for each series
    """
    n_clusters = len(set(clusters.values()))
    colors = plt.cm.Set1(np.linspace(0, 1, n_clusters))

    plt.figure(figsize=(12, 6))
    for name, data in series_dict.items():
        cluster = clusters[name]
        plt.plot(data, label=f"{name} (Cluster {cluster})",
                color=colors[cluster], alpha=0.7, linewidth=1.5)

    plt.xlabel("Time Step")
    plt.ylabel("Value")
    plt.title("Time Series Grouped by Synchronization Clusters")
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

def create_comprehensive_report_numpyro(results: Dict[str, Any]):
    """
    PyMCç‰ˆã¨å®Œå…¨ã«åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        results: è§£æçµæœã®è¾æ›¸
    """
    series_dict = results.get('series_dict', {})
    features_dict = results.get('features_dict', {})
    inference_results = results.get('inference_results', {})
    sync_matrix = results.get('sync_matrix')
    series_names = results.get('series_names', list(series_dict.keys()))
    scaling_info = results.get('scaling_info', {})
    
    print("\n" + "="*60)
    print("COMPREHENSIVE LAMBDAÂ³ ANALYSIS REPORT")
    print("="*60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    print("\nğŸ“Š DATA OVERVIEW")
    print("-" * 40)
    if series_dict:
        first_series = list(series_dict.keys())[0]
        data_length = len(series_dict[first_series])
        print(f"Time series length: {data_length}")
        print(f"Number of series: {len(series_dict)}")
        print(f"Series names: {', '.join(series_names)}")
        
        # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        print("\nSeries Statistics:")
        for name in series_names:
            data = np.array(series_dict[name])
            mean_val = np.mean(data)
            std_val = np.std(data)
            min_val = np.min(data)
            max_val = np.max(data)
            print(f"  {name:15s} | Mean: {mean_val:8.4f} | Std: {std_val:8.4f} | Range: [{min_val:.4f}, {max_val:.4f}]")
    
    # 2. LambdaÂ³ç‰¹å¾´é‡çµ±è¨ˆï¼ˆPyMCã¨åŒã˜å½¢å¼ï¼‰
    print("\nğŸ” LAMBDAÂ³ FEATURE STATISTICS")
    print("-" * 40)
    print("Jump Event Statistics:")
    print("Series          | Pos Î”Î›C | Neg Î”Î›C | Local | Mean ÏT")
    print("-" * 60)
    
    for name in series_names:
        if name in features_dict:
            features = features_dict[name]
            pos_jumps = int(jnp.sum(features['delta_lambda_pos']))
            neg_jumps = int(jnp.sum(features['delta_lambda_neg']))
            local_jumps = int(jnp.sum(features.get('local_jump', 0)))
            mean_rho = float(jnp.mean(features['rho_t']))
            
            print(f"{name:15s} | {pos_jumps:7d} | {neg_jumps:7d} | {local_jumps:5d} | {mean_rho:7.3f}")
    
    # 3. ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³æ¨è«–çµæœï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    if inference_results:
        print("\nğŸ“ˆ BAYESIAN INFERENCE RESULTS")
        print("-" * 40)
        
        for name, result in inference_results.items():
            print(f"\nSeries: {name}")
            samples = result['samples']
            diagnostics = result['diagnostics']
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šå€¤
            print("  Parameter Estimates:")
            param_order = ['lambda_intercept', 'lambda_flow', 'lambda_struct_pos', 
                          'lambda_struct_neg', 'rho_tension']
            
            for param in param_order:
                if param in samples:
                    values = np.array(samples[param])
                    mean_val = np.mean(values)
                    std_val = np.std(values)
                    hdi_lower = np.percentile(values, 3)
                    hdi_upper = np.percentile(values, 97)
                    print(f"    {param:20s}: {mean_val:7.3f} Â± {std_val:5.3f} HDI:[{hdi_lower:.3f}, {hdi_upper:.3f}]")
            
            # è¨ºæ–­çµ±è¨ˆ
            print("  Diagnostics:")
            print(f"    Divergences: {diagnostics.get('divergences', 0)}")
            print(f"    Accept prob: {diagnostics.get('accept_prob', 0):.3f}")
            if 'energy' in diagnostics:
                print(f"    Energy: {diagnostics['energy']:.3f}")
            
            # R-hatå€¤
            rhat_params = [k for k in diagnostics.keys() if k.startswith('rhat_')]
            if rhat_params:
                print("  Convergence (R-hat):")
                for param in rhat_params:
                    value = diagnostics[param]
                    status = "âœ…" if value < 1.1 else "âš ï¸"
                    print(f"    {param}: {value:.3f} {status}")
    
    # 4. åŒæœŸè§£æçµæœï¼ˆPyMCã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    if sync_matrix is not None and len(series_names) >= 2:
        print("\nğŸ”— SYNCHRONIZATION ANALYSIS")
        print("-" * 40)
        
        # åŒæœŸè¡Œåˆ—ã®è¦ç´„
        sync_np = np.array(sync_matrix)
        n = len(series_names)
        
        # ãƒˆãƒƒãƒ—åŒæœŸãƒšã‚¢
        print("Top Synchronization Pairs:")
        sync_pairs = []
        for i in range(n):
            for j in range(i+1, n):
                sync_rate = float(sync_np[i, j])
                sync_pairs.append((sync_rate, series_names[i], series_names[j]))
        
        sync_pairs.sort(reverse=True)
        for sync_rate, name_a, name_b in sync_pairs[:5]:
            print(f"  {name_a:15s} â†” {name_b:15s} | Ïƒâ‚› = {sync_rate:.3f}")
        
        # å¹³å‡åŒæœŸç‡
        off_diagonal = []
        for i in range(n):
            for j in range(n):
                if i != j:
                    off_diagonal.append(sync_np[i, j])
        
        if off_diagonal:
            mean_sync = np.mean(off_diagonal)
            print(f"\nAverage sync rate (off-diagonal): {mean_sync:.3f}")
    
    # 5. ç›¸äº’ä½œç”¨åŠ¹æœï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    if 'analysis_results' in results and 'interaction_effects' in results['analysis_results']:
        interaction_effects = results['analysis_results']['interaction_effects']
        if interaction_effects:
            print("\nğŸ”„ CROSS-SERIES INTERACTION EFFECTS")
            print("-" * 40)
            
            # ç›¸äº’ä½œç”¨è¡Œåˆ—ã‚’æ§‹ç¯‰
            interaction_matrix = {}
            for (name_a, name_b), effects in interaction_effects.items():
                for effect_name, value in effects.items():
                    if 'to' in effect_name:
                        interaction_matrix[effect_name] = value
            
            # è¡¨ç¤º
            for key, value in sorted(interaction_matrix.items()):
                if abs(value) > 0.01:  # æœ‰æ„ãªåŠ¹æœã®ã¿
                    print(f"  {key}: Î² = {value:.3f}")
    
    # 6. ãƒ¬ã‚¸ãƒ¼ãƒ è§£æï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    if 'analysis_results' in results and 'regime_results' in results['analysis_results']:
        regime_results = results['analysis_results']['regime_results']
        if 'regime_stats' in regime_results:
            print("\nğŸ¯ MARKET REGIME ANALYSIS")
            print("-" * 40)
            
            regime_stats = regime_results['regime_stats']
            for regime_name, stats in sorted(regime_stats.items()):
                freq_pct = stats['frequency'] * 100
                mean_rho = stats['mean_rhoT']
                print(f"  {regime_name}: {freq_pct:.1f}% frequency, Mean ÏT: {mean_rho:.3f}")
    
    # 7. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æƒ…å ±ï¼ˆé©ç”¨ã•ã‚ŒãŸå ´åˆï¼‰
    if scaling_info:
        problematic_count = sum(1 for info in scaling_info.values() if info['is_problematic'])
        if problematic_count > 0:
            print("\nâš™ï¸ DATA PREPROCESSING")
            print("-" * 40)
            print(f"Scaling method applied: {scaling_info[series_names[0]]['scaling_method']}")
            print(f"Problematic series fixed: {problematic_count}")
            
            for name, info in scaling_info.items():
                if info['is_problematic']:
                    print(f"  {name}: {', '.join(info['issues'])}")
    
    # 8. å®Ÿè¡Œã‚µãƒãƒªãƒ¼
    print("\nğŸ“Š EXECUTION SUMMARY")
    print("-" * 40)
    print(f"âœ… Feature extraction: Complete")
    print(f"âœ… Bayesian inference: {len(inference_results)} series analyzed")
    if sync_matrix is not None:
        print(f"âœ… Synchronization analysis: Complete")
    print(f"âœ… JAX backend: {jax.default_backend()}")
    
    print("\n" + "="*60)
    print("END OF REPORT")
    print("="*60)


def plot_interaction_heatmap_pymc_style(interaction_results: Dict[str, Dict[str, float]],
                                       series_names: List[str]):
    """PyMCã‚¹ã‚¿ã‚¤ãƒ«ã®ç›¸äº’ä½œç”¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    n = len(series_names)
    interaction_matrix = np.zeros((n, n))
    
    # è¡Œåˆ—ã‚’æ§‹ç¯‰ï¼ˆPyMCã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    for i, name_a in enumerate(series_names):
        for j, name_b in enumerate(series_names):
            if name_a != name_b:
                # B â†’ A ã®å½±éŸ¿ã‚’æ¢ã™
                if name_a in interaction_results:
                    key = f'{name_b}_to_{name_a}_pos'
                    if key in interaction_results[name_a]:
                        interaction_matrix[i, j] = interaction_results[name_a][key]
                    elif f'interact_{name_b}' in interaction_results[name_a]:
                        interaction_matrix[i, j] = interaction_results[name_a][f'interact_{name_b}']
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(interaction_matrix,
                xticklabels=series_names,
                yticklabels=series_names,
                annot=True, fmt='.3f',
                cmap='RdBu_r', center=0,
                square=True,
                cbar_kws={'label': 'Interaction Coefficient Î²'})
    plt.title("Cross-Series Interaction Effects\n(Column â†’ Row)", fontsize=16)
    plt.xlabel("From Series", fontsize=12)
    plt.ylabel("To Series", fontsize=12)
    plt.tight_layout()
    plt.show()


def create_analysis_summary_pymc_style(series_names: List[str],
                                      sync_mat: jnp.ndarray,
                                      features_dict: Dict[str, Dict[str, jnp.ndarray]]):
    """PyMCã‚¹ã‚¿ã‚¤ãƒ«ã®è§£æã‚µãƒãƒªãƒ¼ä½œæˆ"""
    print("\n" + "="*60)
    print("ANALYSIS SUMMARY")
    print("="*60)
    
    # Jump event statisticsï¼ˆPyMCã¨å®Œå…¨ä¸€è‡´ï¼‰
    print("\nJump Event Statistics:")
    print("-" * 40)
    for name in series_names:
        pos_jumps = int(jnp.sum(features_dict[name]['delta_lambda_pos']))
        neg_jumps = int(jnp.sum(features_dict[name]['delta_lambda_neg']))
        local_jumps = int(jnp.sum(features_dict[name].get('local_jump', 0)))
        print(f"{name:15s} | Pos: {pos_jumps:3d} | Neg: {neg_jumps:3d} | Local: {local_jumps:3d}")
    
    # Top synchronizationsï¼ˆPyMCã¨å®Œå…¨ä¸€è‡´ï¼‰
    print("\nTop Synchronization Pairs:")
    print("-" * 40)
    sync_pairs = []
    sync_np = np.array(sync_mat)
    for i, name_a in enumerate(series_names):
        for j, name_b in enumerate(series_names):
            if i < j:  # Only unique pairs
                sync_pairs.append((sync_np[i, j], name_a, name_b))
    
    sync_pairs.sort(reverse=True)
    for sync_rate, name_a, name_b in sync_pairs[:5]:
        print(f"{name_a:15s} â†” {name_b:15s} | Ïƒâ‚› = {sync_rate:.3f}")
    
    print("\n" + "="*60)


def generate_pymc_compatible_output(analysis_results: Dict[str, Any]):
    """PyMCç‰ˆã¨å®Œå…¨äº’æ›ã®å‡ºåŠ›ã‚’ç”Ÿæˆ"""
    
    # 1. ç‰¹å¾´é‡çµ±è¨ˆã®è¡¨ç¤º
    if 'features_dict' in analysis_results:
        features_dict = analysis_results['features_dict']
        series_names = list(features_dict.keys())
        
        print("\nğŸ“Š FEATURE EXTRACTION SUMMARY (PyMC Compatible)")
        print("=" * 60)
        
        # PyMCã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡¨ç¤º
        for name in series_names:
            features = features_dict[name]
            n_pos = int(jnp.sum(features['delta_lambda_pos']))
            n_neg = int(jnp.sum(features['delta_lambda_neg']))
            avg_rho = float(jnp.mean(features['rho_t']))
            print(f"  {name:15s} | Pos: {n_pos:3d} | Neg: {n_neg:3d} | ÏT: {avg_rho:.3f}")
    
    # 2. ãƒšã‚¢è§£æçµæœã®è¡¨ç¤º
    if 'analysis_results' in analysis_results and 'sync_profiles' in analysis_results['analysis_results']:
        sync_profiles = analysis_results['analysis_results']['sync_profiles']
        
        print("\nğŸ”„ PAIRWISE ANALYSIS RESULTS (PyMC Style)")
        print("=" * 60)
        
        for (name_a, name_b), profile_data in sync_profiles.items():
            max_sync = profile_data['max_sync']
            optimal_lag = profile_data['optimal_lag']
            print(f"\n[{name_a} â†” {name_b}]")
            print(f"  Sync Rate Ïƒâ‚›: {max_sync:.3f}")
            print(f"  Optimal Lag: {optimal_lag} steps")
    
    # 3. å› æœé–¢ä¿‚ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    if 'analysis_results' in analysis_results and 'causality_results' in analysis_results['analysis_results']:
        causality_results = analysis_results['analysis_results']['causality_results']
        
        print("\nğŸ“ˆ CAUSALITY ANALYSIS (PyMC Format)")
        print("=" * 60)
        
        for (name_a, name_b), causality_data in causality_results.items():
            print(f"\nCausality: {name_a} â†” {name_b}")
            for direction, profile in causality_data.items():
                if isinstance(profile, dict) and profile:
                    max_lag = max(profile.items(), key=lambda x: x[1])
                    print(f"  {direction}: Peak at lag {max_lag[0]} (p={max_lag[1]:.3f})")

# ===============================
# Quick Start Example
# ===============================
def quick_start_demo():
    """ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆãƒ‡ãƒ¢ï¼ˆå‡¦ç†é€Ÿåº¦æ¸¬å®šå¼·åŒ–ç‰ˆï¼‰"""
    print("LambdaÂ³ NumPyro Quick Start Demo")
    print("=" * 40)
    
    # å…¨ä½“ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
    total_start_time = time.time()
    
    # è¨­å®š
    config = L3ConfigNumPyro(
        T=100,
        window=5,          # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºå‰Šæ¸›
        local_window=5,    # å±€æ‰€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚‚å‰Šæ¸›
        num_samples=200,   # ã‚µãƒ³ãƒ—ãƒ«æ•°å‰Šæ¸›
        num_warmup=100,    
        num_chains=1       
    )
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆã‚ˆã‚Šå˜ç´”åŒ–ï¼‰
    print("Generating sample data...")
    data_start = time.time()
    
    key = jax.random.PRNGKey(42)
    t = jnp.arange(config.T, dtype=jnp.float32)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ å¤‰åŒ–
    base_trend = 0.01 * t
    jumps = jnp.zeros(config.T)
    jumps = jumps.at[30].set(1.0)   # å˜ä¸€æ­£ã‚¸ãƒ£ãƒ³ãƒ—
    jumps = jumps.at[70].set(-0.8)  # å˜ä¸€è² ã‚¸ãƒ£ãƒ³ãƒ—
    
    noise = jax.random.normal(key, (config.T,)) * 0.2
    data = base_trend + jnp.cumsum(jumps) + noise
    
    data_time = time.time() - data_start
    print(f"Generated data shape: {data.shape}")
    print(f"Data range: [{jnp.min(data):.3f}, {jnp.max(data):.3f}]")
    print(f"Data type: {data.dtype}")
    print(f"â±ï¸  Data generation time: {data_time:.4f}s")
    
    # æ®µéšçš„ç‰¹å¾´æŠ½å‡ºãƒ†ã‚¹ãƒˆï¼ˆå€‹åˆ¥é€Ÿåº¦æ¸¬å®šï¼‰
    print("\nTesting feature extraction components...")
    
    # 1. å·®åˆ†ãƒ»é–¾å€¤è¨ˆç®—
    try:
        diff_start = time.time()
        diff, threshold = calculate_diff_threshold_jax(data, config.delta_percentile)
        diff_time = time.time() - diff_start
        print(f"âœ“ Diff calculation: shape={diff.shape}, threshold={threshold:.3f}")
        print(f"  â±ï¸  Time: {diff_time:.4f}s")
    except Exception as e:
        print(f"âœ— Diff calculation failed: {e}")
        return
    
    # 2. ã‚¸ãƒ£ãƒ³ãƒ—æ¤œå‡º
    try:
        jump_start = time.time()
        delta_pos, delta_neg = detect_jumps_jax(diff, threshold)
        jump_time = time.time() - jump_start
        print(f"âœ“ Jump detection: pos={jnp.sum(delta_pos)}, neg={jnp.sum(delta_neg)}")
        print(f"  â±ï¸  Time: {jump_time:.4f}s")
    except Exception as e:
        print(f"âœ— Jump detection failed: {e}")
        return
    
    # 3. å±€æ‰€æ¨™æº–åå·®ï¼ˆå€‹åˆ¥ãƒ†ã‚¹ãƒˆï¼‰
    try:
        print("Testing local std calculation...")
        local_std_start = time.time()
        local_std = calculate_local_std_jax(data, config.local_window)
        local_std_time = time.time() - local_std_start
        print(f"âœ“ Local std: shape={local_std.shape}, mean={jnp.mean(local_std):.3f}")
        print(f"  â±ï¸  Time: {local_std_time:.4f}s")
    except Exception as e:
        print(f"âœ— Local std failed: {e}")
        print("Using fallback...")
        local_std = jnp.ones_like(data) * jnp.std(data)
        local_std_time = 0.001
    
    # 4. ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆå€‹åˆ¥ãƒ†ã‚¹ãƒˆï¼‰
    try:
        print("Testing rho_t calculation...")
        rho_start = time.time()
        rho_t = calculate_rho_t_jax(data, config.window)
        rho_time = time.time() - rho_start
        print(f"âœ“ Rho_t: shape={rho_t.shape}, mean={jnp.mean(rho_t):.3f}")
        print(f"  â±ï¸  Time: {rho_time:.4f}s")
    except Exception as e:
        print(f"âœ— Rho_t failed: {e}")
        print("Using fallback...")
        rho_t = jnp.ones_like(data) * jnp.std(data)
        rho_time = 0.001
    
    # 5. å®Œå…¨ç‰¹å¾´æŠ½å‡ºï¼ˆçµ±åˆæ¸¬å®šï¼‰
    print("\nRunning full feature extraction...")
    try:
        feature_start = time.time()
        features = extract_lambda3_features_jax(data, config)
        feature_total_time = time.time() - feature_start
        
        print("âœ“ Feature extraction successful!")
        print(f"â±ï¸  Total feature extraction time: {feature_total_time:.4f}s")
        
        for key, value in features.items():
            if hasattr(value, 'shape'):
                print(f"  {key}: shape={value.shape}, mean={jnp.mean(value):.3f}")
            else:
                print(f"  {key}: {value}")
                
    except Exception as e:
        print(f"âœ— Feature extraction failed: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 6. æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆè©³ç´°é€Ÿåº¦æ¸¬å®šï¼‰
    print("\nTesting Bayesian inference...")
    try:
        inference_engine = Lambda3NumPyroInference(config)
        
        print("  Running MCMC sampling...")
        mcmc_start = time.time()
        
        # åˆå›å®Ÿè¡Œï¼ˆJIT compileå«ã‚€ï¼‰
        print("  - JIT compilation + first run...")
        result = inference_engine.fit_base_model(data, features, chain_id=0)
        first_run_time = time.time() - mcmc_start
        
        # 2å›ç›®å®Ÿè¡Œï¼ˆpure execution timeï¼‰
        print("  - Second run (pure execution)...")
        second_start = time.time()
        result2 = inference_engine.fit_base_model(data, features, chain_id=1)
        pure_execution_time = time.time() - second_start
        
        samples = result['samples']
        diagnostics = result['diagnostics']
        
        print("âœ“ Inference successful!")
        print(f"â±ï¸  First run (JIT + execution): {first_run_time:.4f}s")
        print(f"â±ï¸  Pure execution time: {pure_execution_time:.4f}s")
        print(f"â±ï¸  JIT compilation overhead: {first_run_time - pure_execution_time:.4f}s")
        
        print("\nParameter estimates:")
        for param, values in samples.items():
            if jnp.isscalar(values):
                print(f"  {param}: {values:.4f}")
            else:
                print(f"  {param}: {jnp.mean(values):.4f} Â± {jnp.std(values):.4f}")
        
        print("\nDiagnostics:")
        for key, value in diagnostics.items():
            if key.startswith('rhat_'):
                param_name = key.replace('rhat_', '')
                status = "âœ…" if value < 1.1 else "âš ï¸"
                print(f"  {key}: {value:.4f} {status}")
            elif isinstance(value, float):
                print(f"  {key}: {value:.4f}")
            else:
                print(f"  {key}: {value}")
        
        # è©³ç´°è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ
        print("\nGenerating MCMC diagnostics...")
        plot_mcmc_diagnostics(result, title=f"LambdaÂ³ MCMC Diagnostics")
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ºæ–­
        if diagnostics.get('energy') is not None:
            plot_energy_diagnostics(result)
                
        # å¯è¦–åŒ–ãƒ†ã‚¹ãƒˆ
        try:
            print("\nGenerating visualization...")
            viz_start = time.time()
            predictions = result['predictions']['y']
            
            # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
            if predictions.ndim > 1:
                pred_mean = jnp.mean(predictions, axis=0)
                print(f"  Predictions shape: {predictions.shape} -> mean shape: {pred_mean.shape}")
            else:
                pred_mean = predictions
                print(f"  Predictions shape: {predictions.shape}")
            
            plot_lambda3_results_numpyro(data, predictions, features, 
                                        title="LambdaÂ³ NumPyro Demo - SUCCESS!")
            viz_time = time.time() - viz_start
            print("âœ“ Visualization completed!")
            print(f"â±ï¸  Visualization time: {viz_time:.4f}s")
            
        except Exception as viz_e:
            print(f"Visualization failed: {viz_e}")
            print("But inference worked!")
            viz_time = 0
            
    except Exception as e:
        print(f"Inference failed: {e}")
        import traceback
        print("Full traceback:")
        traceback.print_exc()
        print("\nBut feature extraction worked perfectly - that's major progress!")
        first_run_time = 0
        pure_execution_time = 0
        viz_time = 0
    
    # Optional: Parameter grid search demonstration
    try:
        import sys
        if len(sys.argv) > 1 and sys.argv[1] == '--grid':
            print("\nğŸ” Running parameter grid search demonstration...")
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰å®šç¾©
            param_grid = {
                'target_accept_prob': [0.7, 0.8, 0.9],
                'max_tree_depth': [8, 10, 12]
            }
            
            # å°ã•ãªã‚°ãƒªãƒƒãƒ‰ã§ãƒ‡ãƒ¢
            grid_results = grid_search_lambda3_params(data, features, param_grid, config)
            
            if grid_results['best_params']:
                print(f"\nğŸ¯ Optimal configuration found: {grid_results['best_params']}")
                
                # ã‚°ãƒªãƒƒãƒ‰çµæœã®å¯è¦–åŒ–
                plot_grid_search_results(grid_results)
    except:
        pass  # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã¯ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«
    
    # ç·å®Ÿè¡Œæ™‚é–“
    total_time = time.time() - total_start_time
    
    print(f"\n{'='*60}")
    print("LAMBDAÂ³ NUMPYRO PERFORMANCE SUMMARY")
    print(f"{'='*60}")
    print("âœ“ JAX GPU backend active")
    print("âœ“ Data generation successful")
    print("âœ“ Feature extraction (Î”Î›C, ÏT) working") 
    print("âœ“ LambdaÂ³ structure tensor computation stable")
    print("âœ“ All JAX compilation issues resolved")
    print()
    print("âš¡ PERFORMANCE METRICS:")
    print(f"  ğŸ“Š Data generation:     {data_time:.4f}s")
    print(f"  ğŸ” Feature extraction:  {feature_total_time:.4f}s")
    if 'first_run_time' in locals():
        print(f"  ğŸ¯ MCMC (JIT+exec):     {first_run_time:.4f}s")
        print(f"  âš¡ MCMC (pure):         {pure_execution_time:.4f}s")
        print(f"  ğŸ¨ Visualization:       {viz_time:.4f}s")
    print(f"  ğŸ• TOTAL TIME:          {total_time:.4f}s")
    print()
    
    # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
    data_points = config.T
    mcmc_samples = config.num_samples
    if 'pure_execution_time' in locals() and pure_execution_time > 0:
        throughput = mcmc_samples / pure_execution_time
        print(f"ğŸš€ THROUGHPUT ANALYSIS:")
        print(f"  Data points processed:  {data_points}")
        print(f"  MCMC samples:          {mcmc_samples}")
        print(f"  Samples per second:    {throughput:.1f}")
        print(f"  GPU acceleration:      {jax.default_backend().upper()}")
    
    print("\nReady for full-scale analysis!")

# ===============================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ===============================
if __name__ == "__main__":
    # GPUæœ€é©åŒ–è¨­å®š
    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
    os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
    warnings.filterwarnings('ignore', category=FutureWarning)
    
    print("Starting LambdaÂ³ NumPyro Analysis...")
    
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    # quick_start_demo()
    
    # ãƒ•ãƒ«è§£æå®Ÿè¡Œ
    results = main_lambda3_numpyro_analysis(
        config=L3ConfigNumPyro(
            num_chains=4,
            max_workers=3
        )
    )
    
    if results:
        print("\nLambdaÂ³ NumPyro analysis completed successfully!")
    else:
        print("\nAnalysis failed. Check data and configuration.")
